{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: To pull down the earliest version of thhe synapses from 11/9\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "from annotationframeworkclient import FrameworkClient\n",
    "from IPython import html, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = FrameworkClient('minnie65_phase3_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20k = client.materialize.query_table('synapses_pni_2',\n",
    "                                    filter_in_dict=None,\n",
    "                                    filter_out_dict=None,\n",
    "                                    filter_equal_dict=None,\n",
    "                                    filter_spatial=None,#not yet implemented\n",
    "                                    select_columns=None,#reduces the amount of data you are pulling\n",
    "                                   offset=None #this is an integer that will set the offset\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>valid</th>\n",
       "      <th>pre_pt_position</th>\n",
       "      <th>pre_pt_supervoxel_id</th>\n",
       "      <th>pre_pt_root_id</th>\n",
       "      <th>ctr_pt_position</th>\n",
       "      <th>post_pt_position</th>\n",
       "      <th>post_pt_supervoxel_id</th>\n",
       "      <th>post_pt_root_id</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, valid, pre_pt_position, pre_pt_supervoxel_id, pre_pt_root_id, ctr_pt_position, post_pt_position, post_pt_supervoxel_id, post_pt_root_id, size]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = client.materialize.query_table('synapses_pni_2',\n",
    "                                    #filter_in_dict=dict(func_id=[58941,16062]),\n",
    "                                    filter_in_dict=dict(pre_pt_root_id=[864691131771477291,864691131881790489]), #just excludes those\n",
    "                                    \n",
    "                                    #filter_equal_dict=dict(func_id=58941),\n",
    "                                   offset=0 #this is an integer that will set the offset\n",
    "                                   )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the synapses for the neurons we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAll the possible fields:\\n\\nid\\npre_pt_position\\npre_pt_supervoxel_id\\npre_pt_root_id\\npost_pt_position\\npost_pt_supervoxel_id\\npost_pt_root_id\\nsize\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "All the possible fields:\n",
    "\n",
    "id\n",
    "pre_pt_position\n",
    "pre_pt_supervoxel_id\n",
    "pre_pt_root_id\n",
    "post_pt_position\n",
    "post_pt_supervoxel_id\n",
    "post_pt_root_id\n",
    "size\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'pre_pt_position',\n",
       " 'pre_pt_supervoxel_id',\n",
       " 'pre_pt_root_id',\n",
       " 'post_pt_position',\n",
       " 'post_pt_supervoxel_id',\n",
       " 'post_pt_root_id',\n",
       " 'size']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_we_want = [\n",
    "    \"id\",\n",
    "\"pre_pt_position\",\n",
    "\"pre_pt_supervoxel_id\",\n",
    "\"pre_pt_root_id\",\n",
    "\"post_pt_position\",\n",
    "\"post_pt_supervoxel_id\",\n",
    "\"post_pt_root_id\",\n",
    "\"size\"\n",
    "]\n",
    "\n",
    "columns_we_want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2020-11-25 17:55:57,078 - settings - Setting stores to {'minnie65': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65', 'stage': '/mnt/dj-stor01/platinum/minnie65'}, 'meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/meshes'}, 'decimated_meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes'}, 'skeletons': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/skeletons'}}\n",
      "INFO - 2020-11-25 17:55:57,079 - settings - Setting enable_python_native_blobs to True\n",
      "INFO - 2020-11-25 17:55:57,089 - connection - Connected celiib@at-database.ad.bcm.edu:3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@at-database.ad.bcm.edu:3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2020-11-25 17:55:57,365 - settings - Setting enable_python_native_blobs to True\n"
     ]
    }
   ],
   "source": [
    "minnie,schema = du.configure_minnie_vm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seg_ids_to_check_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-559255c0e3a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseg_ids_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mminnie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSegToDecimateFromNuclei\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"segment_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mseg_ids_to_check_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'seg_ids_to_check_total' is not defined"
     ]
    }
   ],
   "source": [
    "seg_ids_to_check = (minnie.SegToDecimateFromNuclei()).fetch(\"segment_id\")\n",
    "seg_ids_to_check_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "soma_segment_ids = np.unique((minnie.BaylorSegmentCentroid & \"multiplicity>0\").fetch(\"segment_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soma_segment_ids)/step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.materialize.query_table('synapses_pni_2',\n",
    "                                            #filter_in_dict=dict(func_id=[58941,16062]),\n",
    "                                            filter_in_dict=curr_filter_dict, #just excludes those\n",
    "\n",
    "                                            #filter_equal_dict=dict(func_id=58941),\n",
    "                                           offset=0 ,#this is an integer that will set the offset\n",
    "                                            #select_columns=columns_we_want\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pseudocode: \n",
    "\n",
    "in batches of 100 segment ids\n",
    "\n",
    "for both presyn and postsyn:\n",
    "1) Get the rows for that attirbute and those segments and turn into dictionaries and add to list\n",
    "2a) If the result is less than 200k don't need to repeat again \n",
    "2b) If they are greater, then repeat 1 and 2 (and add offset) until less than 200k\n",
    "3) Write the entries to datajoint\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def fix_dataframe_and_return_dict(df):\n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    df[[\"centroid_x\",\"centroid_y\",\"centroid_z\"]] = pd.DataFrame(df[\"ctr_pt_position\"].tolist(),index=df.index)\n",
    "    df_renamed = df.rename(columns=dict(pre_pt_root_id=\"presyn\",post_pt_root_id=\"postsyn\",id=\"synapse_id\"))\n",
    "    B = df_renamed.drop([\"valid\",\"pre_pt_position\",\"ctr_pt_position\",\"post_pt_position\"],axis=1)\n",
    "    B = B.astype({'size': \"int32\"})\n",
    "    #print(B.dtypes)\n",
    "    return B.to_dict(\"records\")\n",
    "\n",
    "\n",
    "\n",
    "verbose = False\n",
    "\n",
    "step_size = 10\n",
    "max_return_size = 200000\n",
    "total_ids = soma_segment_ids\n",
    "n_iterations = int(np.ceil(len(total_ids)/step_size))\n",
    "\n",
    "\n",
    "for i in tqdm(1043,range(n_iterations)):\n",
    "    if i == (n_iterations - 1):\n",
    "        curr_seg_ids =  total_ids[i*step_size:]\n",
    "    else:\n",
    "        curr_seg_ids = total_ids[i*step_size:(i+1)*step_size]\n",
    "        \n",
    "    total_dicts = []\n",
    "    for filter_type in [\"pre_pt_root_id\",\"post_pt_root_id\"]:\n",
    "        curr_filter_dict = {filter_type:curr_seg_ids}\n",
    "        curr_offset = 0\n",
    "        #print(f\"curr_filter_dict = {curr_filter_dict}\")\n",
    "        while True:\n",
    "            df = client.materialize.query_table('synapses_pni_2',\n",
    "                                                #filter_in_dict=dict(func_id=[58941,16062]),\n",
    "                                                filter_in_dict=curr_filter_dict, #just excludes those\n",
    "\n",
    "                                                #filter_equal_dict=dict(func_id=58941),\n",
    "                                               offset=curr_offset ,#this is an integer that will set the offset\n",
    "                                                #select_columns=columns_we_want\n",
    "                                               )\n",
    "            \n",
    "            df_dicts = fix_dataframe_and_return_dict(df)\n",
    "            total_dicts += df_dicts\n",
    "            if len(df_dicts) < max_return_size:\n",
    "                break\n",
    "            else:\n",
    "                curr_offset += max_return_size\n",
    "    \n",
    "    #write the data to the datajoint table\n",
    "    if len(total_dicts) > 0:\n",
    "        if verbose:\n",
    "            print(f\"Inserting {len(total_dicts)} dictionaries \")\n",
    "        SynapseTest.insert(total_dicts,skip_duplicates=True)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Skipping dictionary insertion because it was empty\")\n",
    "            \n",
    "    \n",
    "    #raise Exception(\"\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datajoint table to dump the results into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m65.Synapse.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'synapse_id': 1133000,\n",
    "  'pre_pt_supervoxel_id': 74535343491123341,\n",
    "  'presyn': 864691134742108478,\n",
    "  'post_pt_supervoxel_id': 74535343491123341,\n",
    "  'postsyn': 864691134742108478,\n",
    "  'size': 3316.0,\n",
    "  'centroid_x': 70902,\n",
    "  'centroid_y': 171240,\n",
    "  'centroid_z': 14887},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class SynapseTest(dj.Manual):\n",
    "    definition=\"\"\"\n",
    "    synapse_id: bigint unsigned              # synapse index within the segmentation\n",
    "    ---\n",
    "    pre_pt_supervoxel_id: bigint unsigned\n",
    "    presyn: bigint unsigned\n",
    "    post_pt_supervoxel_id: bigint unsigned\n",
    "    postsyn: bigint unsigned\n",
    "    \n",
    "    centroid_x: bigint unsigned\n",
    "    centroid_y: bigint unsigned\n",
    "    centroid_z: bigint unsigned\n",
    "    \n",
    "    size                 : int unsigned                 # (EM voxels)\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
