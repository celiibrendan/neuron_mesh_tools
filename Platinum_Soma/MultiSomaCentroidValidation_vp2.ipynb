{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose: To package up the Soma Finder into a Robust Module that can be used on the cluster \\n(No file location errors)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: To package up the Soma Finder into a Robust Module that can be used on the cluster \n",
    "(No file location errors)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "from whole_neuron_classifier_datajoint_adapted import extract_branches_whole_neuron\n",
    "import whole_neuron_classifier_datajoint_adapted as wcda \n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import os\n",
    "from meshlab import Decimator , Poisson\n",
    "from pathlib import Path\n",
    "from pykdtree.kdtree import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_mesh = total_soma_list_revised_sphere[0]\n",
    "# #[p for p in dir(ex_mesh). if \"box\" in p] #['bounding_box', 'bounding_box_oriented']\n",
    "# ex_mesh.bounding_box.volume>ex_mesh.bounding_box_oriented.volume\n",
    "# ex_mesh.bounding_box_oriented.is_oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checking the new validation checks\n",
    "\"\"\"\n",
    "def side_length_ratios(current_mesh):\n",
    "    \"\"\"\n",
    "    Will compute the ratios of the bounding box sides\n",
    "    To be later used to see if there is skewness\n",
    "    \"\"\"\n",
    "\n",
    "    # bbox = current_mesh.bounding_box_oriented.vertices\n",
    "    bbox = current_mesh.bounding_box_oriented.vertices\n",
    "    x_axis_unique = np.unique(bbox[:,0])\n",
    "    y_axis_unique = np.unique(bbox[:,1])\n",
    "    z_axis_unique = np.unique(bbox[:,2])\n",
    "    x_length = (np.max(x_axis_unique) - np.min(x_axis_unique)).astype(\"float\")\n",
    "    y_length = (np.max(y_axis_unique) - np.min(y_axis_unique)).astype(\"float\")\n",
    "    z_length = (np.max(z_axis_unique) - np.min(z_axis_unique)).astype(\"float\")\n",
    "    #print(x_length,y_length,z_length)\n",
    "    #compute the ratios:\n",
    "    xy_ratio = float(x_length/y_length)\n",
    "    xz_ratio = float(x_length/z_length)\n",
    "    yz_ratio = float(y_length/z_length)\n",
    "    side_ratios = [xy_ratio,xz_ratio,yz_ratio]\n",
    "    flipped_side_ratios = []\n",
    "    for z in side_ratios:\n",
    "        if z < 1:\n",
    "            flipped_side_ratios.append(1/z)\n",
    "        else:\n",
    "            flipped_side_ratios.append(z)\n",
    "    return flipped_side_ratios\n",
    "\n",
    "def side_length_check(current_mesh,side_length_ratio_threshold=3):\n",
    "    side_length_ratio_names = [\"xy\",\"xz\",\"yz\"]\n",
    "    side_ratios = side_length_ratios(current_mesh)\n",
    "    pass_threshold = [(k <= side_length_ratio_threshold) and\n",
    "                      (k >= 1/side_length_ratio_threshold) for k in side_ratios]\n",
    "    for i,(rt,truth) in enumerate(zip(side_ratios,pass_threshold)):\n",
    "        if not truth:\n",
    "            print(f\"{side_length_ratio_names[i]} = {rt} ratio was beyong {side_length_ratio_threshold} multiplier\")\n",
    "\n",
    "    if False in pass_threshold:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# current_Pmesh = trimesh.load_mesh(\"./12346/12346_poisson_soma.off\")\n",
    "# print(side_length_ratios(current_mesh))\n",
    "# side_length_check(current_mesh,side_length_ratio_threshold=1.55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_mesh_piece(msh):\n",
    "    mesh_splits_inner = msh.split(only_watertight=False)\n",
    "    total_mesh_split_lengths_inner = [len(k.faces) for k in mesh_splits_inner]\n",
    "    ordered_mesh_splits_inner = mesh_splits_inner[np.flip(np.argsort(total_mesh_split_lengths_inner))]\n",
    "    return ordered_mesh_splits_inner[0]\n",
    "def soma_volume_ratio(current_mesh):\n",
    "    \"\"\"\n",
    "    bounding_box_oriented: rotates the box to be less volume\n",
    "    bounding_box : does not rotate the box and makes it axis aligned\n",
    "    \n",
    "    ** checks to see if closed mesh and if not then make closed **\n",
    "    \"\"\"\n",
    "    poisson_temp_folder = Path.cwd() / \"Poisson_temp\"\n",
    "    poisson_temp_folder.mkdir(parents=True,exist_ok=True)\n",
    "    Poisson_obj_temp = Poisson(poisson_temp_folder,overwrite=True)\n",
    "    \n",
    "    #get the largest piece\n",
    "    lrg_mesh = largest_mesh_piece(current_mesh)\n",
    "    if not lrg_mesh.is_watertight:\n",
    "        print(\"Using Poisson Surface Reconstruction to make mesh watertight\")\n",
    "        #run the Poisson Surface reconstruction and get the largest piece\n",
    "        new_mesh_inner,poisson_file_obj = Poisson_obj_temp(vertices=lrg_mesh.vertices,\n",
    "               faces=lrg_mesh.faces,\n",
    "               return_mesh=True,\n",
    "               delete_temp_files=True)\n",
    "        lrg_mesh = largest_mesh_piece(new_mesh_inner)\n",
    "\n",
    "    #turn the mesh into a closed mesh based on \n",
    "    \n",
    "    ratio_val = lrg_mesh.bounding_box.volume/lrg_mesh.volume\n",
    "#     if ratio_val < 1:\n",
    "#         raise Exception(\"Less than 1 value in volume ratio computation\")\n",
    "    return ratio_val\n",
    "\n",
    "def soma_volume_check(current_mesh,multiplier=8):\n",
    "    ratio_val= soma_volume_ratio(current_mesh)\n",
    "    print(\"Inside sphere validater: ratio_val = \" + str(ratio_val))\n",
    "    if np.abs(ratio_val) > multiplier:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The actual soma extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "\n",
    "my_segment = 107738877133006848\n",
    "#my_segment = 12347 #The dendrite branch\n",
    "#my_segment = 12346 #the small glia\n",
    "bc_mesh = trimesh.load_mesh(f\"{my_segment}/{my_segment}.off\")\n",
    "\n",
    "segment_id = my_segment\n",
    "current_mesh_verts = bc_mesh.vertices\n",
    "current_mesh_faces = bc_mesh.faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 10000.0 \n",
      "large_mesh_threshold_inner = 10000.0 \n",
      "soma_size_threshold = 1250.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848.off -o /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated.off -s /notebooks/Platinum_Soma/decimation_meshlab_25.mls\n",
      "Total found significant pieces before Poisson = [<trimesh.Trimesh(vertices.shape=(327432, 3), faces.shape=(682632, 3))>]\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(327432, 3), faces.shape=(682632, 3))>\n",
      "pre_largest_mesh_path = /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece.off\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece.off -o /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson.off -s /notebooks/Platinum_Soma/poisson.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(284813, 3), faces.shape=(570782, 3))>, <trimesh.Trimesh(vertices.shape=(121397, 3), faces.shape=(243354, 3))>, <trimesh.Trimesh(vertices.shape=(44644, 3), faces.shape=(89284, 3))>, <trimesh.Trimesh(vertices.shape=(44175, 3), faces.shape=(88350, 3))>, <trimesh.Trimesh(vertices.shape=(5135, 3), faces.shape=(10278, 3))>]\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(284813, 3), faces.shape=(570782, 3))>\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Platinum_Soma/decimation_meshlab_25.mls\n",
      "done exporting decimated mesh: neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0002741813659667969\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Soma/temp/10773887713300684800_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 48.33978724479675\n",
      "2) Finished: Generating CGAL segmentation for neuron: 50.596587896347046\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.04882311820983887\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'soma', 'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 0.00013589859008789062\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.12786436080932617\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.8019764423370361\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.36339350000000004\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([  0, 110, 134,  96, 151,  22,  94,  30,  16,  69,  86,  49, 103,\n",
      "        62, 153,  27,  39,  32, 101,  64, 126,  61,  71, 143,   4,   5,\n",
      "        34,  51,  74,  58, 142, 118,  45,  75,  20,  59,  18,  72,  54,\n",
      "        11,  19,  17,  63, 100,  40,  70,   6,  73, 136,  87,   1, 128,\n",
      "        29, 133,  66,  10,  48,   8,  78,  88,  98,  15,  28, 147,  21,\n",
      "         2, 131,  52,   9, 105,  97,   7,  60,  33,  90, 114,  12, 130,\n",
      "        55, 113, 137, 155,  37, 123, 117, 132,  44,  50, 145,  95, 148,\n",
      "        25,  26,  85,  23,  57,  99,  92, 152, 127,   3, 140,  14,  68,\n",
      "        24,  80,  65, 112, 150, 138,  81,  91, 104,  84,  43,  67,  56,\n",
      "       139, 106, 129,  46,  35,  41,  42, 119,  13, 109,  89,  31, 115,\n",
      "       102, 107,  82, 149, 122, 154, 116, 141, 121, 144, 125, 111,  38,\n",
      "        79, 146,  93,  53, 108,  77,  83,  76, 156,  36, 120,  47, 124,\n",
      "       135]), array([0.3633935 , 0.349543  , 0.2580275 , 0.224891  , 0.213862  ,\n",
      "       0.212222  , 0.2035625 , 0.203019  , 0.202248  , 0.201951  ,\n",
      "       0.201747  , 0.198279  , 0.1976305 , 0.1976115 , 0.197223  ,\n",
      "       0.1969    , 0.196402  , 0.19483   , 0.192706  , 0.192594  ,\n",
      "       0.191338  , 0.189455  , 0.18606   , 0.185546  , 0.185121  ,\n",
      "       0.184382  , 0.1843405 , 0.1805335 , 0.180465  , 0.179929  ,\n",
      "       0.1780855 , 0.177563  , 0.1774525 , 0.177228  , 0.17648   ,\n",
      "       0.175166  , 0.166142  , 0.1653575 , 0.162947  , 0.1595275 ,\n",
      "       0.1534625 , 0.151657  , 0.1502085 , 0.149418  , 0.14906   ,\n",
      "       0.148988  , 0.147093  , 0.147091  , 0.146846  , 0.1467235 ,\n",
      "       0.1462715 , 0.146034  , 0.145847  , 0.14506   , 0.1445255 ,\n",
      "       0.1443665 , 0.1441425 , 0.14314   , 0.143059  , 0.14208   ,\n",
      "       0.141204  , 0.141039  , 0.140836  , 0.140496  , 0.1393635 ,\n",
      "       0.139361  , 0.138815  , 0.138745  , 0.138505  , 0.138429  ,\n",
      "       0.136383  , 0.1358305 , 0.134662  , 0.134625  , 0.133631  ,\n",
      "       0.132983  , 0.132846  , 0.132754  , 0.132594  , 0.132246  ,\n",
      "       0.130764  , 0.1280615 , 0.127706  , 0.12759   , 0.125046  ,\n",
      "       0.124399  , 0.1243905 , 0.1242765 , 0.123122  , 0.122703  ,\n",
      "       0.122108  , 0.121031  , 0.120917  , 0.120567  , 0.119991  ,\n",
      "       0.1185    , 0.118467  , 0.11797   , 0.1177645 , 0.1176835 ,\n",
      "       0.11751   , 0.115218  , 0.113421  , 0.112704  , 0.112448  ,\n",
      "       0.1117405 , 0.111329  , 0.110954  , 0.110439  , 0.108793  ,\n",
      "       0.106758  , 0.106041  , 0.105534  , 0.10528   , 0.103435  ,\n",
      "       0.103381  , 0.100167  , 0.0999749 , 0.0998846 , 0.0994604 ,\n",
      "       0.0986397 , 0.0962986 , 0.0955803 , 0.0928282 , 0.09210555,\n",
      "       0.0898449 , 0.0898197 , 0.08956435, 0.0892027 , 0.0880139 ,\n",
      "       0.08776785, 0.0872024 , 0.086992  , 0.0854588 , 0.0839236 ,\n",
      "       0.0838867 , 0.0825371 , 0.0824227 , 0.0816899 , 0.0816117 ,\n",
      "       0.08129855, 0.0812092 , 0.0803146 , 0.0798021 , 0.0791757 ,\n",
      "       0.0783427 , 0.07591665, 0.0737151 , 0.0723412 , 0.07083715,\n",
      "       0.068692  , 0.0565517 , 0.0522073 , 0.0479906 , 0.0455877 ,\n",
      "       0.0392372 , 0.0331407 ]))\n",
      "Sizes = [20140, 181, 128, 918, 119, 898, 1366, 5946, 357, 181, 169, 1734, 1412, 182, 3188, 137, 231, 2376, 253, 2526, 138, 1475, 4548, 113, 5906, 441, 594, 404, 111, 4656, 580, 175, 204, 71, 1683, 721, 879, 412, 1254, 732, 706, 801, 300, 400, 2719, 1122, 3360, 103, 899, 1674, 1622, 435, 2093, 356, 1388, 3622, 4864, 1151, 81, 611, 1997, 7258, 176, 313, 1586, 1827, 2093, 321, 1980, 370, 251, 2552, 179, 1141, 1183, 53, 229, 1607, 1202, 111, 241, 414, 164, 45, 91, 1602, 474, 664, 139, 329, 147, 87, 174, 225, 1306, 454, 195, 172, 144, 676, 145, 178, 150, 87, 7072, 92, 539, 81, 182, 109, 203, 222, 77, 95, 232, 129, 29, 397, 295, 43, 27, 35, 193, 39, 144, 50, 55, 106, 65, 60, 138, 42, 59, 65, 27, 53, 83, 135, 33, 113, 84, 62, 27, 25, 35, 53, 62, 67, 17, 92, 27, 27, 37, 33, 39, 57, 53]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "Using Poisson Surface Reconstruction to make mesh watertight\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off -o /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off -s /notebooks/Platinum_Soma/poisson.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed temporary input file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off\n",
      "removed temporary output file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off\n",
      "Inside sphere validater: ratio_val = 67.73408606998751\n",
      "--->This soma mesh was not added because it did not pass the sphere validation: <trimesh.Trimesh(vertices.shape=(10261, 3), faces.shape=(20140, 3))>\n",
      "----- working on mesh after poisson #1: <trimesh.Trimesh(vertices.shape=(121397, 3), faces.shape=(243354, 3))>\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Platinum_Soma/decimation_meshlab_25.mls\n",
      "done exporting decimated mesh: neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00024127960205078125\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Soma/temp/10773887713300684801_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 14.12106728553772\n",
      "2) Finished: Generating CGAL segmentation for neuron: 15.019776105880737\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 2\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.02161693572998047\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'soma', 'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 4.601478576660156e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.055991172790527344\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.4011218547821045\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.669756\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 2,  1, 31, 19, 35,  9, 34, 25, 12,  3, 32, 21, 22,  4, 15,  0, 37,\n",
      "        7,  8, 20, 33, 36, 29, 11, 23, 18, 24, 13,  5, 14, 38, 17, 39, 16,\n",
      "       27, 26,  6, 10, 30, 28]), array([0.669756  , 0.191151  , 0.1545475 , 0.149287  , 0.1428195 ,\n",
      "       0.137335  , 0.133648  , 0.12575   , 0.0990114 , 0.0924162 ,\n",
      "       0.09050895, 0.0851386 , 0.0841591 , 0.0820595 , 0.08179415,\n",
      "       0.0782787 , 0.07667025, 0.074077  , 0.0712647 , 0.0711911 ,\n",
      "       0.0710468 , 0.070076  , 0.0686449 , 0.0685909 , 0.0685367 ,\n",
      "       0.0684699 , 0.06735735, 0.0620891 , 0.0613888 , 0.0568314 ,\n",
      "       0.0533108 , 0.052925  , 0.0526946 , 0.0500996 , 0.0498679 ,\n",
      "       0.0469414 , 0.0448609 , 0.0387951 , 0.029109  , 0.0148797 ]))\n",
      "Sizes = [4383, 6876, 362, 131, 558, 5779, 71, 19, 1995, 2929, 506, 745, 128, 1531, 906, 29630, 108, 55, 267, 207, 47, 21, 571, 842, 389, 159, 62, 195, 259, 280, 141, 53, 72, 157, 49, 91, 99, 105, 25, 35]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [2]\n",
      "Using Poisson Surface Reconstruction to make mesh watertight\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off -o /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off -s /notebooks/Platinum_Soma/poisson.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed temporary input file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off\n",
      "removed temporary output file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off\n",
      "Inside sphere validater: ratio_val = 2.878097383240724\n",
      "----- working on mesh after poisson #2: <trimesh.Trimesh(vertices.shape=(44644, 3), faces.shape=(89284, 3))>\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Platinum_Soma/decimation_meshlab_25.mls\n",
      "done exporting decimated mesh: neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00020503997802734375\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Soma/temp/10773887713300684802_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 2.0688209533691406\n",
      "2) Finished: Generating CGAL segmentation for neuron: 2.3927204608917236\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.0077402591705322266\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'soma', 'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 0.00015807151794433594\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.019246578216552734\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.11016345024108887\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.88391\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 0,  1,  2,  9, 13, 12,  4, 10, 14,  7,  6,  3,  8,  5, 11]), array([0.88391   , 0.3791735 , 0.207295  , 0.1382155 , 0.079689  ,\n",
      "       0.0738344 , 0.06865105, 0.0648424 , 0.0585631 , 0.0561739 ,\n",
      "       0.05012795, 0.04851155, 0.0448781 , 0.0399123 , 0.0372798 ]))\n",
      "Sizes = [3044, 460, 687, 248, 2523, 397, 4094, 2270, 665, 921, 1596, 2772, 1321, 1031, 291]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "Using Poisson Surface Reconstruction to make mesh watertight\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off -o /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off -s /notebooks/Platinum_Soma/poisson.mls\n",
      "removed temporary input file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off\n",
      "removed temporary output file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off\n",
      "Inside sphere validater: ratio_val = 2.491117703240259\n",
      "----- working on mesh after poisson #3: <trimesh.Trimesh(vertices.shape=(44175, 3), faces.shape=(88350, 3))>\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Platinum_Soma/decimation_meshlab_25.mls\n",
      "done exporting decimated mesh: neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0001926422119140625\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Soma/temp/10773887713300684803_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 1.9866247177124023\n",
      "2) Finished: Generating CGAL segmentation for neuron: 2.3079216480255127\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 8\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.007731199264526367\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'soma', 'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 5.626678466796875e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.019540071487426758\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.1054384708404541\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.813247\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 8, 11,  9, 12,  6,  7, 10,  5, 13,  3,  2,  4,  0,  1, 15, 14]), array([0.813247  , 0.1122015 , 0.1017345 , 0.0983404 , 0.0875427 ,\n",
      "       0.05975405, 0.0577842 , 0.05605775, 0.0533564 , 0.0521652 ,\n",
      "       0.0437163 , 0.0427301 , 0.0420405 , 0.0401894 , 0.0368083 ,\n",
      "       0.0235886 ]))\n",
      "Sizes = [3833, 492, 3500, 357, 1419, 296, 229, 2786, 2246, 1885, 385, 1228, 767, 2551, 49, 63]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [8]\n",
      "Using Poisson Surface Reconstruction to make mesh watertight\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off -o /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off -s /notebooks/Platinum_Soma/poisson.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed temporary input file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off\n",
      "removed temporary output file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off\n",
      "Inside sphere validater: ratio_val = 3.196828682227806\n",
      "----- working on mesh after poisson #4: <trimesh.Trimesh(vertices.shape=(5135, 3), faces.shape=(10278, 3))>\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Platinum_Soma/107738877133006848/neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Platinum_Soma/decimation_meshlab_25.mls\n",
      "done exporting decimated mesh: neuron_107738877133006848_decimated_largest_piece_poisson_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00020599365234375\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Soma/temp/10773887713300684804_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.24676132202148438\n",
      "2) Finished: Generating CGAL segmentation for neuron: 0.2882246971130371\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4]\n",
      "soma_index = -1\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.0009355545043945312\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 4.57763671875e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.002607107162475586\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.012540578842163086\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([0, 3, 2, 1, 4]), array([0.488016 , 0.140605 , 0.0934171, 0.055946 , 0.0270915]))\n",
      "Sizes = [2445, 63, 35, 14, 11]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "Using Poisson Surface Reconstruction to make mesh watertight\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off -o /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off -s /notebooks/Platinum_Soma/poisson.mls\n",
      "removed temporary input file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None.off\n",
      "removed temporary output file: /notebooks/Platinum_Soma/Poisson_temp/neuron_None_poisson.off\n",
      "Inside sphere validater: ratio_val = 27.3146540589436\n",
      "--->This soma mesh was not added because it did not pass the sphere validation: <trimesh.Trimesh(vertices.shape=(1225, 3), faces.shape=(2445, 3))>\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 302.8691849708557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNeed to delete all files in the temp folder *****\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "global_start_time = time.time()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "outer_decimation_ratio= 0.25\n",
    "#large_mesh_threshold = 70000 ** this was working before but dropping down\n",
    "large_mesh_threshold = 40000\n",
    "large_mesh_threshold_inner = 40000\n",
    "soma_width_threshold = 0.32\n",
    "soma_size_threshold = 20000\n",
    "\n",
    "large_mesh_threshold = large_mesh_threshold*outer_decimation_ratio\n",
    "large_mesh_threshold_inner = large_mesh_threshold_inner*outer_decimation_ratio\n",
    "soma_size_threshold = soma_size_threshold*outer_decimation_ratio\n",
    "\n",
    "inner_decimation_ratio = 0.25\n",
    "soma_size_threshold = soma_size_threshold*inner_decimation_ratio\n",
    "\n",
    "print(f\"Current Arguments Using (adjusted for decimation):\\n large_mesh_threshold= {large_mesh_threshold}\"\n",
    "             f\" \\nlarge_mesh_threshold_inner = {large_mesh_threshold_inner} \\nsoma_size_threshold = {soma_size_threshold}\"\n",
    "             f\"\\nouter_decimation_ratio = {outer_decimation_ratio}\"\n",
    "             f\"\\ninner_decimation_ratio = {inner_decimation_ratio}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "temp_folder = f\"./{segment_id}\"\n",
    "temp_object = Path(temp_folder)\n",
    "#make the temp folder if it doesn't exist\n",
    "temp_object.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "#making the decimation and poisson objections\n",
    "Dec_outer = Decimator(outer_decimation_ratio,temp_folder,overwrite=True)\n",
    "Dec_inner = Decimator(inner_decimation_ratio,temp_folder,overwrite=True)\n",
    "Poisson_obj = Poisson(temp_folder,overwrite=True)\n",
    "\n",
    "#Step 1: Decimate the Mesh and then split into the seperate pieces\n",
    "new_mesh,output_obj = Dec_outer(vertices=current_mesh_verts,\n",
    "         faces=current_mesh_faces,\n",
    "         segment_id=segment_id,\n",
    "         return_mesh=True,\n",
    "         delete_temp_files=False)\n",
    "\n",
    "#preforming the splits of the decimated mesh\n",
    "\n",
    "mesh_splits = new_mesh.split(only_watertight=False)\n",
    "\n",
    "#get the largest mesh\n",
    "mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "\n",
    "\n",
    "total_mesh_split_lengths = [len(k.faces) for k in mesh_splits]\n",
    "ordered_mesh_splits = mesh_splits[np.flip(np.argsort(total_mesh_split_lengths))]\n",
    "list_of_largest_mesh = [k for k in ordered_mesh_splits if len(k.faces) > large_mesh_threshold]\n",
    "\n",
    "print(f\"Total found significant pieces before Poisson = {list_of_largest_mesh}\")\n",
    "\n",
    "#if no significant pieces were found then will use smaller threshold\n",
    "if len(list_of_largest_mesh)<=0:\n",
    "    print(f\"Using smaller large_mesh_threshold because no significant pieces found with {large_mesh_threshold}\")\n",
    "    list_of_largest_mesh = [k for k in ordered_mesh_splits if len(k.faces) > large_mesh_threshold/2]\n",
    "\n",
    "total_soma_list = []\n",
    "total_classifier_list = []\n",
    "total_poisson_list = []\n",
    "\n",
    "#start iterating through where go through all pieces before the poisson reconstruction\n",
    "no_somas_found_in_big_loop = 0\n",
    "for i,largest_mesh in enumerate(list_of_largest_mesh):\n",
    "    print(f\"----- working on large mesh #{i}: {largest_mesh}\")\n",
    "\n",
    "    somas_found_in_big_loop = False\n",
    "    \n",
    "    largest_file_name = str(output_obj.stem) + \"_largest_piece.off\"\n",
    "    pre_largest_mesh_path = temp_object / Path(str(output_obj.stem) + \"_largest_piece.off\")\n",
    "    pre_largest_mesh_path = pre_largest_mesh_path.absolute()\n",
    "    print(f\"pre_largest_mesh_path = {pre_largest_mesh_path}\")\n",
    "    \n",
    "    new_mesh_inner,poisson_file_obj = Poisson_obj(vertices=largest_mesh.vertices,\n",
    "               faces=largest_mesh.faces,\n",
    "               return_mesh=True,\n",
    "               mesh_filename=largest_file_name,\n",
    "               delete_temp_files=False)\n",
    "    \n",
    "    \n",
    "    #splitting the Poisson into the largest pieces and ordering them\n",
    "    mesh_splits_inner = new_mesh_inner.split(only_watertight=False)\n",
    "    total_mesh_split_lengths_inner = [len(k.faces) for k in mesh_splits_inner]\n",
    "    ordered_mesh_splits_inner = mesh_splits_inner[np.flip(np.argsort(total_mesh_split_lengths_inner))]\n",
    "\n",
    "    list_of_largest_mesh_inner = [k for k in ordered_mesh_splits_inner if len(k.faces) > large_mesh_threshold_inner]\n",
    "    print(f\"Total found significant pieces AFTER Poisson = {list_of_largest_mesh_inner}\")\n",
    "\n",
    "    n_failed_inner_soma_loops = 0\n",
    "    for j, largest_mesh_inner in enumerate(list_of_largest_mesh_inner):\n",
    "        print(f\"----- working on mesh after poisson #{j}: {largest_mesh_inner}\")\n",
    "    \n",
    "        largest_mesh_path_inner = str(poisson_file_obj.stem) + \"_largest_inner.off\"\n",
    "        \n",
    "        #Decimate the inner poisson piece\n",
    "        largest_mesh_path_inner_decimated,output_obj_inner = Dec_inner(\n",
    "                            vertices=largest_mesh_inner.vertices,\n",
    "                             faces=largest_mesh_inner.faces,\n",
    "                            mesh_filename=largest_mesh_path_inner,\n",
    "                             return_mesh=True,\n",
    "                             delete_temp_files=False)\n",
    "        \n",
    "        print(f\"done exporting decimated mesh: {largest_mesh_path_inner}\")\n",
    "        \n",
    "        faces = np.array(largest_mesh_path_inner_decimated.faces)\n",
    "        verts = np.array(largest_mesh_path_inner_decimated.vertices)\n",
    "        \n",
    "        segment_id_new = int(str(segment_id) + f\"{i}{j}\")\n",
    "        \n",
    "        verts_labels, faces_labels, soma_value,classifier = wcda.extract_branches_whole_neuron(\n",
    "                                import_Off_Flag=False,\n",
    "                                segment_id=segment_id_new,\n",
    "                                vertices=verts,\n",
    "                                 triangles=faces,\n",
    "                                pymeshfix_Flag=False,\n",
    "                                 import_CGAL_Flag=False,\n",
    "                                 return_Only_Labels=True,\n",
    "                                 clusters=3,\n",
    "                                 smoothness=0.2,\n",
    "                                soma_only=True,\n",
    "                                return_classifier = True\n",
    "                                )\n",
    "        print(f\"soma_sdf_value = {soma_value}\")\n",
    "        \n",
    "        total_classifier_list.append(classifier)\n",
    "        #total_poisson_list.append(largest_mesh_path_inner_decimated)\n",
    "\n",
    "        # Save all of the portions that resemble a soma\n",
    "        median_values = np.array([v[\"median\"] for k,v in classifier.sdf_final_dict.items()])\n",
    "        segmentation = np.array([k for k,v in classifier.sdf_final_dict.items()])\n",
    "\n",
    "        #order the compartments by greatest to smallest\n",
    "        sorted_medians = np.flip(np.argsort(median_values))\n",
    "        print(f\"segmentation[sorted_medians],median_values[sorted_medians] = {(segmentation[sorted_medians],median_values[sorted_medians])}\")\n",
    "        print(f\"Sizes = {[classifier.sdf_final_dict[g]['n_faces'] for g in segmentation[sorted_medians]]}\")\n",
    "\n",
    "        valid_soma_segments_width = [g for g,h in zip(segmentation[sorted_medians],median_values[sorted_medians]) if ((h > soma_width_threshold)\n",
    "                                                            and (classifier.sdf_final_dict[g][\"n_faces\"] > soma_size_threshold))]\n",
    "\n",
    "        print(\"valid_soma_segments_width\")\n",
    "        to_add_list = []\n",
    "        if len(valid_soma_segments_width) > 0:\n",
    "            print(f\"      ------ Found {len(valid_soma_segments_width)} viable somas: {valid_soma_segments_width}\")\n",
    "            somas_found_in_big_loop = True\n",
    "            #get the meshes only if signfiicant length\n",
    "            labels_list = classifier.labels_list\n",
    "\n",
    "            for v in valid_soma_segments_width:\n",
    "                submesh_face_list = np.where(classifier.labels_list == v)[0]\n",
    "                soma_mesh = largest_mesh_path_inner_decimated.submesh([submesh_face_list],append=True)\n",
    "\n",
    "                if side_length_check(soma_mesh) and soma_volume_check(soma_mesh):\n",
    "                    to_add_list.append(soma_mesh)\n",
    "                else:\n",
    "                    print(f\"--->This soma mesh was not added because it did not pass the sphere validation: {soma_mesh}\")\n",
    "\n",
    "            n_failed_inner_soma_loops = 0\n",
    "\n",
    "        else:\n",
    "            n_failed_inner_soma_loops += 1\n",
    "\n",
    "        total_soma_list += to_add_list\n",
    "\n",
    "        # --------------- KEEP TRACK IF FAILED TO FIND SOMA (IF TOO MANY FAILS THEN BREAK)\n",
    "        if n_failed_inner_soma_loops >= 2:\n",
    "            print(\"breaking inner loop because 2 soma fails in a row\")\n",
    "            break\n",
    "\n",
    "\n",
    "    # --------------- KEEP TRACK IF FAILED TO FIND SOMA (IF TOO MANY FAILS THEN BREAK)\n",
    "    if somas_found_in_big_loop == False:\n",
    "        no_somas_found_in_big_loop += 1\n",
    "        if no_somas_found_in_big_loop >= 2:\n",
    "            print(\"breaking because 2 fails in a row in big loop\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        no_somas_found_in_big_loop = 0\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    large_mesh_threshold= 150000.0 \n",
    "    large_mesh_threshold_inner = 10000.0 \n",
    "    soma_size_threshold = 1250.0\n",
    "    decimation_ratio = 0.05\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\" IF THERE ARE MULTIPLE SOMAS THAT ARE WITHIN A CERTAIN DISTANCE OF EACH OTHER THEN JUST COMBINE THEM INTO ONE\"\"\"\n",
    "pairings = []\n",
    "for y,soma_1 in enumerate(total_soma_list):\n",
    "    for z,soma_2 in enumerate(total_soma_list):\n",
    "        if y<z:\n",
    "            mesh_tree = KDTree(soma_1.vertices)\n",
    "            distances,closest_node = mesh_tree.query(soma_2.vertices)\n",
    "\n",
    "            if np.min(distances) < 4000:\n",
    "                pairings.append([y,z])\n",
    "\n",
    "\n",
    "#creating the combined meshes from the list\n",
    "total_soma_list_revised = []\n",
    "if len(pairings) > 0:\n",
    "    \"\"\"\n",
    "    Pseudocode: \n",
    "    Use a network function to find components\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    import networkx as nx\n",
    "    new_graph = nx.Graph()\n",
    "    new_graph.add_edges_from(pairings)\n",
    "    grouped_somas = list(nx.connected_components(new_graph))\n",
    "\n",
    "    somas_being_combined = []\n",
    "    print(f\"There were soma pariings: Connected components in = {grouped_somas} \")\n",
    "    for comp in grouped_somas:\n",
    "        comp = list(comp)\n",
    "        somas_being_combined += list(comp)\n",
    "        current_mesh = total_soma_list[comp[0]]\n",
    "        for i in range(1,len(comp)):\n",
    "            current_mesh += total_soma_list[comp[i]]\n",
    "\n",
    "        total_soma_list_revised.append(current_mesh)\n",
    "\n",
    "    #add those that weren't combined to total_soma_list_revised\n",
    "    leftover_somas = [total_soma_list[k] for k in range(0,len(total_soma_list)) if k not in somas_being_combined]\n",
    "    if len(leftover_somas) > 0:\n",
    "        total_soma_list_revised += leftover_somas\n",
    "    print(f\"Final total_soma_list_revised = {total_soma_list_revised}\")\n",
    "\n",
    "    total_soma_list_revised\n",
    "\n",
    "if len(total_soma_list_revised) == 0:\n",
    "    total_soma_list_revised = total_soma_list\n",
    "\n",
    "# # run through the sphere validator again (DON'T THINK WE NEED THIS ANYMORE)\n",
    "# print(f\"BEFORE last step sphere validation number of somas = {len(total_soma_list_revised)}\")\n",
    "# total_soma_list_revised_sphere = [m for m in total_soma_list_revised if validate_soma_by_sphere(m)]\n",
    "# print(f\"AFTER last step sphere validation number of somas = {len(total_soma_list_revised_sphere)}\")\n",
    "\n",
    "    \n",
    "run_time = time.time() - global_start_time\n",
    "\n",
    "print(f\"\\n\\n\\n Total time for run = {time.time() - global_start_time}\")\n",
    "\n",
    "#need to erase all of the temporary files ******\n",
    "#import shutil\n",
    "#shutil.rmtree(directory)\n",
    "\n",
    "\"\"\"\n",
    "Need to delete all files in the temp folder *****\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#return total_soma_list, run_time\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "final_soma_mesh = trimesh.Trimesh(vertices=np.array([]),\n",
    "                                 faces=np.array([]))\n",
    "for sm in total_soma_list_revised:\n",
    "    final_soma_mesh += sm\n",
    "final_soma_mesh.export(f\"{temp_folder}/{segment_id}_final_soma_meshes.off\")\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<trimesh.Trimesh(vertices.shape=(2281, 3), faces.shape=(4383, 3))>,\n",
       " <trimesh.Trimesh(vertices.shape=(1568, 3), faces.shape=(3044, 3))>,\n",
       " <trimesh.Trimesh(vertices.shape=(1936, 3), faces.shape=(3833, 3))>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_soma_list_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
