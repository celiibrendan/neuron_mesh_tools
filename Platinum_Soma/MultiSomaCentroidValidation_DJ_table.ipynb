{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose: Creates the table that will store the soma centroid from meshes used for testing\\n\\nPseudocode: \\n1) Pull down the mesh\\n2) Create a folder where all of the mesh pieces will be stored\\n3) Run the soma extraction to get the soma meshes\\n4) Calculate the soma centers and save the distance from the predicted soma centers\\n5) Things then to save off:\\n- index of the soma (based on tehe number of somas found for that one mesh)\\n- the soma center\\n- Distance from predicted soma center\\n- Mesh of the soma\\n6) Delete all the temporary files that were used for that certain mesh\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: Creates the table that will store the soma centroid from meshes used for testing\n",
    "\n",
    "Pseudocode: \n",
    "1) Pull down the mesh\n",
    "2) Create a folder where all of the mesh pieces will be stored\n",
    "3) Run the soma extraction to get the soma meshes\n",
    "4) Calculate the soma centers and save the distance from the predicted soma centers\n",
    "5) Things then to save off:\n",
    "- index of the soma (based on tehe number of somas found for that one mesh)\n",
    "- the soma center\n",
    "- Distance from predicted soma center\n",
    "- Mesh of the soma\n",
    "6) Delete all the temporary files that were used for that certain mesh\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "import datajoint as dj\n",
    "import numpy as np\n",
    "m65 = dj.create_virtual_module('m65', 'microns_minnie65_01')\n",
    "schema = dj.schema(\"microns_minnie65_01\")\n",
    "dj.config[\"display.limit\"] = 30\n",
    "\n",
    "import minfig\n",
    "minnie = minfig.configure_minnie(return_virtual_module=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "from whole_neuron_classifier_datajoint_adapted import extract_branches_whole_neuron\n",
    "import whole_neuron_classifier_datajoint_adapted as wcda \n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "        <style type=\"text/css\">\n",
       "            .Relation{\n",
       "                border-collapse:collapse;\n",
       "            }\n",
       "            .Relation th{\n",
       "                background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "                font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "            }\n",
       "            .Relation td{\n",
       "                padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "            }\n",
       "            .Relation tr:nth-child(odd){\n",
       "                background: #ffffff;\n",
       "            }\n",
       "            .Relation tr:nth-child(even){\n",
       "                background: #f3f1ff;\n",
       "            }\n",
       "            /* Tooltip container */\n",
       "            .djtooltip {\n",
       "            }\n",
       "            /* Tooltip text */\n",
       "            .djtooltip .djtooltiptext {\n",
       "                visibility: hidden;\n",
       "                width: 120px;\n",
       "                background-color: black;\n",
       "                color: #fff;\n",
       "                text-align: center;\n",
       "                padding: 5px 0;\n",
       "                border-radius: 6px;\n",
       "                /* Position the tooltip text - see examples below! */\n",
       "                position: absolute;\n",
       "                z-index: 1;\n",
       "            }\n",
       "            #primary {\n",
       "                font-weight: bold;\n",
       "                color: black;\n",
       "            }\n",
       "\n",
       "            #nonprimary {\n",
       "                font-weight: normal;\n",
       "                color: white;\n",
       "            }\n",
       "\n",
       "            /* Show the tooltip text when you mouse over the tooltip container */\n",
       "            .djtooltip:hover .djtooltiptext {\n",
       "                visibility: visible;\n",
       "            }\n",
       "        </style>\n",
       "        \n",
       "        <b>Decimated meshes</b>\n",
       "            <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "            <table border=\"1\" class=\"Relation\">\n",
       "                <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">segment_id</p>\n",
       "                                <span class=\"djtooltiptext\">segment id unique within each Segmentation</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">version</p>\n",
       "                                <span class=\"djtooltiptext\"></span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">decimation_ratio</p>\n",
       "                                <span class=\"djtooltiptext\">ratio of remaining mesh vertices/faces (which ones depends on what metric the decimation technique uses)</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">n_vertices</p>\n",
       "                                <span class=\"djtooltiptext\"></span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">n_faces</p>\n",
       "                                <span class=\"djtooltiptext\"></span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">mesh</p>\n",
       "                                <span class=\"djtooltiptext\">in-place path to the hdf5 (decimated) mesh file</span>\n",
       "                            </div> </th> </tr> </thead>\n",
       "                <tbody> <tr> <td>77489456693007645</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td>\n",
       "<td>218373</td>\n",
       "<td>438232</td>\n",
       "<td>=BLOB=</td></tr><tr><td>90153356676844854</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td>\n",
       "<td>544744</td>\n",
       "<td>1090856</td>\n",
       "<td>=BLOB=</td></tr><tr><td>90518669414951752</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td>\n",
       "<td>147610</td>\n",
       "<td>296288</td>\n",
       "<td>=BLOB=</td></tr><tr><td>91208269699305759</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td>\n",
       "<td>1550148</td>\n",
       "<td>3100434</td>\n",
       "<td>=BLOB=</td></tr><tr><td>96147481821744505</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td>\n",
       "<td>110154</td>\n",
       "<td>220298</td>\n",
       "<td>=BLOB=</td></tr><tr><td>107319757380005445</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td>\n",
       "<td>385607</td>\n",
       "<td>774152</td>\n",
       "<td>=BLOB=</td></tr><tr><td>107681564887986552</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td>\n",
       "<td>1455588</td>\n",
       "<td>2922789</td>\n",
       "<td>=BLOB=</td></tr><tr><td>107738877133006848</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td>\n",
       "<td>1361540</td>\n",
       "<td>2746652</td>\n",
       "<td>=BLOB=</td> </tr> </tbody>\n",
       "            </table>\n",
       "            \n",
       "            <p>Total: 8</p></div>\n",
       "            "
      ],
      "text/plain": [
       "*segment_id    *version    *decimation_ra n_vertices     n_faces     mesh      \n",
       "+------------+ +---------+ +------------+ +------------+ +---------+ +--------+\n",
       "77489456693007 0           0.25           218373         438232      =BLOB=    \n",
       "90153356676844 0           0.25           544744         1090856     =BLOB=    \n",
       "90518669414951 0           0.25           147610         296288      =BLOB=    \n",
       "91208269699305 0           0.25           1550148        3100434     =BLOB=    \n",
       "96147481821744 0           0.25           110154         220298      =BLOB=    \n",
       "10731975738000 0           0.25           385607         774152      =BLOB=    \n",
       "10768156488798 0           0.25           1455588        2922789     =BLOB=    \n",
       "10773887713300 0           0.25           1361540        2746652     =BLOB=    \n",
       " (Total: 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minnie.Decimation() & (m65.AllenMultiSomas() & \"status > 0\").proj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate_mesh_from_verts_faces(vertices,faces,segment_id,current_folder,decimation_ratio):\n",
    "    #write the file to the temp folder\n",
    "    input_file_base = write_Whole_Neuron_Off_file(vertices,faces,segment_id,folder=current_folder)\n",
    "    output_file = input_file_base + \"_decimated\"\n",
    "    \n",
    "    \n",
    "    if decimation_ratio <= 0:\n",
    "        script_name = \"decimation_meshlab_0_25.mls\"\n",
    "    else:\n",
    "        script_name = f\"decimation_meshlab_0_{str(decimation_ratio)[2:4]}.mls\"\n",
    "    meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "\n",
    "\n",
    "    meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_file_base + \".off\",\n",
    "                                                       output_path_and_filename=output_file + \".off\",\n",
    "                                                       meshlab_script=meshlab_script_path_and_name)\n",
    "    \n",
    "    #read in the output mesh and return the vertices and faces\n",
    "    current_mesh = trimesh.load_mesh(output_file + '.off')\n",
    "    \n",
    "    #check if file exists and then delete the temporary decimated mesh filess\n",
    "    if os.path.exists(input_file_base + \".off\"):\n",
    "        os.remove(input_file_base + \".off\")\n",
    "    if os.path.exists(output_file + \".off\"):\n",
    "        os.remove(output_file + \".off\")\n",
    " \n",
    "    return current_mesh.vertices,current_mesh.faces,output_file+\".off\"\n",
    "\n",
    "def decimate_mesh_from_path(arg_path,decimation_ratio=0):\n",
    "    #write the file to the temp folder\n",
    "    input_file_base = arg_path[:-4]\n",
    "    output_file = input_file_base + \"_decimated\"\n",
    "    \n",
    "    \n",
    "    if decimation_ratio <= 0:\n",
    "        script_name = \"decimation_meshlab_0_25.mls\"\n",
    "    else:\n",
    "        script_name = f\"decimation_meshlab_0_{str(decimation_ratio)[2:4]}.mls\"\n",
    "    meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "    \n",
    "\n",
    "\n",
    "    meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_file_base + \".off\",\n",
    "                                                       output_path_and_filename=output_file + \".off\",\n",
    "                                                       meshlab_script=meshlab_script_path_and_name)\n",
    "    \n",
    "    #read in the output mesh and return the vertices and faces\n",
    "    current_mesh = trimesh.load_mesh(output_file + '.off')\n",
    "    \n",
    "#     #check if file exists and then delete the temporary decimated mesh filess\n",
    "#     if os.path.exists(input_file_base + \".off\"):\n",
    "#         os.remove(input_file_base + \".off\")\n",
    "#     if os.path.exists(output_file + \".off\"):\n",
    "#         os.remove(output_file + \".off\")\n",
    " \n",
    "    return current_mesh,output_file+\".off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate_mesh_from_path(input_file_path,mls_file_path):\n",
    "    #write the file to the temp folder\n",
    "    input_file_base = input_file_path[:-4]\n",
    "    output_file = input_file_base + \"_decimated\"\n",
    "    \n",
    "    meshlab_script_path_and_name = mls_file_path\n",
    "    \n",
    "\n",
    "\n",
    "    meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_file_base + \".off\",\n",
    "                                                       output_path_and_filename=output_file + \".off\",\n",
    "                                                       meshlab_script=meshlab_script_path_and_name)\n",
    "    \n",
    "    current_mesh = trimesh.load_mesh(output_file + '.off')\n",
    "    \n",
    "    return current_mesh,output_file+\".off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def run_poisson_surface_reconstruction(pre_largest_mesh_path,\n",
    "                                       segment_id = \"None\",\n",
    "                                      script_name = \"poisson_working_meshlab.mls\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Will run the poisson surface reconstruction\n",
    "    \n",
    "    \"\"\"\n",
    "    # run the meshlab server script\n",
    "\n",
    "    meshlab_script_path_and_name = str(pathlib.Path.cwd()) + \"/\" + script_name\n",
    "    #input_path =str(pathlib.Path.cwd()) + \"/\" +  pre_largest_mesh_path\n",
    "    input_path =  pre_largest_mesh_path\n",
    "\n",
    "    indices = [i for i, a in enumerate(input_path) if a == \"_\"]\n",
    "    stripped_ending = input_path[:-4]\n",
    "\n",
    "    output_path = stripped_ending + \"_mls.off\"\n",
    "    # print(meshlab_script_path_and_name)\n",
    "    # print(input_path)\n",
    "    # print(output_path)\n",
    "    print(\"Running the mls function\")\n",
    "    meshlab_fix_manifold_path_specific_mls(input_path_and_filename=input_path,\n",
    "                                               output_path_and_filename=output_path,\n",
    "                                               segment_id=segment_id,\n",
    "                                               meshlab_script=meshlab_script_path_and_name)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, contextlib\n",
    "import pathlib\n",
    "import subprocess\n",
    "def meshlab_fix_manifold_path_specific_mls(input_path_and_filename,\n",
    "                                           output_path_and_filename=\"\",\n",
    "                                           segment_id=-1,meshlab_script=\"\"):\n",
    "    #fix the path if it comes with the extension\n",
    "    if input_path_and_filename[-4:] == \".off\":\n",
    "        path_and_filename = input_path_and_filename[:-4]\n",
    "        input_mesh = input_path_and_filename\n",
    "    else:\n",
    "        raise Exception(\"Not passed off file\")\n",
    "    \n",
    "    \n",
    "    if output_path_and_filename == \"\":\n",
    "        output_mesh = path_and_filename+\"_mls.off\"\n",
    "    else:\n",
    "        output_mesh = output_path_and_filename\n",
    "    \n",
    "    if meshlab_script == \"\":\n",
    "        meshlab_script = str(pathlib.Path.cwd()) + \"/\" + \"remeshing_remove_non_man_edges.mls\"\n",
    "    \n",
    "    #print(\"meshlab_script = \" + str(meshlab_script))\n",
    "    #print(\"starting meshlabserver fixing non-manifolds\")\n",
    "    subprocess_result_1 = run_meshlab_script(meshlab_script,\n",
    "                      input_mesh,\n",
    "                      output_mesh)\n",
    "    #print(\"Poisson subprocess_result= \"+ str(subprocess_result_1))\n",
    "    \n",
    "    if str(subprocess_result_1)[-13:] != \"returncode=0)\":\n",
    "        raise Exception('neuron' + str(segment_id) + \n",
    "                         ' did not fix the manifold edges')\n",
    "    \n",
    "    return output_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_meshlab_script(mlx_script,input_mesh_file,output_mesh_file):\n",
    "    script_command = (\" -i \" + str(input_mesh_file) + \" -o \" + \n",
    "                                    str(output_mesh_file) + \" -s \" + str(mlx_script))\n",
    "    #return script_command\n",
    "    command_to_run = 'xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@ ' + script_command\n",
    "    #command_to_run = 'meshlabserver ' + script_command\n",
    "    \n",
    "    print(command_to_run)\n",
    "    subprocess_result = subprocess.run(command_to_run,shell=True)\n",
    "    \n",
    "    return subprocess_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function that will extract soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykdtree.kdtree import KDTree\n",
    "\n",
    "import numpy as np\n",
    "def validate_soma_by_sphere(current_mesh):\n",
    "    ratio_val = current_mesh.bounding_sphere.volume/current_mesh.volume\n",
    "    multiplier = 100\n",
    "    print(\"Inside sphere validater: ratio_val = \" + str(ratio_val))\n",
    "    if np.abs(ratio_val) > multiplier or np.abs(ratio_val) < 1/multiplier:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_soma_center(segment_id,current_mesh_verts,current_mesh_faces):    \n",
    "    \"\"\"\n",
    "    Loop that will compute the soma meshes and locations from the mesh data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------parameters------------------\n",
    "\n",
    "    import time\n",
    "    global_start_time = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    Working for larger neuron: \n",
    "\n",
    "\n",
    "    Try 1: With 0.25 decimation\n",
    "    outer_decimation_ratio= 0.25\n",
    "    large_mesh_threshold = 600000\n",
    "    large_mesh_threshold_inner = 40000\n",
    "    soma_width_threshold = 0.32\n",
    "    soma_size_threshold = 20000\n",
    "\n",
    "    large_mesh_threshold = large_mesh_threshold*outer_decimation_ratio\n",
    "    large_mesh_threshold_inner = large_mesh_threshold_inner*outer_decimation_ratio\n",
    "    soma_size_threshold = soma_size_threshold*outer_decimation_ratio\n",
    "\n",
    "    inner_decimation_ratio = 0.25\n",
    "    soma_size_threshold = soma_size_threshold*inner_decimation_ratio\n",
    "\n",
    "    Faster at: 467.977885723114 seconds\n",
    "\n",
    "\n",
    "    --------------------   Try 2: Using 0.05 decimation for outer layer\n",
    "\n",
    "    Try 1: With 0.25 decimation\n",
    "    outer_decimation_ratio= 0.05\n",
    "    large_mesh_threshold = 1800000\n",
    "    large_mesh_threshold_inner = 40000\n",
    "    soma_width_threshold = 0.32\n",
    "    soma_size_threshold = 20000\n",
    "\n",
    "    large_mesh_threshold = large_mesh_threshold*outer_decimation_ratio\n",
    "    large_mesh_threshold_inner = large_mesh_threshold_inner*outer_decimation_ratio\n",
    "    soma_size_threshold = soma_size_threshold*outer_decimation_ratio\n",
    "\n",
    "    inner_decimation_ratio = 0.25\n",
    "    soma_size_threshold = soma_size_threshold*inner_decimation_ratio\n",
    "\n",
    "                    what it turned out to be ------\n",
    "     large_mesh_threshold= 30000.0 \n",
    "    large_mesh_threshold_inner = 2000.0 \n",
    "    soma_size_threshold = 250.0\\outer_decimation_ratio = 0.05\\inner_decimation_ratio = 0.25\n",
    "\n",
    "\n",
    "    The 0.05 decimation took almost 597 seconds\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    outer_decimation_ratio= 0.25\n",
    "    large_mesh_threshold = 70000\n",
    "    large_mesh_threshold_inner = 40000\n",
    "    soma_width_threshold = 0.32\n",
    "    soma_size_threshold = 20000\n",
    "\n",
    "    large_mesh_threshold = large_mesh_threshold*outer_decimation_ratio\n",
    "    large_mesh_threshold_inner = large_mesh_threshold_inner*outer_decimation_ratio\n",
    "    soma_size_threshold = soma_size_threshold*outer_decimation_ratio\n",
    "\n",
    "    inner_decimation_ratio = 0.25\n",
    "    soma_size_threshold = soma_size_threshold*inner_decimation_ratio\n",
    "\n",
    "    print(f\"Current Arguments Using (adjusted for decimation):\\n large_mesh_threshold= {large_mesh_threshold}\"\n",
    "                 f\" \\nlarge_mesh_threshold_inner = {large_mesh_threshold_inner} \\nsoma_size_threshold = {soma_size_threshold}\"\n",
    "                 f\"\\nouter_decimation_ratio = {outer_decimation_ratio}\"\n",
    "                 f\"\\ninner_decimation_ratio = {inner_decimation_ratio}\")\n",
    "\n",
    "\n",
    "    # ------------------------------\n",
    "\n",
    "\n",
    "    \"\"\" Create a folder and write the neuron there for all experiments to take place\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    folder_name = str(segment_id)\n",
    "    directory = \"./\" + str(folder_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    full_output_mesh_name = str(pathlib.Path.cwd()) + \"/\" + folder_name\n",
    "    mls_path = str(pathlib.Path.cwd()) + \"/\" + \"decimation_meshlab_0_25.mls\"\n",
    "    \n",
    "    \n",
    "    base_file_path = full_output_mesh_name + \"/\" + folder_name + \".off\"\n",
    "    trimesh.Trimesh(vertices=current_mesh_verts,faces=current_mesh_faces).export(base_file_path)\n",
    "    print(f\"base_file_path = {base_file_path}\")\n",
    "    \n",
    "    decimation_script_path = full_output_mesh_name\n",
    "\n",
    "\n",
    "    new_mesh,decimated_output_mesh_file = decimate_mesh_from_path(base_file_path,mls_path)\n",
    "\n",
    "    print(\"decimated_output_mesh_file = \" + str(decimated_output_mesh_file))\n",
    "\n",
    "    new_mesh = trimesh.load_mesh(decimated_output_mesh_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mesh_splits = new_mesh.split(only_watertight=False)\n",
    "\n",
    "    #len(\"Total mesh splits = \" + str(mesh_splits))\n",
    "    #get the largest mesh\n",
    "    mesh_lengths = np.array([len(split.faces) for split in mesh_splits])\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # import seaborn as sns\n",
    "    # sns.set()\n",
    "    # sns.distplot(mesh_lengths)\n",
    "\n",
    "    # largest_index = np.where(mesh_lengths == np.max(mesh_lengths))\n",
    "    # largest_mesh = mesh_splits[largest_index][0]\n",
    "\n",
    "    \"\"\" -- temporarily changing to the second largest mesh\"\"\"\n",
    "    total_mesh_split_lengths = [len(k.faces) for k in mesh_splits]\n",
    "    ordered_mesh_splits = mesh_splits[np.flip(np.argsort(total_mesh_split_lengths))]\n",
    "    list_of_largest_mesh = [k for k in ordered_mesh_splits if len(k.faces) > large_mesh_threshold]\n",
    "\n",
    "    print(f\"Total found significant pieces before Poisson = {list_of_largest_mesh}\")\n",
    "    \n",
    "    if len(list_of_largest_mesh)<=0:\n",
    "        print(f\"Using smaller large_mesh_threshold because no significant pieces found with {large_mesh_threshold}\")\n",
    "        list_of_largest_mesh = [k for k in ordered_mesh_splits if len(k.faces) > large_mesh_threshold/2]\n",
    "        \n",
    "\n",
    "    # total_soma_mesh = trimesh.Trimesh(vertices=np.array([]),\n",
    "    #                                  triangles = np.array([]))\n",
    "\n",
    "    total_soma_list = []\n",
    "    total_classifier_list = []\n",
    "    total_poisson_list = []\n",
    "\n",
    "    #start iterating through where go through all pieces before the poisson reconstruction\n",
    "    no_somas_found_in_big_loop = 0\n",
    "    for i,largest_mesh in enumerate(list_of_largest_mesh):\n",
    "        print(f\"----- working on large mesh #{i}: {largest_mesh}\")\n",
    "\n",
    "        somas_found_in_big_loop = False\n",
    "\n",
    "        stripped_ending = decimated_output_mesh_file[:-4]\n",
    "        pre_largest_mesh_path = stripped_ending + \"_\" + str(i) + \"_largest_piece.off\"\n",
    "\n",
    "        largest_mesh.export(pre_largest_mesh_path)\n",
    "        print(\"done exporting\")\n",
    "\n",
    "        output_path = run_poisson_surface_reconstruction(pre_largest_mesh_path)\n",
    "\n",
    "        #---------------- Will carry out the cgal segmentation -------- #\n",
    "        #import the mesh\n",
    "        new_mesh_inner = trimesh.load_mesh(output_path)\n",
    "\n",
    "        mesh_splits_inner = new_mesh_inner.split(only_watertight=False)\n",
    "        total_mesh_split_lengths_inner = [len(k.faces) for k in mesh_splits_inner]\n",
    "        ordered_mesh_splits_inner = mesh_splits_inner[np.flip(np.argsort(total_mesh_split_lengths_inner))]\n",
    "        #print(f\"Mesh lengths of inner after split: {[len(k.faces) for k in ordered_mesh_splits_inner]}\")\n",
    "\n",
    "        list_of_largest_mesh_inner = [k for k in ordered_mesh_splits_inner if len(k.faces) > large_mesh_threshold_inner]\n",
    "        print(f\"Total found significant pieces AFTER Poisson = {list_of_largest_mesh_inner}\")\n",
    "\n",
    "        stripped_ending = output_path[:-4]\n",
    "        print(f\"stripped_ending 2 = {stripped_ending}\")\n",
    "        n_failed_inner_soma_loops = 0\n",
    "        for j, largest_mesh_inner in enumerate(list_of_largest_mesh_inner):\n",
    "\n",
    "            print(f\"----- working on mesh after poisson #{j}: {largest_mesh_inner}\")\n",
    "\n",
    "            largest_mesh_path_inner = stripped_ending +\"_\" + str(j) + \"_largest_inner.off\"\n",
    "\n",
    "            #DON'T NEED THIS WRITE NOW BECAUSE IT ALREADY OUTPUTS THE MESH\n",
    "            largest_mesh_inner.export(largest_mesh_path_inner)\n",
    "            print(f\"done exporting {largest_mesh_path_inner}\")\n",
    "\n",
    "            largest_mesh_path_inner_decimated,decimated_output_mesh_file_inner = decimate_mesh_from_path(largest_mesh_path_inner,\n",
    "                                                                       mls_path)\n",
    "            largest_mesh_path_inner_decimated.export(largest_mesh_path_inner[:-4] + \"_decimated.off\")\n",
    "            print(f\"done exporting decimated mesh: {largest_mesh_path_inner}\")\n",
    "            # Starts the actual cgal segmentation:\n",
    "\n",
    "            faces = np.array(largest_mesh_path_inner_decimated.faces)\n",
    "            verts = np.array(largest_mesh_path_inner_decimated.vertices)\n",
    "            #run the whole algorithm on the neuron to test\n",
    "            segment_id_new = int(str(segment_id) + f\"{i}{j}\")\n",
    "            verts_labels, faces_labels, soma_value,classifier = wcda.extract_branches_whole_neuron(\n",
    "                                import_Off_Flag=False,\n",
    "                                segment_id=segment_id_new,\n",
    "                                vertices=verts,\n",
    "                                 triangles=faces,\n",
    "                                pymeshfix_Flag=False,\n",
    "                                 import_CGAL_Flag=False,\n",
    "                                 return_Only_Labels=True,\n",
    "                                 clusters=3,\n",
    "                                 smoothness=0.2,\n",
    "                                soma_only=True,\n",
    "                                return_classifier = True\n",
    "                                )\n",
    "\n",
    "            total_classifier_list.append(classifier)\n",
    "            #total_poisson_list.append(largest_mesh_path_inner_decimated)\n",
    "\n",
    "            # Save all of the portions that resemble a soma\n",
    "            median_values = np.array([v[\"median\"] for k,v in classifier.sdf_final_dict.items()])\n",
    "            segmentation = np.array([k for k,v in classifier.sdf_final_dict.items()])\n",
    "\n",
    "            #order the compartments by greatest to smallest\n",
    "            sorted_medians = np.flip(np.argsort(median_values))\n",
    "            print(f\"segmentation[sorted_medians],median_values[sorted_medians] = {(segmentation[sorted_medians],median_values[sorted_medians])}\")\n",
    "            print(f\"Sizes = {[classifier.sdf_final_dict[g]['n_faces'] for g in segmentation[sorted_medians]]}\")\n",
    "\n",
    "            valid_soma_segments_width = [g for g,h in zip(segmentation[sorted_medians],median_values[sorted_medians]) if ((h > soma_width_threshold)\n",
    "                                                                and (classifier.sdf_final_dict[g][\"n_faces\"] > soma_size_threshold))]\n",
    "\n",
    "            print(\"valid_soma_segments_width\")\n",
    "            to_add_list = []\n",
    "            if len(valid_soma_segments_width) > 0:\n",
    "                print(f\"      ------ Found {len(valid_soma_segments_width)} viable somas: {valid_soma_segments_width}\")\n",
    "                somas_found_in_big_loop = True\n",
    "                #get the meshes only if signfiicant length\n",
    "                labels_list = classifier.labels_list\n",
    "\n",
    "                for v in valid_soma_segments_width:\n",
    "                    submesh_face_list = np.where(classifier.labels_list == v)[0]\n",
    "                    soma_mesh = largest_mesh_path_inner_decimated.submesh([submesh_face_list],append=True)\n",
    "                    \n",
    "                    if validate_soma_by_sphere(soma_mesh):\n",
    "                        to_add_list.append(soma_mesh)\n",
    "                    else:\n",
    "                        print(f\"--->This soma mesh was not added because it did not pass the sphere validation: {soma_mesh}\")\n",
    "\n",
    "                n_failed_inner_soma_loops = 0\n",
    "\n",
    "            else:\n",
    "                n_failed_inner_soma_loops += 1\n",
    "\n",
    "            total_soma_list += to_add_list\n",
    "\n",
    "            # --------------- KEEP TRACK IF FAILED TO FIND SOMA (IF TOO MANY FAILS THEN BREAK)\n",
    "            if n_failed_inner_soma_loops >= 2:\n",
    "                print(\"breaking inner loop because 2 soma fails in a row\")\n",
    "                break\n",
    "\n",
    "\n",
    "        # --------------- KEEP TRACK IF FAILED TO FIND SOMA (IF TOO MANY FAILS THEN BREAK)\n",
    "        if somas_found_in_big_loop == False:\n",
    "            no_somas_found_in_big_loop += 1\n",
    "            if no_somas_found_in_big_loop >= 2:\n",
    "                print(\"breaking because 2 fails in a row in big loop\")\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            no_somas_found_in_big_loop = 0\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        large_mesh_threshold= 150000.0 \n",
    "        large_mesh_threshold_inner = 10000.0 \n",
    "        soma_size_threshold = 1250.0\n",
    "        decimation_ratio = 0.05\n",
    "        \"\"\"\n",
    "    \n",
    "    \"\"\" IF THERE ARE MULTIPLE SOMAS THAT ARE WITHIN A CERTAIN DISTANCE OF EACH OTHER THEN JUST COMBINE THEM INTO ONE\"\"\"\n",
    "    pairings = []\n",
    "    for y,soma_1 in enumerate(total_soma_list):\n",
    "        for z,soma_2 in enumerate(total_soma_list):\n",
    "            if y<z:\n",
    "                mesh_tree = KDTree(soma_1.vertices)\n",
    "                distances,closest_node = mesh_tree.query(soma_2.vertices)\n",
    "                \n",
    "                if np.min(distances) < 4000:\n",
    "                    pairings.append([y,z])\n",
    "                    \n",
    "                    \n",
    "    #creating the combined meshes from the list\n",
    "    if len(pairings) > 0:\n",
    "        \"\"\"\n",
    "        Pseudocode: \n",
    "        Use a network function to find components\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        import networkx as nx\n",
    "        new_graph = nx.Graph()\n",
    "        new_graph.add_edges_from(pairings)\n",
    "        grouped_somas = list(nx.connected_components(new_graph))\n",
    "        \n",
    "        total_soma_list_revised = [] \n",
    "        somas_being_combined = []\n",
    "        print(f\"There were soma pariings: Connected components in = {grouped_somas} \")\n",
    "        for comp in grouped_somas:\n",
    "            comp = list(comp)\n",
    "            somas_being_combined += list(comp)\n",
    "            current_mesh = total_soma_list[comp[0]]\n",
    "            for i in range(1,len(comp)):\n",
    "                current_mesh += total_soma_list[comp[i]]\n",
    "            \n",
    "            total_soma_list_revised.append(current_mesh)\n",
    "        \n",
    "        #add those that weren't combined to total_soma_list_revised\n",
    "        leftover_somas = [total_soma_list[k] for k in range(0,len(total_soma_list)) if k not in somas_being_combined]\n",
    "        if len(leftover_somas) > 0:\n",
    "            total_soma_list_revised += leftover_somas\n",
    "        print(f\"Final total_soma_list_revised = {total_soma_list_revised}\")\n",
    "        \n",
    "        total_soma_list = total_soma_list_revised\n",
    "            \n",
    "    \n",
    "    run_time = time.time() - global_start_time\n",
    "\n",
    "    print(f\"\\n\\n\\n Total time for run = {time.time() - global_start_time}\")\n",
    "\n",
    "    #need to erase all of the temporary files ******\n",
    "    #import shutil\n",
    "    #shutil.rmtree(directory)\n",
    "    \n",
    "    \"\"\"\n",
    "    Need to delete all files in the temp folder *****\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return total_soma_list, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "        <style type=\"text/css\">\n",
       "            .Relation{\n",
       "                border-collapse:collapse;\n",
       "            }\n",
       "            .Relation th{\n",
       "                background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "                font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "            }\n",
       "            .Relation td{\n",
       "                padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "            }\n",
       "            .Relation tr:nth-child(odd){\n",
       "                background: #ffffff;\n",
       "            }\n",
       "            .Relation tr:nth-child(even){\n",
       "                background: #f3f1ff;\n",
       "            }\n",
       "            /* Tooltip container */\n",
       "            .djtooltip {\n",
       "            }\n",
       "            /* Tooltip text */\n",
       "            .djtooltip .djtooltiptext {\n",
       "                visibility: hidden;\n",
       "                width: 120px;\n",
       "                background-color: black;\n",
       "                color: #fff;\n",
       "                text-align: center;\n",
       "                padding: 5px 0;\n",
       "                border-radius: 6px;\n",
       "                /* Position the tooltip text - see examples below! */\n",
       "                position: absolute;\n",
       "                z-index: 1;\n",
       "            }\n",
       "            #primary {\n",
       "                font-weight: bold;\n",
       "                color: black;\n",
       "            }\n",
       "\n",
       "            #nonprimary {\n",
       "                font-weight: normal;\n",
       "                color: white;\n",
       "            }\n",
       "\n",
       "            /* Show the tooltip text when you mouse over the tooltip container */\n",
       "            .djtooltip:hover .djtooltiptext {\n",
       "                visibility: visible;\n",
       "            }\n",
       "        </style>\n",
       "        \n",
       "        \n",
       "            <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "            <table border=\"1\" class=\"Relation\">\n",
       "                <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">segment_id</p>\n",
       "                                <span class=\"djtooltiptext\">segment id unique within each Segmentation</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">version</p>\n",
       "                                <span class=\"djtooltiptext\"></span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">decimation_ratio</p>\n",
       "                                <span class=\"djtooltiptext\">ratio of remaining mesh vertices/faces (which ones depends on what metric the decimation technique uses)</span>\n",
       "                            </div> </th> </tr> </thead>\n",
       "                <tbody> <tr> <td>77489456693007645</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td></tr><tr><td>90153356676844854</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td></tr><tr><td>90518669414951752</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td></tr><tr><td>91208269699305759</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td></tr><tr><td>96147481821744505</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td></tr><tr><td>107319757380005445</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td></tr><tr><td>107681564887986552</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td></tr><tr><td>107738877133006848</td>\n",
       "<td>0</td>\n",
       "<td>0.25</td> </tr> </tbody>\n",
       "            </table>\n",
       "            \n",
       "            <p>Total: 8</p></div>\n",
       "            "
      ],
      "text/plain": [
       "*segment_id    *version    *decimation_ra\n",
       "+------------+ +---------+ +------------+\n",
       "77489456693007 0           0.25          \n",
       "90153356676844 0           0.25          \n",
       "90518669414951 0           0.25          \n",
       "91208269699305 0           0.25          \n",
       "96147481821744 0           0.25          \n",
       "10731975738000 0           0.25          \n",
       "10768156488798 0           0.25          \n",
       "10773887713300 0           0.25          \n",
       " (Total: 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(minnie.Decimation() & (m65.AllenMultiSomas() & \"status > 0\")).proj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "import time\n",
    "\n",
    "\n",
    "@schema\n",
    "class MultiSomaCentroidValidation(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation\n",
    "    soma_index : tinyint unsigned #index given to this soma to account for multiple somas in one base semgnet\n",
    "    ---\n",
    "    centroid_x           : int unsigned                 # (EM voxels)\n",
    "    centroid_y           : int unsigned                 # (EM voxels)\n",
    "    centroid_z           : int unsigned                 # (EM voxels)\n",
    "    distance_from_prediction : double                   # the distance of the ALLEN predicted centroid soma center from the algorithms prediction\n",
    "    prediction_matching_index : int unsigned            # the soma index that was used to compute the error\n",
    "    soma_vertices             : longblob                # array of vertices\n",
    "    soma_faces             : longblob                   # array of faces\n",
    "    multiplicity         : tinyint unsigned             # the number of somas found for this base segment\n",
    "    run_time : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #key_source = (minnie.Decimation & (dj.U(\"segment_id\") & (m65.AllenSegmentCentroid & \"status=1\").proj()) & \"version=\" + str(decimation_version))\n",
    "    key_source = minnie.Decimation() & (m65.AllenMultiSomas() & \"status > 0\").proj()\n",
    "    \n",
    "    def make(self,key):\n",
    "        #get the mesh data\n",
    "        print(f\"\\n\\n\\n---- Working on {key['segment_id']} ----\")\n",
    "        \n",
    "        new_mesh = (minnie.Decimation() & key).fetch1(\"mesh\")\n",
    "        current_mesh_verts,current_mesh_faces = new_mesh.vertices,new_mesh.faces\n",
    "        \n",
    "        segment_id = key[\"segment_id\"]\n",
    "        \n",
    "        total_soma_list, run_time = extract_soma_center(segment_id,current_mesh_verts,current_mesh_faces)\n",
    "        print(f\"Run time was {run_time}  and the total_soma_list = {total_soma_list} and \")\n",
    "        \n",
    "        #check if soma list is empty and did not find soma\n",
    "        if len(total_soma_list) <= 0:\n",
    "            print(\"There were no somas found for this mesh so just writing empty data\")\n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=-1,\n",
    "                              centroid_x=None,\n",
    "                               centroid_y=None,\n",
    "                               centroid_z=None,\n",
    "                               distance_from_prediction=None,\n",
    "                               prediction_matching_index = None,\n",
    "                               soma_vertices=None,\n",
    "                               soma_faces=None,\n",
    "                               multiplicity=0,\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            raise Exception(\"to prevent writing because none were found\")\n",
    "            self.insert1(insert_dict,skip_duplicates=True)\n",
    "            return\n",
    "        \n",
    "        #if there is one or more soma found\n",
    "        \n",
    "        #get the soma prediction\n",
    "        \n",
    "        \n",
    "        new_array = (m65.AllenMultiSomas.Centroids()  & key).fetch(\"centroid_x\",\"centroid_y\",\"centroid_z\")\n",
    "        soma_ids = (m65.AllenMultiSomas.Centroids()  & key).fetch(\"soma_id\")\n",
    "        allen_centroid_prediction = np.array(new_array).T\n",
    "        allen_centroid_prediction\n",
    "        #print(\"soma_ids = \" + str(soma_ids))\n",
    "        from pykdtree.kdtree import KDTree\n",
    "        mesh_tree = KDTree(allen_centroid_prediction)\n",
    "\n",
    "        \n",
    "        dicts_to_insert = []\n",
    "        \n",
    "\n",
    "            \n",
    "        for i,current_soma in enumerate(total_soma_list):\n",
    "            print(\"Trying to write off file\")\n",
    "            current_soma.export(f\"{key['segment_id']}/{key['segment_id']}_soma_{i}.off\")\n",
    "            auto_prediction_center = np.mean(current_soma.vertices,axis=0)\n",
    "            \n",
    "            distances,closest_node = mesh_tree.query(auto_prediction_center.reshape(1,3))\n",
    "            error_distance = distances[0]\n",
    "            prediction_matching_index = soma_ids[closest_node[0]] #closest nodes and the distances\n",
    "            \n",
    "            \n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=i,\n",
    "                              centroid_x=auto_prediction_center[0],\n",
    "                               centroid_y=auto_prediction_center[1],\n",
    "                               centroid_z=auto_prediction_center[2],\n",
    "                               distance_from_prediction=error_distance,\n",
    "                               prediction_matching_index = prediction_matching_index,\n",
    "                               soma_vertices=current_soma.vertices,\n",
    "                               soma_faces=current_soma.faces,\n",
    "                               multiplicity=len(total_soma_list),\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            \n",
    "            \n",
    "            dicts_to_insert.append(insert_dict)\n",
    "            \n",
    "        #raise Exception(\"to prevent writing\")\n",
    "        \n",
    "        if len(total_soma_list) != len(soma_ids):\n",
    "            raise Exception(\"to prevent writing SOMAS NOT EQUAL TO That registered in datase\")\n",
    "            \n",
    "        self.insert(dicts_to_insert,skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_array = (m65.AllenMultiSomas.Centroids()  & dict(segment_id=92345990027434104)).fetch(\"centroid_x\",\"centroid_y\",\"centroid_z\")\n",
    "# soma_ids = (m65.AllenMultiSomas.Centroids()  & dict(segment_id=92345990027434104)).fetch(\"soma_id\")\n",
    "# allen_centroid_prediction = np.array(new_array).T\n",
    "# allen_centroid_prediction\n",
    "# print(\"soma_ids = \" + str(soma_ids))\n",
    "# from pykdtree.kdtree import KDTree\n",
    "# mesh_tree = KDTree(allen_centroid_prediction)\n",
    "# distances,closest_node = mesh_tree.query([np.array([1,1,1])])\n",
    "# distances[0],soma_ids[closest_node[0]] #closest nodes and the distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(SomaCentroidValidation & \"segment_id=90231147459666747\").delete()\n",
    "(schema.jobs & \"table_name='__multi_soma_centroid_validation'\").delete()\n",
    "#m65.SomaCentroidValidation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "---- Working on 107319757380005445 ----\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 17500.0 \n",
      "large_mesh_threshold_inner = 10000.0 \n",
      "soma_size_threshold = 1250.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "base_file_path = /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445.off -o /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "decimated_output_mesh_file = /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated.off\n",
      "Total found significant pieces before Poisson = [<trimesh.Trimesh(vertices.shape=(92951, 3), faces.shape=(191260, 3))>]\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(92951, 3), faces.shape=(191260, 3))>\n",
      "done exporting\n",
      "Running the mls function\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece.off -o /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls.off -s /notebooks/Platinum_Blender/Brendan_Soma/poisson_working_meshlab.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(39215, 3), faces.shape=(78562, 3))>, <trimesh.Trimesh(vertices.shape=(8257, 3), faces.shape=(16510, 3))>]\n",
      "stripped_ending 2 = /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(39215, 3), faces.shape=(78562, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls_0_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls_0_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls_0_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls_0_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0002765655517578125\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10731975738000544500_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 2.191293478012085\n",
      "2) Finished: Generating CGAL segmentation for neuron: 2.5377745628356934\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 4\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.007686614990234375\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 0.00021505355834960938\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.01673579216003418\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.09244346618652344\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 5,  4,  2,  3,  8,  0,  7, 11,  1,  6,  9, 10, 12]), array([0.773407  , 0.559952  , 0.0754894 , 0.0640131 , 0.0628262 ,\n",
      "       0.0594215 , 0.0573583 , 0.0562646 , 0.0517184 , 0.0454245 ,\n",
      "       0.0437381 , 0.0405194 , 0.03103845]))\n",
      "Sizes = [2803, 4643, 549, 914, 1035, 5705, 685, 355, 865, 291, 782, 945, 68]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 2 viable somas: [5, 4]\n",
      "Inside sphere validater: ratio_val = 0.0231876813249392\n",
      "Inside sphere validater: ratio_val = -0.0594609061271534\n",
      "----- working on mesh after poisson #1: <trimesh.Trimesh(vertices.shape=(8257, 3), faces.shape=(16510, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls_1_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls_1_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls_1_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107319757380005445/107319757380005445_decimated_0_largest_piece_mls_1_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00020599365234375\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10731975738000544501_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.27242207527160645\n",
      "2) Finished: Generating CGAL segmentation for neuron: 0.3336367607116699\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.0015225410461425781\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 4.3392181396484375e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.0025260448455810547\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.018825054168701172\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([0]), array([0.6330015]))\n",
      "Sizes = [4126]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "Inside sphere validater: ratio_val = 12588.347928111518\n",
      "--->This soma mesh was not added because it did not pass the sphere validation: <trimesh.Trimesh(vertices.shape=(2065, 3), faces.shape=(4126, 3))>\n",
      "There were soma pariings: Connected components in = [{0, 1}] \n",
      "Final total_soma_list_revised = [<trimesh.Trimesh(vertices.shape=(3783, 3), faces.shape=(7446, 3))>]\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 55.31589961051941\n",
      "Run time was 55.315895557403564  and the total_soma_list = [<trimesh.Trimesh(vertices.shape=(3783, 3), faces.shape=(7446, 3))>] and \n",
      "Trying to write off file\n",
      "\n",
      "\n",
      "\n",
      "---- Working on 107681564887986552 ----\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 17500.0 \n",
      "large_mesh_threshold_inner = 10000.0 \n",
      "soma_size_threshold = 1250.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "base_file_path = /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552.off -o /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "decimated_output_mesh_file = /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated.off\n",
      "Total found significant pieces before Poisson = [<trimesh.Trimesh(vertices.shape=(347618, 3), faces.shape=(718031, 3))>]\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(347618, 3), faces.shape=(718031, 3))>\n",
      "done exporting\n",
      "Running the mls function\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece.off -o /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls.off -s /notebooks/Platinum_Blender/Brendan_Soma/poisson_working_meshlab.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(117889, 3), faces.shape=(235782, 3))>, <trimesh.Trimesh(vertices.shape=(34351, 3), faces.shape=(68786, 3))>, <trimesh.Trimesh(vertices.shape=(22410, 3), faces.shape=(44820, 3))>]\n",
      "stripped_ending 2 = /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(117889, 3), faces.shape=(235782, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_0_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_0_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_0_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_0_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00027251243591308594\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10768156488798655200_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 6.319504499435425\n",
      "2) Finished: Generating CGAL segmentation for neuron: 7.148816347122192\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 22\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.019847869873046875\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 4.7206878662109375e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.05133223533630371\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.3662722110748291\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([22,  1, 28, 24, 16, 26, 25, 30, 31, 27,  3, 12, 17, 20, 21, 14,  7,\n",
      "       11,  2,  9,  4,  0, 15,  5,  8, 13, 10, 19,  6, 18, 23, 29]), array([0.824693  , 0.1285065 , 0.1099115 , 0.109312  , 0.108077  ,\n",
      "       0.0960329 , 0.0683513 , 0.0536999 , 0.0419683 , 0.0372025 ,\n",
      "       0.0365829 , 0.036344  , 0.0360901 , 0.0356036 , 0.0355667 ,\n",
      "       0.0349428 , 0.0342818 , 0.0342208 , 0.03344825, 0.03221465,\n",
      "       0.0318848 , 0.0310304 , 0.0302303 , 0.02920405, 0.0284967 ,\n",
      "       0.02819465, 0.028157  , 0.0274583 , 0.02674675, 0.0247966 ,\n",
      "       0.02447825, 0.0191236 ]))\n",
      "Sizes = [5377, 13682, 168, 333, 885, 217, 131, 1246, 473, 389, 1377, 2798, 930, 2286, 1201, 411, 1257, 761, 1576, 490, 1993, 3521, 2075, 1348, 1821, 2906, 1635, 45, 4024, 2655, 792, 141]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [22]\n",
      "Inside sphere validater: ratio_val = -2.487896062185282\n",
      "----- working on mesh after poisson #1: <trimesh.Trimesh(vertices.shape=(34351, 3), faces.shape=(68786, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_1_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_1_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_1_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_1_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00020265579223632812\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10768156488798655201_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 1.7142701148986816\n",
      "2) Finished: Generating CGAL segmentation for neuron: 2.050591468811035\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 2\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.006008625030517578\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 0.00025010108947753906\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.01469564437866211\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.08137774467468262\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 2,  5,  6, 14,  1,  7,  4, 12, 19, 16, 18, 15,  3,  0, 10,  8, 11,\n",
      "       17,  9, 13]), array([0.6358465 , 0.116997  , 0.106379  , 0.0708241 , 0.0687648 ,\n",
      "       0.0685934 , 0.0649434 , 0.06341295, 0.0631097 , 0.0624379 ,\n",
      "       0.0610806 , 0.0592682 , 0.05689215, 0.0519146 , 0.0485834 ,\n",
      "       0.0434135 , 0.0393548 , 0.0359608 , 0.0217758 , 0.0125656 ]))\n",
      "Sizes = [3342, 1127, 3113, 907, 2858, 3399, 219, 88, 37, 95, 445, 313, 116, 535, 311, 137, 61, 19, 47, 27]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [2]\n",
      "Inside sphere validater: ratio_val = -5.764712677026253\n",
      "----- working on mesh after poisson #2: <trimesh.Trimesh(vertices.shape=(22410, 3), faces.shape=(44820, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_2_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_2_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_2_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107681564887986552/107681564887986552_decimated_0_largest_piece_mls_2_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00021910667419433594\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10768156488798655202_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.9051167964935303\n",
      "2) Finished: Generating CGAL segmentation for neuron: 1.0809240341186523\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.003963470458984375\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 5.602836608886719e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.009156465530395508\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.053160905838012695\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 7, 12, 14,  4,  6,  9, 10,  3, 13,  1,  5, 11,  0,  2,  8]), array([0.8419715, 0.116383 , 0.106175 , 0.103217 , 0.0985661, 0.0913738,\n",
      "       0.0868006, 0.0477783, 0.0415856, 0.0409653, 0.0397015, 0.0396954,\n",
      "       0.0377991, 0.0346838, 0.0207909]))\n",
      "Sizes = [2174, 581, 248, 701, 151, 406, 159, 515, 417, 801, 137, 557, 3754, 408, 195]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [7]\n",
      "Inside sphere validater: ratio_val = 4.5564283065629025\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 171.58073949813843\n",
      "Run time was 171.58073687553406  and the total_soma_list = [<trimesh.Trimesh(vertices.shape=(2731, 3), faces.shape=(5377, 3))>, <trimesh.Trimesh(vertices.shape=(1700, 3), faces.shape=(3342, 3))>, <trimesh.Trimesh(vertices.shape=(1116, 3), faces.shape=(2174, 3))>] and \n",
      "Trying to write off file\n",
      "Trying to write off file\n",
      "Trying to write off file\n",
      "\n",
      "\n",
      "\n",
      "---- Working on 107738877133006848 ----\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 17500.0 \n",
      "large_mesh_threshold_inner = 10000.0 \n",
      "soma_size_threshold = 1250.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "base_file_path = /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848.off -o /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "decimated_output_mesh_file = /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated.off\n",
      "Total found significant pieces before Poisson = [<trimesh.Trimesh(vertices.shape=(327432, 3), faces.shape=(682632, 3))>]\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(327432, 3), faces.shape=(682632, 3))>\n",
      "done exporting\n",
      "Running the mls function\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece.off -o /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls.off -s /notebooks/Platinum_Blender/Brendan_Soma/poisson_working_meshlab.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(284813, 3), faces.shape=(570782, 3))>, <trimesh.Trimesh(vertices.shape=(121397, 3), faces.shape=(243354, 3))>, <trimesh.Trimesh(vertices.shape=(44644, 3), faces.shape=(89284, 3))>, <trimesh.Trimesh(vertices.shape=(44175, 3), faces.shape=(88350, 3))>, <trimesh.Trimesh(vertices.shape=(5135, 3), faces.shape=(10278, 3))>]\n",
      "stripped_ending 2 = /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(284813, 3), faces.shape=(570782, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_0_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_0_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_0_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_0_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00027441978454589844\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10773887713300684800_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 47.80140805244446\n",
      "2) Finished: Generating CGAL segmentation for neuron: 49.91857624053955\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.05052995681762695\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 6.628036499023438e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.12366771697998047\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.8705165386199951\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([  0, 110, 134,  96, 151,  22,  94,  30,  16,  69,  86,  49, 103,\n",
      "        62, 153,  27,  39,  32, 101,  64, 126,  61,  71, 143,   4,   5,\n",
      "        34,  51,  74,  58, 142, 118,  45,  75,  20,  59,  18,  72,  54,\n",
      "        11,  19,  17,  63, 100,  40,  70,   6,  73, 136,  87,   1, 128,\n",
      "        29, 133,  66,  10,  48,   8,  78,  88,  98,  15,  28, 147,  21,\n",
      "         2, 131,  52,   9, 105,  97,   7,  60,  33,  90, 114,  12, 130,\n",
      "        55, 113, 137, 155,  37, 123, 117, 132,  44,  50, 145,  95, 148,\n",
      "        25,  26,  85,  23,  57,  99,  92, 152, 127,   3, 140,  14,  68,\n",
      "        24,  80,  65, 112, 150, 138,  81,  91, 104,  84,  43,  67,  56,\n",
      "       139, 106, 129,  46,  35,  41,  42, 119,  13, 109,  89,  31, 115,\n",
      "       102, 107,  82, 149, 122, 154, 116, 141, 121, 144, 125, 111,  38,\n",
      "        79, 146,  93,  53, 108,  77,  83,  76, 156,  36, 120,  47, 124,\n",
      "       135]), array([0.3633935 , 0.349543  , 0.2580275 , 0.224891  , 0.213862  ,\n",
      "       0.212222  , 0.2035625 , 0.203019  , 0.202248  , 0.201951  ,\n",
      "       0.201747  , 0.198279  , 0.1976305 , 0.1976115 , 0.197223  ,\n",
      "       0.1969    , 0.196402  , 0.19483   , 0.192706  , 0.192594  ,\n",
      "       0.191338  , 0.189455  , 0.18606   , 0.185546  , 0.185121  ,\n",
      "       0.184382  , 0.1843405 , 0.1805335 , 0.180465  , 0.179929  ,\n",
      "       0.1780855 , 0.177563  , 0.1774525 , 0.177228  , 0.17648   ,\n",
      "       0.175166  , 0.166142  , 0.1653575 , 0.162947  , 0.1595275 ,\n",
      "       0.1534625 , 0.151657  , 0.1502085 , 0.149418  , 0.14906   ,\n",
      "       0.148988  , 0.147093  , 0.147091  , 0.146846  , 0.1467235 ,\n",
      "       0.1462715 , 0.146034  , 0.145847  , 0.14506   , 0.1445255 ,\n",
      "       0.1443665 , 0.1441425 , 0.14314   , 0.143059  , 0.14208   ,\n",
      "       0.141204  , 0.141039  , 0.140836  , 0.140496  , 0.1393635 ,\n",
      "       0.139361  , 0.138815  , 0.138745  , 0.138505  , 0.138429  ,\n",
      "       0.136383  , 0.1358305 , 0.134662  , 0.134625  , 0.133631  ,\n",
      "       0.132983  , 0.132846  , 0.132754  , 0.132594  , 0.132246  ,\n",
      "       0.130764  , 0.1280615 , 0.127706  , 0.12759   , 0.125046  ,\n",
      "       0.124399  , 0.1243905 , 0.1242765 , 0.123122  , 0.122703  ,\n",
      "       0.122108  , 0.121031  , 0.120917  , 0.120567  , 0.119991  ,\n",
      "       0.1185    , 0.118467  , 0.11797   , 0.1177645 , 0.1176835 ,\n",
      "       0.11751   , 0.115218  , 0.113421  , 0.112704  , 0.112448  ,\n",
      "       0.1117405 , 0.111329  , 0.110954  , 0.110439  , 0.108793  ,\n",
      "       0.106758  , 0.106041  , 0.105534  , 0.10528   , 0.103435  ,\n",
      "       0.103381  , 0.100167  , 0.0999749 , 0.0998846 , 0.0994604 ,\n",
      "       0.0986397 , 0.0962986 , 0.0955803 , 0.0928282 , 0.09210555,\n",
      "       0.0898449 , 0.0898197 , 0.08956435, 0.0892027 , 0.0880139 ,\n",
      "       0.08776785, 0.0872024 , 0.086992  , 0.0854588 , 0.0839236 ,\n",
      "       0.0838867 , 0.0825371 , 0.0824227 , 0.0816899 , 0.0816117 ,\n",
      "       0.08129855, 0.0812092 , 0.0803146 , 0.0798021 , 0.0791757 ,\n",
      "       0.0783427 , 0.07591665, 0.0737151 , 0.0723412 , 0.07083715,\n",
      "       0.068692  , 0.0565517 , 0.0522073 , 0.0479906 , 0.0455877 ,\n",
      "       0.0392372 , 0.0331407 ]))\n",
      "Sizes = [20140, 181, 128, 918, 119, 898, 1366, 5946, 357, 181, 169, 1734, 1412, 182, 3188, 137, 231, 2376, 253, 2526, 138, 1475, 4548, 113, 5906, 441, 594, 404, 111, 4656, 580, 175, 204, 71, 1683, 721, 879, 412, 1254, 732, 706, 801, 300, 400, 2719, 1122, 3360, 103, 899, 1674, 1622, 435, 2093, 356, 1388, 3622, 4864, 1151, 81, 611, 1997, 7258, 176, 313, 1586, 1827, 2093, 321, 1980, 370, 251, 2552, 179, 1141, 1183, 53, 229, 1607, 1202, 111, 241, 414, 164, 45, 91, 1602, 474, 664, 139, 329, 147, 87, 174, 225, 1306, 454, 195, 172, 144, 676, 145, 178, 150, 87, 7072, 92, 539, 81, 182, 109, 203, 222, 77, 95, 232, 129, 29, 397, 295, 43, 27, 35, 193, 39, 144, 50, 55, 106, 65, 60, 138, 42, 59, 65, 27, 53, 83, 135, 33, 113, 84, 62, 27, 25, 35, 53, 62, 67, 17, 92, 27, 27, 37, 33, 39, 57, 53]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "Inside sphere validater: ratio_val = 104.9505442565549\n",
      "--->This soma mesh was not added because it did not pass the sphere validation: <trimesh.Trimesh(vertices.shape=(10261, 3), faces.shape=(20140, 3))>\n",
      "----- working on mesh after poisson #1: <trimesh.Trimesh(vertices.shape=(121397, 3), faces.shape=(243354, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_1_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_1_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_1_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_1_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0002560615539550781\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10773887713300684801_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 13.70248556137085\n",
      "2) Finished: Generating CGAL segmentation for neuron: 14.560102224349976\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 2\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.020751953125\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 4.9591064453125e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.05299115180969238\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.3914468288421631\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 2,  1, 31, 19, 35,  9, 34, 25, 12,  3, 32, 21, 22,  4, 15,  0, 37,\n",
      "        7,  8, 20, 33, 36, 29, 11, 23, 18, 24, 13,  5, 14, 38, 17, 39, 16,\n",
      "       27, 26,  6, 10, 30, 28]), array([0.669756  , 0.191151  , 0.1545475 , 0.149287  , 0.1428195 ,\n",
      "       0.137335  , 0.133648  , 0.12575   , 0.0990114 , 0.0924162 ,\n",
      "       0.09050895, 0.0851386 , 0.0841591 , 0.0820595 , 0.08179415,\n",
      "       0.0782787 , 0.07667025, 0.074077  , 0.0712647 , 0.0711911 ,\n",
      "       0.0710468 , 0.070076  , 0.0686449 , 0.0685909 , 0.0685367 ,\n",
      "       0.0684699 , 0.06735735, 0.0620891 , 0.0613888 , 0.0568314 ,\n",
      "       0.0533108 , 0.052925  , 0.0526946 , 0.0500996 , 0.0498679 ,\n",
      "       0.0469414 , 0.0448609 , 0.0387951 , 0.029109  , 0.0148797 ]))\n",
      "Sizes = [4383, 6876, 362, 131, 558, 5779, 71, 19, 1995, 2929, 506, 745, 128, 1531, 906, 29630, 108, 55, 267, 207, 47, 21, 571, 842, 389, 159, 62, 195, 259, 280, 141, 53, 72, 157, 49, 91, 99, 105, 25, 35]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [2]\n",
      "Inside sphere validater: ratio_val = 0.06344271260424493\n",
      "----- working on mesh after poisson #2: <trimesh.Trimesh(vertices.shape=(44644, 3), faces.shape=(89284, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_2_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_2_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_2_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_2_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00021123886108398438\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10773887713300684802_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 2.050935745239258\n",
      "2) Finished: Generating CGAL segmentation for neuron: 2.3611807823181152\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.007668256759643555\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 4.410743713378906e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.01940178871154785\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.10446977615356445\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 0,  1,  2,  9, 13, 12,  4, 10, 14,  7,  6,  3,  8,  5, 11]), array([0.88391   , 0.3791735 , 0.207295  , 0.1382155 , 0.079689  ,\n",
      "       0.0738344 , 0.06865105, 0.0648424 , 0.0585631 , 0.0561739 ,\n",
      "       0.05012795, 0.04851155, 0.0448781 , 0.0399123 , 0.0372798 ]))\n",
      "Sizes = [3044, 460, 687, 248, 2523, 397, 4094, 2270, 665, 921, 1596, 2772, 1321, 1031, 291]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "Inside sphere validater: ratio_val = -0.2898633068607502\n",
      "----- working on mesh after poisson #3: <trimesh.Trimesh(vertices.shape=(44175, 3), faces.shape=(88350, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_3_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_3_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_3_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_3_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0002205371856689453\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10773887713300684803_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 1.9794859886169434\n",
      "2) Finished: Generating CGAL segmentation for neuron: 2.2922849655151367\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 8\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.007959604263305664\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 4.2438507080078125e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.01859426498413086\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.1039433479309082\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 8, 11,  9, 12,  6,  7, 10,  5, 13,  3,  2,  4,  0,  1, 15, 14]), array([0.813247  , 0.1122015 , 0.1017345 , 0.0983404 , 0.0875427 ,\n",
      "       0.05975405, 0.0577842 , 0.05605775, 0.0533564 , 0.0521652 ,\n",
      "       0.0437163 , 0.0427301 , 0.0420405 , 0.0401894 , 0.0368083 ,\n",
      "       0.0235886 ]))\n",
      "Sizes = [3833, 492, 3500, 357, 1419, 296, 229, 2786, 2246, 1885, 385, 1228, 767, 2551, 49, 63]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [8]\n",
      "Inside sphere validater: ratio_val = -8.296182675770174\n",
      "----- working on mesh after poisson #4: <trimesh.Trimesh(vertices.shape=(5135, 3), faces.shape=(10278, 3))>\n",
      "done exporting /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_4_largest_inner.off\n",
      "xvfb-run -a -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_4_largest_inner.off -o /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_4_largest_inner_decimated.off -s /notebooks/Platinum_Blender/Brendan_Soma/decimation_meshlab_0_25.mls\n",
      "done exporting decimated mesh: /notebooks/Platinum_Blender/Brendan_Soma/107738877133006848/107738877133006848_decimated_0_largest_piece_mls_4_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00021576881408691406\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Blender/Brendan_Soma/temp/10773887713300684804_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.2450270652770996\n",
      "2) Finished: Generating CGAL segmentation for neuron: 0.2853696346282959\n",
      "3) Staring: Generating Graph Structure and Identifying Soma\n",
      "my_list_keys = [0, 1, 2, 3, 4]\n",
      "soma_index = -1\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.0009508132934570312\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 4.7206878662109375e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.0023179054260253906\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.011992216110229492\n",
      "Returning the soma_sdf value AND the classifier\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([0, 3, 2, 1, 4]), array([0.488016 , 0.140605 , 0.0934171, 0.055946 , 0.0270915]))\n",
      "Sizes = [2445, 63, 35, 14, 11]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "Inside sphere validater: ratio_val = -13.636990256978942\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 254.96419215202332\n",
      "Run time was 254.964186668396  and the total_soma_list = [<trimesh.Trimesh(vertices.shape=(2281, 3), faces.shape=(4383, 3))>, <trimesh.Trimesh(vertices.shape=(1568, 3), faces.shape=(3044, 3))>, <trimesh.Trimesh(vertices.shape=(1936, 3), faces.shape=(3833, 3))>, <trimesh.Trimesh(vertices.shape=(1225, 3), faces.shape=(2445, 3))>] and \n",
      "Trying to write off file\n",
      "Trying to write off file\n",
      "Trying to write off file\n",
      "Trying to write off file\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "to prevent writing SOMAS NOT EQUAL TO That registered in datase",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c7fecc8d12e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mMultiSomaCentroidValidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreserve_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total time for MultiSomaCentroidValidation populate = {time.time() - start_time}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datajoint/autopopulate.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, suppress_errors, return_exception_objects, reserve_jobs, order, limit, max_calls, display_progress, *restrictions)\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_insert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                         \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f2e2350d0e46>\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_soma_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoma_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"to prevent writing SOMAS NOT EQUAL TO That registered in datase\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicts_to_insert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: to prevent writing SOMAS NOT EQUAL TO That registered in datase"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actually will do the population\n",
    "\"\"\"\n",
    "\n",
    "#(schema.jobs & \"table_name='__whole_auto_annotations_label_clusters3'\")#.delete()\n",
    "dj.config[\"enable_python_native_blobs\"] = True\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "MultiSomaCentroidValidation.populate(reserve_jobs=True)\n",
    "print(f\"Total time for MultiSomaCentroidValidation populate = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config[\"display.limit\"] = 100\n",
    "m65.SomaCentroidValidation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
