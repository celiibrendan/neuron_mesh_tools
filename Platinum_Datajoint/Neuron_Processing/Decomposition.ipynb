{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: To Run the neuron preprocessing\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "from importlib import reload\n",
    "\n",
    "import datajoint as dj\n",
    "from pathlib import Path\n",
    "\n",
    "import datajoint_utils as du\n",
    "du = reload(du)\n",
    "from importlib import reload\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuring the virtual module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minfig\n",
    "du.config_celii()\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)\n",
    "\n",
    "#configuring will include the adapters\n",
    "minnie,schema = du.configure_minnie_vm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Our Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_utils as nru\n",
    "import neuron\n",
    "import trimesh_utils as tu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so that it will have the adapter defined\n",
    "from datajoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "@schema\n",
    "class Decomposition(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation\n",
    "    ---\n",
    "    decomposition: <decomposition>\n",
    "    n_error_limbs: int #the number of branches that are touching multiple somas or 1 soma in multiple places\n",
    "    n_somas: int #number of soma meshes detected\n",
    "    n_limbs: int\n",
    "    n_branches: int\n",
    "    max_limb_n_branches:int\n",
    "    \n",
    "    skeletal_length: double\n",
    "    max_limb_skeletal_length:double\n",
    "    median_branch_length:double #gives information on average skeletal length to next branch point\n",
    "    \n",
    "    \n",
    "    width_median: double #median width from mesh center without spines removed\n",
    "    width_no_spine_median: double #median width from mesh center with spines removed\n",
    "    width_90_perc: double # 90th percentile for width without spines removed\n",
    "    width_no_spine_90_perc: double  # 90th percentile for width with spines removed\n",
    "    \n",
    "    \n",
    "    n_spines: bigint\n",
    "\n",
    "    spine_density: double # n_spines/ skeletal_length\n",
    "    spines_per_branch: double\n",
    "    \n",
    "    skeletal_length_eligible: double # the skeletal length for all branches searched for spines\n",
    "    n_spine_eligible_branches; int # the number of branches that were checked for spines because passed width threshold\n",
    "    \n",
    "    spine_density_eligible:double # n_spines/skeletal_length_eligible\n",
    "    spines_per_branch_eligible:double # n_spines/n_spine_eligible_branches\n",
    "    \n",
    "    total_spine_volume: double # the sum of all spine volume\n",
    "    spine_volume_density: double #total_spine_volume/skeletal_length\n",
    "    spine_volume_density_eligible: double: #total_spine_volume/skeletal_length_eligible\n",
    "    spine_volume_per_branch_eligible: double #total_spine_volume/n_spine_eligible_branche\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    key_source = minnie.Decimation() & (minnie.BaylorSegmentCentroid() & \"multiplicity>0\").proj()\n",
    "\n",
    "    def make(self,key):\n",
    "        \"\"\"\n",
    "        Pseudocode for process:\n",
    "\n",
    "        1) Get the segment id from the key\n",
    "        2) Get the decimated mesh\n",
    "        3) Get the somas info\n",
    "        4) Run the preprocessing\n",
    "        5) Calculate all starter stats\n",
    "        6) Save the file in a certain location\n",
    "        7) Pass stats and file location to insert\n",
    "        \"\"\"\n",
    "        #1) Get the segment id from the key\n",
    "        segment_id = key[\"segment_id\"]\n",
    "        description = key['version']\n",
    "        print(f\"\\n\\n----- Working on {segment_id}-------\")\n",
    "\n",
    "        #2) Get the decimated mesh\n",
    "        current_neuron_mesh = du.fetch_segment_id_mesh(segment_id)\n",
    "\n",
    "        #3) Get the somas info *************************** Need to change this when actually run *******************\n",
    "        #somas = du.get_soma_mesh_list(segment_id) \n",
    "        somas = None\n",
    "\n",
    "        #4) Run the preprocessing\n",
    "\n",
    "\n",
    "        total_neuron_process_time = time.time()\n",
    "\n",
    "        print(f\"\\n--- Beginning preprocessing of {segment_id}---\")\n",
    "        recovered_neuron = neuron.Neuron(\n",
    "        mesh = current_neuron_mesh,\n",
    "        somas = somas,\n",
    "        segment_id=segment_id,\n",
    "        description=description,\n",
    "        suppress_preprocessing_print=False,\n",
    "        suppress_output=False,\n",
    "        calculate_spines=True,\n",
    "        widths_to_calculate=[\"no_spine_median_mesh_center\"]\n",
    "\n",
    "                )\n",
    "\n",
    "        print(f\"\\n\\n\\n---- Total preprocessing time = {time.time() - total_neuron_process_time}\")\n",
    "\n",
    "\n",
    "        #5) Calculate all starter stats\n",
    "        # ----------- calculating the attributes --------- #\n",
    "        n_error_limbs = len(nru.error_limb_indexes(recovered_neuron))\n",
    "\n",
    "        n_somas = len(recovered_neuron.get_soma_node_names())\n",
    "\n",
    "        n_limbs = len(recovered_neuron.get_limb_node_names())\n",
    "\n",
    "        n_branches_per_limb = [len(ex_limb.get_branch_names()) for ex_limb in recovered_neuron]\n",
    "        n_branches = np.sum(n_branches_per_limb)\n",
    "\n",
    "        n_spines = len(recovered_neuron.spines)\n",
    "\n",
    "        #for total skeletal length\n",
    "        sk_len_per_limb = [sk.calculate_skeleton_distance(limb.skeleton) for limb in recovered_neuron]\n",
    "        skeletal_length = np.sum(sk_len_per_limb)\n",
    "\n",
    "        if skeletal_length > 0:\n",
    "            spine_density = n_spines/skeletal_length\n",
    "        else:\n",
    "            spine_density = 0\n",
    "\n",
    "        max_limb_skeletal_length = np.max(sk_len_per_limb)\n",
    "\n",
    "        max_limb_n_branches = np.max(n_branches_per_limb)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #5b) --------------- Compute the stats suggested by Jake -------------------------\n",
    "        \n",
    "        \n",
    "        # --> preprocessing\n",
    "        all_skeletal_lengths = []\n",
    "        all_widths_no_spine = []\n",
    "        all_widths = []\n",
    "\n",
    "        n_spine_eligible_branches = 0\n",
    "        spine_eligible_branch_lengths = []\n",
    "\n",
    "        for curr_limb in recovered_neuron:\n",
    "            for curr_branch in curr_limb:\n",
    "                curr_branch_sk_len = sk.calculate_skeleton_distance(curr_branch.skeleton)\n",
    "                all_skeletal_lengths.append(curr_branch_sk_len)\n",
    "                all_widths_no_spine.append(curr_branch.width_new[\"no_spine_median_mesh_center\"])\n",
    "                all_widths.append(curr_branch.width_new[\"median_mesh_center\"])\n",
    "\n",
    "                if not curr_branch.spines is None:\n",
    "                    spine_eligible_branch_lengths.append(curr_branch_sk_len)\n",
    "                    n_spine_eligible_branches += 1\n",
    "\n",
    "        all_skeletal_lengths = np.array(all_skeletal_lengths)\n",
    "        median_branch_length = np.round(np.median(all_skeletal_lengths),3)\n",
    "        \n",
    "        \n",
    "        #--> width data\n",
    "        width_median = np.round(np.median(all_widths),3)\n",
    "        width_no_spine_median = np.round(np.median(all_widths_no_spine),3)\n",
    "\n",
    "        width_90_perc = np.round(np.percentile(all_widths,90),3)\n",
    "        width_no_spine_90_perc = np.round(np.percentile(all_widths_no_spine,90),3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # -->  spine data\n",
    "        if n_branches > 0:\n",
    "            spines_per_branch = np.round(n_spines/n_branches,3)\n",
    "        else:\n",
    "            spines_per_branch = 0\n",
    "        \n",
    "        #spine eligible density and per branch\n",
    "        skeletal_length_eligible = np.sum(spine_eligible_branch_lengths)\n",
    "        if skeletal_length_eligible > 0:\n",
    "            spine_density_eligible = n_spines/skeletal_length_eligible\n",
    "        else:\n",
    "            spine_density_eligible = 0\n",
    "\n",
    "        if n_branches > 0:\n",
    "            spines_per_branch_eligible = np.round(n_spines/n_spine_eligible_branches,3)\n",
    "        else:\n",
    "            spines_per_branch_eligible = 0\n",
    "\n",
    "        skeletal_length_eligible,n_spine_eligible_branches,spine_density_eligible,spines_per_branch_eligible\n",
    "        \n",
    "        \n",
    "        spine_time = time.time()\n",
    "        total_spines = recovered_neuron.spines\n",
    "        total_volume =[]\n",
    "        for ts in total_spines:\n",
    "            vol = tu.mesh_volume(total_spines[0],\n",
    "                                     watertight_method=None,\n",
    "                                     return_closed_mesh=False,\n",
    "                             verbose=False)\n",
    "            total_volume.append(vol)\n",
    "\n",
    "        print(f\"Total time for calculating spine volume = {time.time() - spine_time}\")\n",
    "        total_volume = np.array(total_volume)\n",
    "\n",
    "        spine_volume_median = np.median(total_volume)\n",
    "\n",
    "        total_spine_volume = np.sum(total_volume)\n",
    "\n",
    "        if skeletal_length > 0:\n",
    "            spine_volume_density = total_spine_volume/skeletal_length\n",
    "        else:\n",
    "            spine_volume_density = 0\n",
    "\n",
    "\n",
    "        if skeletal_length_eligible > 0:\n",
    "            spine_volume_density_eligible = total_spine_volume/skeletal_length_eligible\n",
    "        else:\n",
    "            spine_volume_density_eligible = 0\n",
    "\n",
    "        if n_branches > 0:\n",
    "            spine_volume_per_branch_eligible = np.round(total_spine_volume/n_spine_eligible_branches,3)\n",
    "        else:\n",
    "            spine_volume_per_branch_eligible = 0\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #6) Save the file in a certain location\n",
    "        save_time = time.time()\n",
    "        ret_file_path = recovered_neuron.save_compressed_neuron(output_folder=str(du.get_decomposition_path()),\n",
    "                                          return_file_path=True,\n",
    "                                         export_mesh=False,\n",
    "                                         suppress_output=True)\n",
    "\n",
    "        ret_file_path_str = str(ret_file_path.absolute()) + \".pbz2\"\n",
    "        print(f\"Save time = {time.time() - save_time}\")\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        n_error_limbs: int #the number of branches that are touching multiple somas or 1 soma in multiple places\n",
    "        n_somas: int #number of soma meshes detected\n",
    "        n_limbs: int\n",
    "        n_branches: int\n",
    "        max_limb_n_branches:int\n",
    "\n",
    "        skeletal_length: double\n",
    "        max_limb_skeletal_length:double\n",
    "        median_branch_length:double #gives information on average skeletal length to next branch point\n",
    "\n",
    "\n",
    "        width_median: double #median width from mesh center without spines removed\n",
    "        width_no_spine_median: double #median width from mesh center with spines removed\n",
    "        width_90_perc: double # 90th percentile for width without spines removed\n",
    "        width_no_spine_90_perc: double  # 90th percentile for width with spines removed\n",
    "\n",
    "\n",
    "        n_spines: bigint\n",
    "\n",
    "        spine_density: double # n_spines/ skeletal_length\n",
    "        spines_per_branch: double\n",
    "\n",
    "        skeletal_length_eligible: double # the skeletal length for all branches searched for spines\n",
    "        n_spine_eligible_branches; int # the number of branches that were checked for spines because passed width threshold\n",
    "\n",
    "        spine_density_eligible:double # n_spines/skeletal_length_eligible\n",
    "        spines_per_branch_eligible:double # n_spines/n_spine_eligible_branches\n",
    "\n",
    "        total_spine_volume: double # the sum of all spine volume\n",
    "        spine_volume_density: double #total_spine_volume/skeletal_length\n",
    "        spine_volume_density_eligible: double: #total_spine_volume/skeletal_length_eligible\n",
    "        spine_volume_per_branch_eligible: double #total_spine_volume/n_spine_eligible_branche\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        #7) Pass stats and file location to insert\n",
    "        new_key = dict(key,\n",
    "                       decomposition=ret_file_path_str,\n",
    "                        n_error_limbs=n_error_limbs,\n",
    "                        n_somas=n_somas,\n",
    "                        n_limbs=n_limbs,\n",
    "                        n_branches=n_branches,\n",
    "                        max_limb_n_branches=max_limb_n_branches,\n",
    "                       \n",
    "                        skeletal_length=skeletal_length,\n",
    "                        max_limb_skeletal_length=max_limb_skeletal_length,\n",
    "                        median_branch_length=median_branch_length,\n",
    "\n",
    "                        width_median=width_median, #median width from mesh center without spines removed\n",
    "                        width_no_spine_median=width_no_spine_median, #median width from mesh center with spines removed\n",
    "                        width_90_perc=width_90_perc, # 90th percentile for width without spines removed\n",
    "                        width_no_spine_90_perc=width_no_spine_90_perc,  # 90th percentile for width with spines removed\n",
    "\n",
    "                        n_spines=n_spines,\n",
    "\n",
    "                        spine_density=spine_density, # n_spines/ skeletal_length\n",
    "                        spines_per_branch=spines_per_branch,\n",
    "\n",
    "                        skeletal_length_eligible=skeletal_length_eligible, # the skeletal length for all branches searched for spines\n",
    "                        n_spine_eligible_branches=n_spine_eligible_branches,\n",
    "\n",
    "                        total_spine_volume=total_spine_volume, # the sum of all spine volume\n",
    "                        spine_volume_density=spine_volume_density, #total_spine_volume/skeletal_length\n",
    "                        spine_volume_density_eligible=spine_volume_density_eligible, #total_spine_volume/skeletal_length_eligible\n",
    "                        spine_volume_per_branch_eligible=spine_volume_per_branch_eligible, #total_spine_volume/n_spine_eligible_branche\n",
    "\n",
    "                       \n",
    "                      )\n",
    "#         else:\n",
    "#             ret_file_path_str = \"/mnt/dj-stor01/platinum/minnie65/02/decomposition/864691135574982852_0.pbz2\"\n",
    "#             new_key = dict(key,\n",
    "#                            decomposition=ret_file_path_str,\n",
    "#                             n_error_limbs=0,\n",
    "#                             n_somas=0,\n",
    "#                             n_limbs=0,\n",
    "#                             n_branches=0,\n",
    "#                             n_spines=0,\n",
    "#                             skeletal_length=0,\n",
    "#                             spine_density=0,\n",
    "#                             max_limb_skeletal_length=0,\n",
    "#                             max_limb_n_branches=0\n",
    "#                           )\n",
    "        \n",
    "        \n",
    "        self.insert1(new_key, allow_direct_insert=True, skip_duplicates=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minnie.Decimation & (minnie.BaylorSegmentCentroid() & \"multiplicity>=2\").proj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((schema.jobs & \"table_name = '__decomposition'\") & \"timestamp>'2020-11-16 00:26:00'\").delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errored_keys = (schema.jobs & \"table_name = '__decomposition'\").fetch(\"key\")#.delete()\n",
    "# errored_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import compartment_utils as cu\n",
    "cu = reload(cu)\n",
    "import preprocessing_vp2 as pre\n",
    "pre = reload(pre)\n",
    "import trimesh as tu\n",
    "tu = reload(tu)\n",
    "\n",
    "Decomposition.populate(reserve_jobs=True, suppress_errors=False, order='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_objects[\"decomposition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "debugging the insert\n",
    "saved path: /mnt/dj-stor01/platinum/minnie65/02/decomposition/864691135574982852_0.pbz2\n",
    "self.spec['stage'] did not have a stage key\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import system_utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh_utils as tu\n",
    "main_mesh = tu.load_mesh_no_processing(\"main_mesh\")\n",
    "current_soma = tu.load_mesh_no_processing(\"current_soma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_visualizations as nviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nviz.plot_objects(meshes=[main_mesh,current_soma],\n",
    "                 meshes_colors=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitching_mesh = su.decompress_pickle(\"stitching_mesh.pbz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh_utils as tu\n",
    "tu.split(stitching_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_visualizations as nviz\n",
    "nviz.plot_objects(submesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
