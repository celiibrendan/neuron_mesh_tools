{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To Run the Error Labeling Pipeline\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "\n",
    "import datajoint_utils as du\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuring the virtual module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minfig\n",
    "import time\n",
    "import numpy as np\n",
    "#want to add in a wait for the connection part\n",
    "random_sleep_sec = np.random.randint(0, 400)\n",
    "print(f\"Sleeping {random_sleep_sec} sec before conneting\")\n",
    "\n",
    "if not test_mode:\n",
    "    time.sleep(random_sleep_sec)\n",
    "\n",
    "print(\"Done sleeping\")\n",
    "import datajoint_utils as du\n",
    "du.config_celii()\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)\n",
    "\n",
    "#configuring will include the adapters\n",
    "success_flag = False\n",
    "for i in range(10):\n",
    "    try:\n",
    "        minnie,schema = du.configure_minnie_vm()\n",
    "        \n",
    "    except:\n",
    "        print(\"Locked out trying agin in 30 seconds\")\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        success_flag = True\n",
    "        \n",
    "        \n",
    "    if success_flag:\n",
    "        print(\"successfully configured minnie\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Our Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_utils as nru\n",
    "import neuron\n",
    "import trimesh_utils as tu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so that it will have the adapter defined\n",
    "from datajoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import error_detection as ed\n",
    "ed = reload(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "\n",
    "@schema\n",
    "class AutoProofreadLabels(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decomposition\n",
    "    ---\n",
    "    n_face_errors : int #the number of faces that were errored out\n",
    "    face_idx_for_error : longblob #the face indices for the errors computed\n",
    "    n_synapses: smallint unsigned #total number of synpases\n",
    "    n_errored_synapses: smallint unsigned #the number of synapses\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    key_source = (minnie.Decomposition() & \"n_somas = 1\" & \"n_faces>500000\")\n",
    "                  \n",
    "    def make(self,key):\n",
    "        global_start = time.time()\n",
    "        segment_id = key[\"segment_id\"]\n",
    "        verbose = True\n",
    "        \n",
    "        print(f\"\\n\\n----- Working on {segment_id}-------\")\n",
    "        whole_pass_time = time.time()\n",
    "        \n",
    "        #new method that checks if the information exists in the error table and if not then \n",
    "        error_table = (minnie.DecompositionErrorLabels() & dict(segment_id=segment_id))\n",
    "        if len(error_table)>0:\n",
    "            print(\"using quick fetch\")\n",
    "            current_mesh = du.fetch_segment_id_mesh(segment_id)\n",
    "            returned_error_faces = error_table.fetch1(\"face_idx_for_error\")\n",
    "            \n",
    "        else:\n",
    "            neuron_obj = (minnie.Decomposition() & key).fetch1(\"decomposition\")\n",
    "\n",
    "            returned_error_faces = ed.error_faces_by_axons(neuron_obj,verbose=True,visualize_errors_at_end=False)\n",
    "            current_mesh = neuron_obj.mesh\n",
    "            \n",
    "        #------- Doing the synapse Exclusion Writing ---------- #\n",
    "        data_to_write_new,n_synapses,n_errored_synapses = ed.get_error_synapse_inserts(current_mesh,\n",
    "                                                                                       segment_id,\n",
    "                                                                                       returned_error_faces,minnie=minnie,\n",
    "                                                         return_synapse_stats=True,\n",
    "                                                         verbose=True)\n",
    "        \n",
    "        if len(data_to_write_new)>0:\n",
    "            print(\"Preparing to write errored synapses\")\n",
    "            minnie.SynapseExclude.insert(data_to_write_new,skip_duplicates=True)\n",
    "            \n",
    "        #------- Doing the Label Writing ---------- #\n",
    "        new_key = dict(key,\n",
    "                       n_face_errors = len(returned_error_faces),\n",
    "                       face_idx_for_error = returned_error_faces,\n",
    "                        n_synapses=n_synapses,\n",
    "                        n_errored_synapses=n_errored_synapses)\n",
    "        \n",
    "        \n",
    "        self.insert1(new_key, allow_direct_insert=True, skip_duplicates=True)\n",
    "        \n",
    "        print(f\"\\n\\n ------ Total time for {segment_id} = {time.time() - global_start} ------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(schema.jobs & \"table_name='__decomposition_error_labels'\").delete()\n",
    "#minnie.SynapseExclude.delete()\n",
    "#minnie.DecompositionErrorLabels.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "start_time = time.time()\n",
    "if not test_mode:\n",
    "    time.sleep(random.randint(0, 900))\n",
    "print('Populate Started')\n",
    "if test_mode:\n",
    "    AutoProofreadLabels.populate(reserve_jobs=True, suppress_errors=False)\n",
    "else:\n",
    "    AutoProofreadLabels.populate(reserve_jobs=True, suppress_errors=True)\n",
    "print('Populate Done')\n",
    "\n",
    "print(f\"Total time for DecompositionErrorLabels populate = {time.time() - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
