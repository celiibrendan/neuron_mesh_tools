{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: To practice retrieving decimated meshes\n",
    "from the segmentation = 3 version of the Platinum data\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules for Datajoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Need to pip install annotationframeworkclient to repair mesh with pychunkedgraph\n",
      "WARNING:root:Need to pip install annotationframeworkclient to use dataset_name parameters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "\n",
    "import datajoint_utils as du\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path for external_segmentation_path = /mnt/dj-stor01/platinum/minnie65/02\n",
      "Current path for external_mesh_path = /mnt/dj-stor01/platinum/minnie65/02/meshes\n",
      "Current path for external_decimated_mesh_path = /mnt/dj-stor01/platinum/minnie65/02/decimated_meshes\n",
      "Current path for external_skeleton_path = /mnt/dj-stor01/platinum/minnie65/02/skeletons\n"
     ]
    }
   ],
   "source": [
    "import minfig\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datajoint.settings:Setting enable_python_native_blobs to True\n",
      "INFO:datajoint.settings:Setting enable_python_native_blobs to True\n"
     ]
    }
   ],
   "source": [
    "minnie = minfig.configure_minnie(return_virtual_module=True)\n",
    "\n",
    "# Old way of getting access to the virtual modules\n",
    "# m65 = dj.create_virtual_module('minnie', 'microns_minnie65_02')\n",
    "\n",
    "#New way of getting access to module\n",
    "import datajoint as dj\n",
    "from minfig import adapter_objects # included with wildcard imports\n",
    "minnie = dj.create_virtual_module('minnie', 'microns_minnie65_02', add_objects=adapter_objects)\n",
    "\n",
    "schema = dj.schema(\"microns_minnie65_02\")\n",
    "dj.config[\"enable_python_native_blobs\"] = True\n",
    "#(schema.jobs & \"table_name='__baylor_segment_centroid_seg3'\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.ERD(minnie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules for Soma Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soma_extraction_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The table that will do the soma extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "\n",
    "@schema\n",
    "class BaylorSegmentCentroid(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation.proj(decimation_version='version')\n",
    "    soma_index : tinyint unsigned #index given to this soma to account for multiple somas in one base semgnet\n",
    "    ---\n",
    "    centroid_x=NULL           : int unsigned                 # (EM voxels)\n",
    "    centroid_y=NULL           : int unsigned                 # (EM voxels)\n",
    "    centroid_z=NULL           : int unsigned                 # (EM voxels)\n",
    "    n_vertices=NULL           : bigint                 #number of vertices\n",
    "    n_faces=NULL            : bigint                  #number of faces\n",
    "    soma_vertices=NULL        : longblob                # array of vertices\n",
    "    soma_faces=NULL           : longblob                   # array of faces\n",
    "    multiplicity=NULL         : tinyint unsigned             # the number of somas found for this base segment\n",
    "    sdf=NULL                  : double                       # sdf width value for the soma\n",
    "    max_side_ratio=NULL       : double                       # the maximum of the side length ratios used for check if soma\n",
    "    bbox_volume_ratio=NULL    : double                       # ratio of bbox (axis aligned) volume to mesh volume to use for check if soma\n",
    "    run_time=NULL : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \"\"\"\n",
    "    # this size restriction is already enforced in the meshes that were \n",
    "    segments = (minnie.Mesh() & f'n_vertices  > {np.str(np.round(np.exp(12)).astype(np.int))}' & f'n_vertices  < {np.str(np.round(np.exp(15)).astype(np.int))}')\n",
    "    key_source =  minnie.Decimation.proj(decimation_version='version') & segments.proj() & \"decimation_version=\" + str(decimation_version)\n",
    "    \n",
    "    decimation_ratio = 0.25\n",
    "    decimation_version = 0\n",
    "\n",
    "    valid_segment_ids_with_nucleus_id = dj.U(\"segment_id\") & (minnie.NucleusID() & \"segment_id>0\")\n",
    "    segments = (minnie.Mesh())# & f'n_vertices  > {np.str(np.round(np.exp(12)).astype(np.int))}' & f'n_vertices  < {np.str(np.round(np.exp(15)).astype(np.int))}')\n",
    "    key_source =  (minnie.Decimation.proj(decimation_version='version') \n",
    "                & segments.proj() \n",
    "                & f\"decimation_ratio={decimation_ratio}\" \n",
    "                & f\"decimation_version={decimation_version}\" \n",
    "                & valid_segment_ids_with_nucleus_id)\n",
    "    key_source\n",
    "    \n",
    "    \n",
    "    \n",
    "    def make(self,key):\n",
    "        #get the mesh data\n",
    "        print(f\"\\n\\n\\n---- Working on {key['segment_id']} ----\")\n",
    "\n",
    "        new_mesh = (minnie.Decimation() & key).fetch1(\"mesh\")\n",
    "        current_mesh_verts,current_mesh_faces = new_mesh.vertices,new_mesh.faces\n",
    "\n",
    "        segment_id = key[\"segment_id\"]\n",
    "\n",
    "        (total_soma_list, \n",
    "         run_time, \n",
    "         total_soma_list_sdf) = extract_soma_center(\n",
    "                            segment_id,\n",
    "                            current_mesh_verts,\n",
    "                            current_mesh_faces,\n",
    "        )\n",
    "        \n",
    "        print(f\"Run time was {run_time} \\n    total_soma_list = {total_soma_list}\"\n",
    "             f\"\\n    with sdf values = {total_soma_list_sdf}\")\n",
    "        \n",
    "        #check if soma list is empty and did not find soma\n",
    "        if len(total_soma_list) <= 0:\n",
    "            print(\"There were no somas found for this mesh so just writing empty data\")\n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=0,\n",
    "                              centroid_x=None,\n",
    "                               centroid_y=None,\n",
    "                               centroid_z=None,\n",
    "                               #distance_from_prediction=None,\n",
    "                               #prediction_matching_index = None,\n",
    "                               n_vertices=0,\n",
    "                               n_faces=0,\n",
    "                               soma_vertices=None,\n",
    "                               soma_faces=None,\n",
    "                               multiplicity=0,\n",
    "                               sdf = None,\n",
    "                               max_side_ratio = None,\n",
    "                               bbox_volume_ratio = None,\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            #raise Exception(\"to prevent writing because none were found\")\n",
    "            self.insert1(insert_dict,skip_duplicates=True)\n",
    "            return\n",
    "        \n",
    "        #if there is one or more soma found, get the volume and side length checks\n",
    "        max_side_ratio =  [np.max(side_length_ratios(m)) for m in total_soma_list]\n",
    "        bbox_volume_ratio =  [soma_volume_ratio(m) for m in total_soma_list]\n",
    "        dicts_to_insert = []\n",
    "\n",
    "\n",
    "        for i,(current_soma,soma_sdf,sz_ratio,vol_ratio) in enumerate(zip(total_soma_list,total_soma_list_sdf,max_side_ratio,bbox_volume_ratio)):\n",
    "            print(\"Trying to write off file\")\n",
    "            \"\"\" Currently don't need to export the meshes\n",
    "            current_soma.export(f\"{key['segment_id']}/{key['segment_id']}_soma_{i}.off\")\n",
    "            \"\"\"\n",
    "            auto_prediction_center = np.mean(current_soma.vertices,axis=0) / np.array([4,4,40])\n",
    "            auto_prediction_center = auto_prediction_center.astype(\"int\")\n",
    "            print(f\"Predicted Coordinates are {auto_prediction_center}\")\n",
    "\n",
    "\n",
    "\n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=i+1,\n",
    "                              centroid_x=auto_prediction_center[0],\n",
    "                               centroid_y=auto_prediction_center[1],\n",
    "                               centroid_z=auto_prediction_center[2],\n",
    "                               n_vertices = len(current_soma.vertices),\n",
    "                               n_faces = len(current_soma.faces),\n",
    "                               soma_vertices=current_soma.vertices,\n",
    "                               soma_faces=current_soma.faces,\n",
    "                               multiplicity=len(total_soma_list),\n",
    "                               sdf = np.round(soma_sdf,3),\n",
    "                               max_side_ratio = np.round(sz_ratio,3),\n",
    "                               bbox_volume_ratio = np.round(vol_ratio,3),\n",
    "                               run_time=np.round(run_time,4)\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "            dicts_to_insert.append(insert_dict)\n",
    "\n",
    "        self.insert(dicts_to_insert,skip_duplicates=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(schema.jobs & \"table_name='__baylor_segment_centroid'\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "BaylorSegmentCentroid.populate(reserve_jobs=True)\n",
    "print(f\"Total time for BaylorSegmentCentroid populate = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
