{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Try the skeletonization that uses both the meshparty and the meshafterparty\n",
    "\n",
    "Pseudocode: \n",
    "1) Do MP skeletonization and mesh corresondence that divides it into branches\n",
    "2) Find all of the pices that need MAP skeletonization\n",
    "and then combine them into connected component meshes\n",
    "3) Do MAP skeletonization and mesh correspondence for those larger pieces\n",
    "4) For each MP connected skeleton:\n",
    "a. Find the closest MAP skeleton branch endpoint and add stitch to the appropriate branch\n",
    "  of the MP skeleton\n",
    "5) Check all of skeleton is connected:\n",
    "a. If No --> then stitch until fully connected and add stitching point \n",
    "to the smaller widths of the branches\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Need to pip install annotationframeworkclient to repair mesh with pychunkedgraph\n",
      "WARNING:root:Need to pip install annotationframeworkclient to use dataset_name parameters\n"
     ]
    }
   ],
   "source": [
    "import trimesh_utils as tu\n",
    "import meshparty_skeletonize as m_sk\n",
    "import neuron_utils as nru\n",
    "from meshparty import trimesh_io\n",
    "import neuron_visualizations as nviz\n",
    "import time\n",
    "import numpy as np\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing Neuron in minimal output mode...please wait\n"
     ]
    }
   ],
   "source": [
    "neur_file = \"/notebooks/test_neurons/Segmentation_2/meshparty/864691135548568516_single_soma_inhib_axon_cloud\"\n",
    "current_neuron = nru.decompress_neuron(filepath=neur_file,\n",
    "                      original_mesh=neur_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_limb = current_neuron[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_limb.current_touching_soma_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d747f60b64443c8b5c15ba485afec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nviz.plot_objects(meshes=curr_limb.mesh,\n",
    "                 scatters=[curr_limb.current_touching_soma_vertices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Working on visualization type: mesh\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1008c0293b44233815897f198ece2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nviz.visualize_neuron(current_neuron,\n",
    "                     limb_branch_dict=dict(L1=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting a limb to practice on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limb_obj = current_neuron[0]\n",
    "limb_obj.mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Do MP skeletonization and mesh corresondence that divides it into branches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "m_sk = reload(m_sk)\n",
    "tu = reload(tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will eventually get the current root from soma_to_piece_touching_vertices[i]\n",
    "root_curr = np.array(limb_obj.current_touching_soma_vertices[0])\n",
    "\n",
    "m_sk = reload(m_sk)\n",
    "sk_meshparty_obj,limb_mesh_mparty = m_sk.skeletonize_mesh_largest_component(limb_obj.mesh,\n",
    "                                                        root=root_curr)\n",
    "m_sk = reload(m_sk)\n",
    "\n",
    "(segment_branches, #skeleton branches\n",
    "divided_submeshes, divided_submeshes_idx, #mesh correspondence (mesh and indices)\n",
    "segment_widths_median) = m_sk.skeleton_obj_to_branches(sk_meshparty_obj,\n",
    "                                                      mesh = limb_mesh_mparty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the width threshold\n",
    "width_threshold = 450\n",
    "size_threshold = 1000\n",
    "\n",
    "pieces_above_threshold = np.where(segment_widths_median>width_threshold)[0]\n",
    "\n",
    "width_large = segment_widths_median[pieces_above_threshold]\n",
    "sk_large = [segment_branches[k] for k in pieces_above_threshold]\n",
    "mesh_large_idx = [divided_submeshes_idx[k] for k in pieces_above_threshold]\n",
    "\n",
    "mesh_large_connectivity = tu.mesh_list_connectivity(meshes = mesh_large_idx,\n",
    "                        main_mesh = limb_mesh_mparty,\n",
    "                        print_flag = False)\n",
    "\"\"\"\n",
    "Pseudocode: \n",
    "1) build a networkx graph with all nodes for mesh_large_idx indexes\n",
    "2) Add the edges\n",
    "3) Find the connected components\n",
    "4) Find sizes of connected components\n",
    "5) For all those connected components that are of a large enough size, \n",
    "add the mesh branches and skeletons to the final list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(np.arange(len(mesh_large_idx)))\n",
    "G.add_edges_from(mesh_large_connectivity)\n",
    "conn_comp = list(nx.connected_components(G))\n",
    "\n",
    "filtered_pieces = []\n",
    "\n",
    "sk_large_size_filt = []\n",
    "mesh_large_idx_size_filt = []\n",
    "width_large_size_filt = []\n",
    "\n",
    "for cc in conn_comp:\n",
    "    total_cc_size = np.sum([len(mesh_large_idx[k]) for k in cc])\n",
    "    if total_cc_size>size_threshold:\n",
    "        #print(f\"cc ({cc}) passed the size threshold because size was {total_cc_size}\")\n",
    "        filtered_pieces.append(pieces_above_threshold[list(cc)])\n",
    "\n",
    "if len(filtered_pieces) > 0:\n",
    "    #all the pieces that will require MAP mesh correspondence and skeletonization\n",
    "    #(already organized into their components)\n",
    "    mesh_pieces_for_MAP = [limb_mesh_mparty.submesh([np.concatenate(divided_submeshes_idx[k])],append=True,repair=False) for k in filtered_pieces]\n",
    "\n",
    "    pieces_idx_MP = np.setdiff1d(np.arange(len(divided_submeshes_idx)),np.concatenate(filtered_pieces))\n",
    "    mesh_idx_MP = [divided_submeshes_idx[k] for k in pieces_idx_MP]\n",
    "\n",
    "    mesh_large_connectivity_MP = tu.mesh_list_connectivity(meshes = mesh_idx_MP,\n",
    "                            main_mesh = limb_mesh_mparty,\n",
    "                            print_flag = False)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(np.arange(len(mesh_idx_MP)))\n",
    "    G.add_edges_from(mesh_large_connectivity_MP)\n",
    "    sublimbs_MP = list(nx.connected_components(G))\n",
    "    sublimbs_MP_orig_idx = [pieces_idx_MP[list(k)] for k in sublimbs_MP]\n",
    "    \n",
    "    \n",
    "    #concatenate into sublimbs the skeletons and meshes\n",
    "    sublimb_mesh_idx_branches_MP = [divided_submeshes_idx[k] for k in sublimbs_MP_orig_idx]\n",
    "    sublimb_meshes_MP = [limb_mesh_mparty.submesh([np.concatenate(k)],append=True,repair=False)\n",
    "                                                 for k in sublimb_mesh_idx_branches_MP]\n",
    "    sublimb_skeleton_branches = [segment_branches[k] for k in sublimbs_MP_orig_idx]\n",
    "    widths_MP = [segment_widths_median[k] for k in sublimbs_MP_orig_idx]\n",
    "\n",
    "else: #if no pieces were determine to need MAP processing\n",
    "    print(\"No MAP processing needed: just returning the Meshparty skeletonization and mesh correspondence\")\n",
    "    raise Exception(\"Returning MP correspondence\")\n",
    "\n",
    "\n",
    "#         for indiv_cc in cc:\n",
    "#             sk_large_size_filt.append(sk_large[indiv_cc])\n",
    "#             mesh_large_idx_size_filt.append(mesh_large_idx[indiv_cc])\n",
    "#             width_large_size_filt.append(width_large[indiv_cc])\n",
    "\n",
    "# nviz.plot_objects(main_mesh=tu.combine_meshes([limb_mesh_mparty,current_neuron[\"S0\"].mesh]),\n",
    "#                   main_mesh_color=\"green\",\n",
    "#     skeletons=sk_large_size_filt,\n",
    "#      meshes=[limb_mesh_mparty.submesh([k],append=True) for k in mesh_large_idx_size_filt],\n",
    "#       meshes_colors=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skeleton_utils as sk\n",
    "start_time = time.time()\n",
    "skeletons_MAP = [sk.skeletonize_connected_branch(branch) for branch in mesh_pieces_for_MAP]\n",
    "print(f\"Total MAP skeleton time = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeletons_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find which pieces are actually touching the soma so know when to add soma extending piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the Mesh Correspondence for the skeletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm_utils import tqdm\n",
    "import compartment_utils as cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary mapping soma to its touching border vertices\n",
    "curr_soma_to_piece_touching_vertices = dict()\n",
    "curr_soma_to_piece_touching_vertices[0] = limb_obj.current_touching_soma_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Idea: Find which sublimb has the soma_touching_soma \n",
    "so can add on branch\n",
    "\n",
    "Pseudocode: \n",
    "1) get the vertices touching the soma\n",
    "2) Find the sublimbs that contain these vertices\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu = reload(cu)\n",
    "branch_skeletons_MAP = []\n",
    "branch_meshes_idx_MAP = []\n",
    "branch_meshes_MAP = []\n",
    "branch_widths_MAP = []\n",
    "distance_by_mesh_center = True\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for curr_limb_sk,curr_limb_mesh in zip(skeletons_MAP,mesh_pieces_for_MAP):\n",
    "    \n",
    "    filter_end_node_length=4001\n",
    "    distance_cleaned_skeleton = sk.clean_skeleton(curr_limb_sk,\n",
    "                            distance_func=sk.skeletal_distance,\n",
    "                      min_distance_to_junction=filter_end_node_length, #this used to be a tuple i think when moved the parameter up to function defintion\n",
    "                      return_skeleton=True,\n",
    "                        soma_border_vertices = None,\n",
    "                        skeleton_mesh=curr_limb_mesh,\n",
    "                        endpoints_must_keep = None,\n",
    "                      print_flag=False)\n",
    "    new_cleaned_skeleton = sk.clean_skeleton_with_decompose(distance_cleaned_skeleton)\n",
    "    \n",
    "    \n",
    "    curr_limb_branches_sk_uneven = sk.decompose_skeleton_to_branches(new_cleaned_skeleton)\n",
    "    sub_limb_mesh_idx = []\n",
    "    sub_limb_width = []\n",
    "    sub_limb_mesh = []\n",
    "    for j,curr_branch_sk in tqdm(enumerate(curr_limb_branches_sk_uneven)):\n",
    "        returned_data = cu.mesh_correspondence_adaptive_distance(curr_branch_sk,\n",
    "                                                          curr_limb_mesh,\n",
    "                                                         skeleton_segment_width = 1000,\n",
    "                                                         distance_by_mesh_center=distance_by_mesh_center)\n",
    "        curr_branch_face_correspondence, width_from_skeleton = returned_data\n",
    "        sub_limb_corr.append(curr_branch_face_correspondence)\n",
    "        sub_limb_width.append(width_from_skeleton)\n",
    "        \n",
    "        if len(curr_branch_face_correspondence) > 0:\n",
    "            sub_limb_mesh.append(curr_limb_mesh.submesh([list(curr_branch_face_correspondence)],append=True,repair=False))\n",
    "        else:\n",
    "            sub_limb_mesh.append(trimesh.Trimesh(vertices=np.array([]),faces=np.array([])))\n",
    "    \n",
    "    branch_meshes_MAP.append(sub_limb_mesh)\n",
    "    branch_skeletons_MAP.append(curr_limb_branches_sk_uneven)\n",
    "    branch_meshes_idx_MAP.append(sub_limb_corr)\n",
    "    branch_widths_MAP.append(sub_limb_width)\n",
    "        \n",
    "print(f\"Total time for mesh correspondence = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b_mesh,b_skel in zip(branch_meshes_MAP,branch_skeletons_MAP):\n",
    "    nviz.plot_objects(meshes=b_mesh,\n",
    "                      meshes_colors=\"random\",\n",
    "                     skeletons=b_skel,\n",
    "                     skeletons_colors=\"random\",\n",
    "                     scatters=[curr_soma_to_piece_touching_vertices[0]],\n",
    "                    scatter_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
