{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRun the preprocessing pipeline with \\nthe fusion decomposition that will \\nbe packaged up into preprocessing version 2\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the preprocessing pipeline with \n",
    "the fusion decomposition that will \n",
    "be packaged up into preprocessing version 2\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu = reload(tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skeleton_utils as sk\n",
    "import soma_extraction_utils as sm\n",
    "import trimesh_utils as tu\n",
    "import trimesh\n",
    "import numpy_utils as nu\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import networkx as nx\n",
    "import time\n",
    "import compartment_utils as cu\n",
    "import networkx_utils as xu\n",
    "import matplotlib_utils as mu\n",
    "\n",
    "#importing at the bottom so don't get any conflicts\n",
    "import itertools\n",
    "from tqdm_utils import tqdm\n",
    "\n",
    "#for meshparty preprocessing\n",
    "import meshparty_skeletonize as m_sk\n",
    "import general_utils as gu\n",
    "import compartment_utils as cu\n",
    "from meshparty import trimesh_io\n",
    "from copy import deepcopy\n",
    "\n",
    "from neuron_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh_utils as tu\n",
    "curent_neuron = tu.load_mesh_no_processing(\"/notebooks/test_neurons/Segmentation_2/meshparty/864691135548568516_single_soma_inhib_axon_cloud.off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_id = 864691135548568516\n",
    "description = \"single_soma_inhib_axon_cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predefined arguments for the Neuron constructor\n",
    "\n",
    "decomposition_type=\"meshafterparty\"\n",
    "mesh_correspondence=\"meshparty\" #meshafterparty_adaptive\n",
    "distance_by_mesh_center=True #how the distance is calculated for mesh correspondence\n",
    "meshparty_segment_size = 100\n",
    "meshparty_n_surface_downsampling = 2\n",
    "meshparty_adaptive_correspondence_after_creation=False\n",
    "suppress_preprocessing_print=True\n",
    "computed_attribute_dict=None\n",
    "somas = None\n",
    "branch_skeleton_data=None\n",
    "combine_close_skeleton_nodes = True\n",
    "combine_close_skeleton_nodes_threshold=700\n",
    "ignore_warnings=True\n",
    "suppress_output=False\n",
    "calculate_spines=True\n",
    "widths_to_calculate=[\"no_spine_median_mesh_center\",\n",
    "                \"no_spine_mean_mesh_center\"]\n",
    "fill_hole_size=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arguments for the preprocess neuron\n",
    "mesh = curent_neuron\n",
    "segment_id=segment_id\n",
    "description=description\n",
    "\n",
    "sig_th_initial_split=15 #for significant splitting meshes in the intial mesh split\n",
    "limb_threshold = 2000 #the mesh faces threshold for a mesh to be qualified as a limb (otherwise too small)\n",
    "filter_end_node_length=4001 #used in cleaning the skeleton during skeletonizations\n",
    "return_no_somas = False\n",
    "\n",
    "decomposition_type=decomposition_type\n",
    "mesh_correspondence=mesh_correspondence\n",
    "distance_by_mesh_center=distance_by_mesh_center\n",
    "meshparty_segment_size =meshparty_segment_size\n",
    "meshparty_n_surface_downsampling = meshparty_n_surface_downsampling\n",
    "somas=somas\n",
    "branch_skeleton_data=branch_skeleton_data\n",
    "combine_close_skeleton_nodes = combine_close_skeleton_nodes\n",
    "combine_close_skeleton_nodes_threshold=combine_close_skeleton_nodes_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_processing_tiempo = time.time()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Purpose: To process the mesh into a format that can be loaded into the neuron class\n",
    "and used for higher order processing (how to visualize is included)\n",
    "\n",
    "\"\"\"\n",
    "if description is None:\n",
    "    description = \"no_description\"\n",
    "if segment_id is None:\n",
    "    #pick a random segment id\n",
    "    segment_id = np.random.randint(100000000)\n",
    "    print(f\"picking a random 7 digit segment id: {segment_id}\")\n",
    "    description += \"_random_id\"\n",
    "\n",
    "\n",
    "if mesh is None:\n",
    "    if current_mesh_file is None:\n",
    "        raise Exception(\"No mesh or mesh_file file were given\")\n",
    "    else:\n",
    "        current_neuron = trimesh.load_mesh(current_mesh_file)\n",
    "else:\n",
    "    current_neuron = mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************************ Phase A ********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 15000.0 \n",
      "large_mesh_threshold_inner = 10000.0 \n",
      "soma_size_threshold = 937.5 \n",
      "soma_size_threshold_max = 12000.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "xvfb-run -n 7377 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/864691135548568516/neuron_864691135548568516.off -o /notebooks/Platinum_Decomp_Fusion/864691135548568516/neuron_864691135548568516_decimated.off -s /notebooks/Platinum_Decomp_Fusion/864691135548568516/decimation_meshlab_25862031.mls\n",
      "Total found significant pieces before Poisson = [<trimesh.Trimesh(vertices.shape=(191269, 3), faces.shape=(379018, 3))>]\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(191269, 3), faces.shape=(379018, 3))>\n",
      "pre_largest_mesh_path = /notebooks/Platinum_Decomp_Fusion/864691135548568516/neuron_864691135548568516_decimated_largest_piece.off\n",
      "xvfb-run -n 765 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/864691135548568516/neuron_864691135548568516_decimated_largest_piece.off -o /notebooks/Platinum_Decomp_Fusion/864691135548568516/neuron_864691135548568516_decimated_largest_piece_poisson.off -s /notebooks/Platinum_Decomp_Fusion/864691135548568516/poisson_769206.mls\n",
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(48099, 3), faces.shape=(96194, 3))>]\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(48099, 3), faces.shape=(96194, 3))>\n",
      "xvfb-run -n 7879 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/864691135548568516/neuron_864691135548568516_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Platinum_Decomp_Fusion/864691135548568516/neuron_864691135548568516_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Platinum_Decomp_Fusion/864691135548568516/decimation_meshlab_25334424.mls\n",
      "done exporting decimated mesh: neuron_864691135548568516_decimated_largest_piece_poisson_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00027251243591308594\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Decomp_Fusion/temp/86469113554856851600_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 2.3162331581115723\n",
      "2) Finished: Generating CGAL segmentation for neuron: 2.6601152420043945\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 1\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.007954835891723633\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 4.744529724121094e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.02099752426147461\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.11497712135314941\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.848731\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 1,  5,  8,  3,  2,  9,  7,  6,  0, 10,  4]), array([0.848731  , 0.299497  , 0.178388  , 0.06392905, 0.05540815,\n",
      "       0.0527049 , 0.0524823 , 0.0519053 , 0.05173975, 0.049012  ,\n",
      "       0.0467932 ]))\n",
      "Sizes = [4344, 331, 132, 3990, 3288, 2013, 1240, 3793, 2528, 1456, 929]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [1]\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 439 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_178241.off -o /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_178241_poisson.off -s /notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_408863.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_178241.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_178241_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_408863.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 2.6141178927767426\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 119.24351119995117\n",
      "Before Filtering the number of somas found = 1\n",
      "Performing Soma Mesh Backtracking to original mesh\n",
      "# total split meshes = 12\n",
      "viable_meshes = [0]\n",
      "There were 11 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0]}\n",
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "current_soma_mesh_list = [<trimesh.Trimesh(vertices.shape=(2209, 3), faces.shape=(4344, 3))>]\n",
      "current_mesh = <trimesh.Trimesh(vertices.shape=(191269, 3), faces.shape=(378525, 3))>\n",
      "\n",
      "inside Soma subtraction\n",
      "mesh pieces in subtact soma BEFORE the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(122248, 3), faces.shape=(241890, 3))>, <trimesh.Trimesh(vertices.shape=(23355, 3), faces.shape=(46192, 3))>, <trimesh.Trimesh(vertices.shape=(16468, 3), faces.shape=(32480, 3))>, <trimesh.Trimesh(vertices.shape=(12096, 3), faces.shape=(23917, 3))>, <trimesh.Trimesh(vertices.shape=(9190, 3), faces.shape=(18171, 3))>, <trimesh.Trimesh(vertices.shape=(3788, 3), faces.shape=(7526, 3))>]\n",
      "There were 6 pieces found after size threshold\n",
      "mesh pieces in subtact soma AFTER the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(122248, 3), faces.shape=(241890, 3))>, <trimesh.Trimesh(vertices.shape=(23355, 3), faces.shape=(46192, 3))>, <trimesh.Trimesh(vertices.shape=(16468, 3), faces.shape=(32480, 3))>, <trimesh.Trimesh(vertices.shape=(12096, 3), faces.shape=(23917, 3))>, <trimesh.Trimesh(vertices.shape=(9190, 3), faces.shape=(18171, 3))>, <trimesh.Trimesh(vertices.shape=(3788, 3), faces.shape=(7526, 3))>]\n",
      "Total Time for soma mesh cancellation = 0.713\n",
      "mesh_pieces_without_soma = [<trimesh.Trimesh(vertices.shape=(122248, 3), faces.shape=(241890, 3))>, <trimesh.Trimesh(vertices.shape=(23355, 3), faces.shape=(46192, 3))>, <trimesh.Trimesh(vertices.shape=(16468, 3), faces.shape=(32480, 3))>, <trimesh.Trimesh(vertices.shape=(12096, 3), faces.shape=(23917, 3))>, <trimesh.Trimesh(vertices.shape=(9190, 3), faces.shape=(18171, 3))>, <trimesh.Trimesh(vertices.shape=(3788, 3), faces.shape=(7526, 3))>]\n",
      "Total time for Subtract Soam = 0.7141759395599365\n",
      "mesh_pieces_without_soma_stacked = <trimesh.Trimesh(vertices.shape=(187145, 3), faces.shape=(370176, 3))>\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 0.2328336238861084\n",
      "soma_faces = [109701 109702 109703 ... 183817 183818 183819]\n",
      "soma_meshes = <trimesh.Trimesh(vertices.shape=(4251, 3), faces.shape=(8349, 3))>\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 4130 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_613688.off -o /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_613688_poisson.off -s /notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_881738.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_613688.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_613688_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_881738.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 2.898320107139027\n",
      "Soma List = [<trimesh.Trimesh(vertices.shape=(4251, 3), faces.shape=(8349, 3))>]\n",
      "soma_mesh_list_centers = [array([577248.53133381, 778008.04476594, 890132.63702658])]\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Doing the soma detection\n",
    "if somas is None:\n",
    "    soma_mesh_list,run_time,total_soma_list_sdf = sm.extract_soma_center(segment_id,\n",
    "                                             current_neuron.vertices,\n",
    "                                             current_neuron.faces)\n",
    "else:\n",
    "    soma_mesh_list,run_time,total_soma_list_sdf = somas\n",
    "\n",
    "# geting the soma centers\n",
    "if len(soma_mesh_list) <= 0:\n",
    "    print(f\"**** No Somas Found for Mesh {segment_id} so just one mesh\")\n",
    "    soma_mesh_list_centers = []\n",
    "    if return_no_somas:\n",
    "        return_value= soma_mesh_list_centers\n",
    "    raise Exception(\"Processing of No Somas is not yet implemented yet\")\n",
    "else:\n",
    "    #compute the soma centers\n",
    "    print(f\"Soma List = {soma_mesh_list}\")\n",
    "\n",
    "    soma_mesh_list_centers = sm.find_soma_centroids(soma_mesh_list)\n",
    "    print(f\"soma_mesh_list_centers = {soma_mesh_list_centers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total split meshes = 35\n",
      "There were 34 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0]}\n"
     ]
    }
   ],
   "source": [
    "#--- 2) getting the soma submeshes that are connected to each soma and identifiying those that aren't (and eliminating any mesh pieces inside the soma)\n",
    "\n",
    "main_mesh_total = current_neuron\n",
    "\n",
    "\n",
    "#finding the mesh pieces that contain the soma\n",
    "#splitting the current neuron into distinct pieces\n",
    "split_meshes = tu.split_significant_pieces(\n",
    "                            main_mesh_total,\n",
    "                            significance_threshold=sig_th_initial_split,\n",
    "                            print_flag=False)\n",
    "\n",
    "print(f\"# total split meshes = {len(split_meshes)}\")\n",
    "\n",
    "\n",
    "#returns the index of the split_meshes index that contains each soma    \n",
    "containing_mesh_indices = sm.find_soma_centroid_containing_meshes(soma_mesh_list,\n",
    "                                        split_meshes)\n",
    "\n",
    "# filtering away any of the inside floating pieces: \n",
    "non_soma_touching_meshes = [m for i,m in enumerate(split_meshes)\n",
    "                 if i not in list(containing_mesh_indices.values())]\n",
    "\n",
    "\n",
    "#Adding the step that will filter away any pieces that are inside the soma\n",
    "if len(non_soma_touching_meshes) > 0 and len(soma_mesh_list) > 0:\n",
    "    \"\"\"\n",
    "    *** want to save these pieces that are inside of the soma***\n",
    "    \"\"\"\n",
    "\n",
    "    non_soma_touching_meshes,inside_pieces = sm.filter_away_inside_soma_pieces(soma_mesh_list,non_soma_touching_meshes,\n",
    "                                    significance_threshold=sig_th_initial_split,\n",
    "                                    return_inside_pieces = True)                                                      \n",
    "\n",
    "\n",
    "split_meshes # the meshes of the original mesh\n",
    "containing_mesh_indices #the mapping of each soma centroid to the correct split mesh\n",
    "soma_containing_meshes = sm.grouping_containing_mesh_indices(containing_mesh_indices)\n",
    "\n",
    "soma_touching_meshes = [split_meshes[k] for k in soma_containing_meshes.keys()]\n",
    "\n",
    "\n",
    "#     print(f\"# of non soma touching seperate meshes = {len(non_soma_touching_meshes)}\")\n",
    "#     print(f\"# of inside pieces = {len(inside_pieces)}\")\n",
    "print(f\"# of soma containing seperate meshes = {len(soma_touching_meshes)}\")\n",
    "print(f\"meshes with somas = {soma_containing_meshes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "\n",
      "inside Soma subtraction\n",
      "mesh pieces in subtact soma BEFORE the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(442924, 3), faces.shape=(883241, 3))>, <trimesh.Trimesh(vertices.shape=(108008, 3), faces.shape=(215445, 3))>, <trimesh.Trimesh(vertices.shape=(75957, 3), faces.shape=(151431, 3))>, <trimesh.Trimesh(vertices.shape=(54061, 3), faces.shape=(107833, 3))>, <trimesh.Trimesh(vertices.shape=(42023, 3), faces.shape=(83820, 3))>, <trimesh.Trimesh(vertices.shape=(17933, 3), faces.shape=(35808, 3))>]\n",
      "There were 6 pieces found after size threshold\n",
      "mesh pieces in subtact soma AFTER the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(442924, 3), faces.shape=(883241, 3))>, <trimesh.Trimesh(vertices.shape=(108008, 3), faces.shape=(215445, 3))>, <trimesh.Trimesh(vertices.shape=(75957, 3), faces.shape=(151431, 3))>, <trimesh.Trimesh(vertices.shape=(54061, 3), faces.shape=(107833, 3))>, <trimesh.Trimesh(vertices.shape=(42023, 3), faces.shape=(83820, 3))>, <trimesh.Trimesh(vertices.shape=(17933, 3), faces.shape=(35808, 3))>]\n",
      "Total Time for soma mesh cancellation = 2.254\n",
      "Total time for Subtract Soam = 2.2543272972106934\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 2.479623556137085\n",
      "Total time for Original_mesh_faces_map for somas= 1.412766456604004\n",
      "Total time for sig_non_soma_pieces= 2.5903067588806152\n",
      "Total time for split= 0.028629064559936523\n",
      "Total time for mesh_pieces_connectivity= 15.238999366760254\n",
      "# of insignificant_limbs = 0 with trimesh : []\n"
     ]
    }
   ],
   "source": [
    "#--- 3)  Soma Extraction was great (but it wasn't the original soma faces), so now need to get the original soma faces and the original non-soma faces of original pieces\n",
    "\n",
    "#     sk.graph_skeleton_and_mesh(other_meshes=[soma_meshes])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for each soma touching mesh get the following:\n",
    "1) original soma meshes\n",
    "2) significant mesh pieces touching these somas\n",
    "3) The soma connectivity to each of the significant mesh pieces\n",
    "-- later will just translate the \n",
    "\n",
    "\n",
    "Process: \n",
    "\n",
    "1) Final all soma faces (through soma extraction and then soma original faces function)\n",
    "2) Subtact all soma faces from original mesh\n",
    "3) Find all significant mesh pieces\n",
    "4) Backtrack significant mesh pieces to orignal mesh and find connectivity of each to all\n",
    "   the available somas\n",
    "Conclusion: Will have connectivity map\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "soma_touching_mesh_data = dict()\n",
    "\n",
    "for z,(mesh_idx, soma_idxes) in enumerate(soma_containing_meshes.items()):\n",
    "    soma_touching_mesh_data[z] = dict()\n",
    "    print(f\"\\n\\n----Working on soma-containing mesh piece {z}----\")\n",
    "\n",
    "    #1) Final all soma faces (through soma extraction and then soma original faces function)\n",
    "    current_mesh = split_meshes[mesh_idx]\n",
    "\n",
    "    current_soma_mesh_list = [soma_mesh_list[k] for k in soma_idxes]\n",
    "\n",
    "    current_time = time.time()\n",
    "    mesh_pieces_without_soma = sm.subtract_soma(current_soma_mesh_list,current_mesh,\n",
    "                                                significance_threshold=250)\n",
    "    print(f\"Total time for Subtract Soam = {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "\n",
    "    mesh_pieces_without_soma_stacked = tu.combine_meshes(mesh_pieces_without_soma)\n",
    "\n",
    "    # find the original soma faces of mesh\n",
    "    soma_faces = tu.original_mesh_faces_map(current_mesh,mesh_pieces_without_soma_stacked,matching=False)\n",
    "    print(f\"Total time for Original_mesh_faces_map for mesh_pieces without soma= {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "    soma_meshes = current_mesh.submesh([soma_faces],append=True,repair=False)\n",
    "\n",
    "    # finding the non-soma original faces\n",
    "    non_soma_faces = tu.original_mesh_faces_map(current_mesh,soma_meshes,matching=False)\n",
    "    non_soma_stacked_mesh = current_mesh.submesh([non_soma_faces],append=True,repair=False)\n",
    "\n",
    "    print(f\"Total time for Original_mesh_faces_map for somas= {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "\n",
    "    # 3) Find all significant mesh pieces\n",
    "    sig_non_soma_pieces,insignificant_limbs = tu.split_significant_pieces(non_soma_stacked_mesh,significance_threshold=limb_threshold,\n",
    "                                                     return_insignificant_pieces=True)\n",
    "\n",
    "    print(f\"Total time for sig_non_soma_pieces= {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "\n",
    "    soma_touching_mesh_data[z][\"branch_meshes\"] = sig_non_soma_pieces\n",
    "\n",
    "    #4) Backtrack significant mesh pieces to orignal mesh and find connectivity of each to all the available somas\n",
    "    # get all the seperate mesh faces\n",
    "\n",
    "    #How to seperate the mesh faces\n",
    "    seperate_soma_meshes,soma_face_components = tu.split(soma_meshes,only_watertight=False)\n",
    "    #take the top largest ones depending how many were originally in the soma list\n",
    "    seperate_soma_meshes = seperate_soma_meshes[:len(soma_mesh_list)]\n",
    "    soma_face_components = soma_face_components[:len(soma_mesh_list)]\n",
    "\n",
    "    soma_touching_mesh_data[z][\"soma_meshes\"] = seperate_soma_meshes\n",
    "\n",
    "    print(f\"Total time for split= {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    soma_to_piece_connectivity = dict()\n",
    "    soma_to_piece_touching_vertices = dict()\n",
    "    soma_to_piece_touching_vertices_idx = dict()\n",
    "    limb_root_nodes = dict()\n",
    "    \n",
    "    m_vert_graph = tu.mesh_vertex_graph(current_mesh)\n",
    "    \n",
    "    for i,curr_soma in enumerate(seperate_soma_meshes):\n",
    "        (connected_mesh_pieces,\n",
    "         connected_mesh_pieces_vertices,\n",
    "         connected_mesh_pieces_vertices_idx) = tu.mesh_pieces_connectivity(\n",
    "                        main_mesh=current_mesh,\n",
    "                        central_piece=curr_soma,\n",
    "                        periphery_pieces = sig_non_soma_pieces,\n",
    "                        return_vertices = True,\n",
    "                        return_vertices_idx=True)\n",
    "        #print(f\"soma {i}: connected_mesh_pieces = {connected_mesh_pieces}\")\n",
    "        soma_to_piece_connectivity[i] = connected_mesh_pieces\n",
    "\n",
    "        soma_to_piece_touching_vertices[i] = dict()\n",
    "        for piece_index,piece_idx in enumerate(connected_mesh_pieces):\n",
    "            limb_root_nodes[piece_idx] = connected_mesh_pieces_vertices[piece_index][0]\n",
    "            \n",
    "            #find the number of touching groups and save those \n",
    "            soma_touching_graph = m_vert_graph.subgraph(connected_mesh_pieces_vertices_idx[piece_index])\n",
    "            soma_con_comp = [current_mesh.vertices[np.array(list(k)).astype(\"int\")] for k in list(nx.connected_components(soma_touching_graph))]\n",
    "            soma_to_piece_touching_vertices[i][piece_idx] = soma_con_comp\n",
    "\n",
    "#         border_debug = False\n",
    "#         if border_debug:\n",
    "#             print(f\"soma_to_piece_connectivity = {soma_to_piece_connectivity}\")\n",
    "#             print(f\"soma_to_piece_touching_vertices = {soma_to_piece_touching_vertices}\")\n",
    "\n",
    "\n",
    "    print(f\"Total time for mesh_pieces_connectivity= {time.time() - current_time}\")\n",
    "\n",
    "    soma_touching_mesh_data[z][\"soma_to_piece_connectivity\"] = soma_to_piece_connectivity\n",
    "\n",
    "print(f\"# of insignificant_limbs = {len(insignificant_limbs)} with trimesh : {insignificant_limbs}\")\n",
    "\n",
    "\n",
    "\n",
    "# Lets have an alert if there was more than one soma disconnected meshes\n",
    "if len(soma_touching_mesh_data.keys()) > 1:\n",
    "    raise Exception(\"More than 1 disconnected meshes that contain somas\")\n",
    "\n",
    "current_mesh_data = soma_touching_mesh_data\n",
    "soma_containing_idx = 0\n",
    "\n",
    "#doing inversion of the connectivity and touching vertices\n",
    "piece_to_soma_touching_vertices = gu.flip_key_orders_for_dict(soma_to_piece_touching_vertices)\n",
    "\n",
    "\n",
    "# ****Soma Touching mesh Data has the branches and the connectivity (So this is where you end up skipping if you don't have somas)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pause: Investigating the Soma Touching Vertices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_visualizations as nviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7ab4f7e78445d9becacb27ced9ed97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nviz.plot_objects(meshes=current_mesh,\n",
    "scatters=[soma_to_piece_touching_vertices[0][1][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** Phase B ***** Skeletonization with MP/MAP Decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing_vp2 as pre\n",
    "pre = reload(pre)\n",
    "sk = reload(sk)\n",
    "tu = reload(tu)\n",
    "m_sk = reload(m_sk)\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the first pass of surface skeletonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_mesh = mesh_data[\"branch_meshes\"][1]\n",
    "\n",
    "total_border_vertices = dict()\n",
    "for l_idx,s_dict in piece_to_soma_touching_vertices.items():\n",
    "    local_b_verts = []\n",
    "    for sm_idx,sm_bord_verts in s_dict.items():\n",
    "        local_b_verts.append(np.vstack(sm_bord_verts))\n",
    "    total_border_vertices[l_idx] = np.vstack(local_b_verts)\n",
    "    \n",
    "\n",
    "# for k in soma_to_piece_touching_vertices.keys():\n",
    "#     if z in soma_to_piece_touching_vertices[k].keys():\n",
    "#         total_border_vertices[k] = np.vstack(soma_to_piece_touching_vertices[k][z])\n",
    "        \n",
    "# possible_soma_border = np.vstack(list(total_border_vertices.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6adfd8798f4836a7d4dc508da13de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nviz.plot_objects(skeletons=[sk_meshparty_obj.vertices[sk_meshparty_obj.edges]],\n",
    "                 scatters=[sk_meshparty_obj.vertices[sk_meshparty_obj.root]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smooth_neighborhood = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5965fea78c5f418399982b6390fe774f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=108007.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/meshAfterParty/meshparty_skeletonize.py:862: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  segment_branches = np.array([sk_meshparty_obj.vertices[np.vstack([k[:-1],k[1:]]).T] for k in segments])\n",
      "/meshAfterParty/meshparty_skeletonize.py:895: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  segment_branches = new_segment_branches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branches_touching_root = [27]\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.004901482977093922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929487438b7d45d89dd470278f08a51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad826fd02660405d8fd6a7845acbcd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:735: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  total_submeshes_idx =np.array(list(total_submeshes_idx.values()))\n"
     ]
    }
   ],
   "source": [
    "#will eventually get the current root from soma_to_piece_touching_vertices[i]\n",
    "root_curr = total_border_vertices[1][0]\n",
    "\n",
    "m_sk = reload(m_sk)\n",
    "sk_meshparty_obj,limb_mesh_mparty = m_sk.skeletonize_mesh_largest_component(curr_mesh,\n",
    "                                                        root=root_curr)\n",
    "m_sk = reload(m_sk)\n",
    "\n",
    "(segment_branches, #skeleton branches\n",
    "divided_submeshes, divided_submeshes_idx, #mesh correspondence (mesh and indices)\n",
    "segment_widths_median) = m_sk.skeleton_obj_to_branches(sk_meshparty_obj,\n",
    "                                                      mesh = limb_mesh_mparty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the width threshold\n",
    "width_threshold = 450\n",
    "size_threshold = 1000\n",
    "\n",
    "#gettin the branches that should be passed through MAP skeletonization\n",
    "pieces_above_threshold = np.where(segment_widths_median>width_threshold)[0]\n",
    "\n",
    "#getting the correspondnece info for those MAP qualifying\n",
    "width_large = segment_widths_median[pieces_above_threshold]\n",
    "sk_large = [segment_branches[k] for k in pieces_above_threshold]\n",
    "mesh_large_idx = [divided_submeshes_idx[k] for k in pieces_above_threshold]\n",
    "\n",
    "if len(mesh_large_idx) > 0: #will only continue processing if found MAP candidates\n",
    "    #finds the connectivity edges of all the MAP candidates\n",
    "    mesh_large_connectivity = tu.mesh_list_connectivity(meshes = mesh_large_idx,\n",
    "                            main_mesh = limb_mesh_mparty,\n",
    "                            print_flag = False)\n",
    "    \"\"\"\n",
    "    --------------- Grouping MAP candidates ----------------\n",
    "    Purpose: Will see what mesh pieces should be grouped together\n",
    "    to pass through CGAL skeletonization\n",
    "    \n",
    "    \n",
    "    Pseudocode: \n",
    "    1) build a networkx graph with all nodes for mesh_large_idx indexes\n",
    "    2) Add the edges\n",
    "    3) Find the connected components\n",
    "    4) Find sizes of connected components\n",
    "    5) For all those connected components that are of a large enough size, \n",
    "    add the mesh branches and skeletons to the final list\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(np.arange(len(mesh_large_idx)))\n",
    "    G.add_edges_from(mesh_large_connectivity)\n",
    "    conn_comp = list(nx.connected_components(G))\n",
    "\n",
    "    filtered_pieces = []\n",
    "\n",
    "    sk_large_size_filt = []\n",
    "    mesh_large_idx_size_filt = []\n",
    "    width_large_size_filt = []\n",
    "\n",
    "    for cc in conn_comp:\n",
    "        total_cc_size = np.sum([len(mesh_large_idx[k]) for k in cc])\n",
    "        if total_cc_size>size_threshold:\n",
    "            #print(f\"cc ({cc}) passed the size threshold because size was {total_cc_size}\")\n",
    "            filtered_pieces.append(pieces_above_threshold[list(cc)])\n",
    "            \n",
    "    #filtered_pieces: will have the indexes of all the branch candidates that should  be \n",
    "    #grouped together and passed through MAP skeletonization\n",
    "\n",
    "    if len(filtered_pieces) > 0:\n",
    "        #all the pieces that will require MAP mesh correspondence and skeletonization\n",
    "        #(already organized into their components)\n",
    "        mesh_pieces_for_MAP = [limb_mesh_mparty.submesh([np.concatenate(divided_submeshes_idx[k])],append=True,repair=False) for k in filtered_pieces]\n",
    "\n",
    "        pieces_idx_MP = np.setdiff1d(np.arange(len(divided_submeshes_idx)),np.concatenate(filtered_pieces))\n",
    "        \n",
    "        \"\"\"\n",
    "        Old Way: Finding connectivity of pieces through\n",
    "        mesh_idx_MP = [divided_submeshes_idx[k] for k in pieces_idx_MP]\n",
    "\n",
    "        mesh_large_connectivity_MP = tu.mesh_list_connectivity(meshes = mesh_idx_MP,\n",
    "                                main_mesh = limb_mesh_mparty,\n",
    "                                print_flag = False)\n",
    "                                \n",
    "        New Way: going to use skeleton connectivity to determine\n",
    "        connectivity of pieces\n",
    "        \n",
    "        Pseudocode: \n",
    "        1)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        skeleton_MP = [segment_branches[k] for k in pieces_idx_MP]\n",
    "        skeleton_connectivity_MP = sk.skeleton_list_connectivity(\n",
    "                                        skeletons=skeleton_MP\n",
    "                                        )\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(np.arange(len(mesh_idx_MP)))\n",
    "        G.add_edges_from(skeleton_connectivity_MP)\n",
    "        sublimbs_MP = list(nx.connected_components(G))\n",
    "        sublimbs_MP_orig_idx = [pieces_idx_MP[list(k)] for k in sublimbs_MP]\n",
    "\n",
    "\n",
    "        #concatenate into sublimbs the skeletons and meshes\n",
    "        sublimb_mesh_idx_branches_MP = [divided_submeshes_idx[k] for k in sublimbs_MP_orig_idx]\n",
    "        sublimb_mesh_branches_MP = [[limb_mesh_mparty.submesh([ki],append=True,repair=False)\n",
    "                                    for ki in k] for k in sublimb_mesh_idx_branches_MP]\n",
    "        sublimb_meshes_MP = [limb_mesh_mparty.submesh([np.concatenate(k)],append=True,repair=False)\n",
    "                                                     for k in sublimb_mesh_idx_branches_MP]\n",
    "        sublimb_skeleton_branches = [segment_branches[k] for k in sublimbs_MP_orig_idx]\n",
    "        widths_MP = [segment_widths_median[k] for k in sublimbs_MP_orig_idx]\n",
    "\n",
    "else: #if no pieces were determine to need MAP processing\n",
    "    print(\"No MAP processing needed: just returning the Meshparty skeletonization and mesh correspondence\")\n",
    "    raise Exception(\"Returning MP correspondence\")\n",
    "\n",
    "\n",
    "#         for indiv_cc in cc:\n",
    "#             sk_large_size_filt.append(sk_large[indiv_cc])\n",
    "#             mesh_large_idx_size_filt.append(mesh_large_idx[indiv_cc])\n",
    "#             width_large_size_filt.append(width_large[indiv_cc])\n",
    "\n",
    "# nviz.plot_objects(main_mesh=tu.combine_meshes([limb_mesh_mparty,current_neuron[\"S0\"].mesh]),\n",
    "#                   main_mesh_color=\"green\",\n",
    "#     skeletons=sk_large_size_filt,\n",
    "#      meshes=[limb_mesh_mparty.submesh([k],append=True) for k in mesh_large_idx_size_filt],\n",
    "#       meshes_colors=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([149.8654772 , 418.55880666, 311.43380537]),\n",
       " array([405.8431516]),\n",
       " array([414.99335148]),\n",
       " array([400.39759195]),\n",
       " array([402.46626928, 186.91510311]),\n",
       " array([421.16696781, 127.10122558, 328.15344523])]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh_pieces_for_MAP #trimesh pieces that should go through CGAL skeletonization\n",
    "sublimb_meshes_MP #trimesh pieces that have already been passed through MP skeletonization (may not need)\n",
    "\n",
    "# -- the decomposition information ---\n",
    "sublimb_mesh_branches_MP #the mesh branches for all the disconnected sublimbs\n",
    "sublimb_mesh_idx_branches_MP #The mesh branches idx that have already passed through MP skeletonization\n",
    "sublimb_skeleton_branches #the skeleton bnraches for all the sublimbs\n",
    "widths_MP #the mesh branches widths for all the disconnected groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding which of the sublimbs the soma touching vertices belogs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [TrackedArray([[580478.6, 771669. , 885399.6],\n",
       "                [579293.8, 772063.6, 885518. ],\n",
       "                [580218.1, 771431.4, 885658.1],\n",
       "                [579967.5, 772038.8, 885108. ],\n",
       "                [579810. , 771918. , 885108. ],\n",
       "                [579749.8, 771992.9, 885150.2],\n",
       "                [580156.6, 771634.1, 885833.4],\n",
       "                [579800.9, 771569.4, 885956.9],\n",
       "                [579190.2, 771672.2, 885807.2],\n",
       "                [580298.4, 771508.4, 885558.6],\n",
       "                [579088.1, 771776.2, 885706. ],\n",
       "                [580237.9, 771731.7, 885095.4],\n",
       "                [580083. , 771839.2, 885097.5],\n",
       "                [579342.4, 771610.4, 885914.6],\n",
       "                [580324.5, 771687. , 885606.8],\n",
       "                [579682.6, 772060.9, 885191. ],\n",
       "                [579552.9, 771584.2, 885938.3],\n",
       "                [579148.5, 771750. , 885801. ],\n",
       "                [580399.5, 771822.4, 885275.6],\n",
       "                [580209.7, 771929.2, 885127.6],\n",
       "                [579396.1, 771929. , 885393.1],\n",
       "                [579529.8, 771752.9, 886008.8],\n",
       "                [579686.8, 771633.1, 885954.6],\n",
       "                [579159.5, 771898.9, 885562.4],\n",
       "                [579519.4, 772073. , 885284.4],\n",
       "                [580234.8, 771703.7, 885771.6],\n",
       "                [579957. , 771553.1, 885885. ]]),\n",
       "  TrackedArray([[572892.4, 784112.7, 890130.9],\n",
       "                [574874.6, 784909.7, 892020. ],\n",
       "                [573535. , 784217.2, 889387.8],\n",
       "                [573227.4, 784303.4, 889605. ],\n",
       "                [573390.1, 784125.6, 889503.9],\n",
       "                [572945. , 783962.2, 890017.7],\n",
       "                [575680.2, 785088.4, 890166.6],\n",
       "                [575244.9, 785021.9, 891612.5],\n",
       "                [574781.2, 785014.6, 888710.4],\n",
       "                [574374.5, 784522.3, 888754.2],\n",
       "                [572451.5, 784365.9, 891610.9],\n",
       "                [575645.8, 785055.8, 891261.9],\n",
       "                [573284.6, 784642.2, 892474.9],\n",
       "                [573875.2, 784410.9, 889063.9],\n",
       "                [575397.4, 784923.8, 891602.9],\n",
       "                [572451.1, 784301.3, 891217.1],\n",
       "                [575660.2, 785045. , 890456.9],\n",
       "                [575716.5, 785054.4, 889921.1],\n",
       "                [574571.1, 784848.5, 888726.6],\n",
       "                [575550.1, 785012.2, 891440.2],\n",
       "                [575446.9, 785097.5, 889604. ],\n",
       "                [575163.9, 784979.5, 891740.9],\n",
       "                [575772.1, 784957.9, 891203.4],\n",
       "                [574481.1, 784669.9, 888718.7],\n",
       "                [575778.2, 785050.3, 891067.9],\n",
       "                [573451.5, 784447.1, 892683.8],\n",
       "                [572657.1, 784240.8, 890560. ],\n",
       "                [572449.5, 784581. , 891891. ],\n",
       "                [575044.8, 785239.4, 888962.1],\n",
       "                [574150.2, 784656.4, 892523.2],\n",
       "                [574904.2, 784986.1, 888674.7],\n",
       "                [575730.8, 785021.4, 890313.5],\n",
       "                [572439. , 784528.5, 891796.5],\n",
       "                [574120.2, 784193.4, 888891.3],\n",
       "                [575780.2, 784941.8, 890518.7],\n",
       "                [575906.8, 784910. , 891063.8],\n",
       "                [572661. , 784409.8, 892315.4],\n",
       "                [573733.2, 784357.2, 889177.4],\n",
       "                [573951. , 784434. , 892741.5],\n",
       "                [574750.6, 784969.8, 891984.8],\n",
       "                [574051.3, 784446.8, 888936.8],\n",
       "                [573149.2, 784620.4, 892462.5],\n",
       "                [574959.9, 785028.8, 891839.9],\n",
       "                [573804. , 784329. , 892815. ],\n",
       "                [573987.8, 784218.8, 889003.5],\n",
       "                [573448.1, 784625.4, 892537.3],\n",
       "                [572449.8, 784457.8, 892073. ],\n",
       "                [572585.6, 784314.4, 890769.6],\n",
       "                [575158.9, 785280.8, 889281.2],\n",
       "                [575050.1, 785019.4, 888739.9],\n",
       "                [573330.7, 784528.3, 892585.9],\n",
       "                [573174. , 784213.5, 889717.5],\n",
       "                [573792.8, 784462.2, 892665.2],\n",
       "                [573026.2, 784141.7, 889863.1],\n",
       "                [574038.9, 784593.9, 892594.9],\n",
       "                [575259.2, 784887. , 891783.5],\n",
       "                [572946.3, 784218.8, 889985.4],\n",
       "                [572964. , 784518. , 892374. ],\n",
       "                [573616.5, 784216.3, 889309.5],\n",
       "                [575695. , 785054.2, 890671.8],\n",
       "                [574413.7, 784957. , 892146.2],\n",
       "                [575514.8, 785160.8, 889848.7],\n",
       "                [572829.1, 784141.8, 890264.2],\n",
       "                [574624.5, 784963.7, 888719.5],\n",
       "                [572730.3, 784174.5, 890428.4],\n",
       "                [575134.6, 785363.2, 889413.2],\n",
       "                [574170. , 784769.2, 892323.5],\n",
       "                [574324.9, 784765. , 892278.7],\n",
       "                [572870.8, 784425.9, 892437.6],\n",
       "                [575492.6, 784941.5, 891520.2],\n",
       "                [572565. , 784486.5, 892227. ],\n",
       "                [575305.5, 785232. , 889570.5],\n",
       "                [574955.1, 785224. , 888785.3],\n",
       "                [572544. , 784476. , 890909.2],\n",
       "                [573923.4, 784527.9, 892629. ],\n",
       "                [573150.6, 784446.4, 892538.2],\n",
       "                [573971.2, 784563.4, 889003.8],\n",
       "                [575842.1, 785020.5, 890883.1],\n",
       "                [572417.9, 784473.3, 891669.2],\n",
       "                [573972.2, 784679.5, 892465.9],\n",
       "                [572484.6, 784428.8, 891115.8],\n",
       "                [575047.4, 784896.4, 891946.6],\n",
       "                [572442.6, 784464.8, 891332.6],\n",
       "                [574286.4, 784457.6, 888801.9],\n",
       "                [574531.7, 784940.7, 892098.4],\n",
       "                [573689.1, 784479.2, 892636.6]])]}"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_value = pre.filter_soma_touching_vertices_dict_by_mesh(\n",
    "    mesh = mesh_pieces_for_MAP[0],\n",
    "    curr_piece_to_soma_touching_vertices = piece_to_soma_touching_vertices[1]\n",
    "    )\n",
    "return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out the Soma Extension and the Changing of Mesh Correspondence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_sk_MP = sk.stack_skeletons(segment_branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new branch to skeleton\n",
      "Adding new branch to skeleton\n"
     ]
    }
   ],
   "source": [
    "return_info = sk.create_soma_extending_branches(current_skeleton=whole_sk_MP,\n",
    "                              skeleton_mesh=branch,\n",
    "                              soma_to_piece_touching_vertices=piece_to_soma_touching_vertices[1],\n",
    "                              return_endpoints_must_keep=True,\n",
    "                                 return_created_branch_info=True)\n",
    "new_sk,endpts,new_branch_info = return_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning Branch = 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261439ba116641a6865eac49e5cb480d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--- Working on 1-to-1 correspondence-----\n",
      "max(original_labels),len(original_labels) = (2, 3)\n",
      "empty_indices % = 0.12254335260115606\n",
      " conflict_indices % = 0.0403339755940912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc414c557044861bc8e4d8bbd55ecb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n",
      "Took 0 iterations to expand the label back\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd18ef2f4db48e29ba2d4e84585d3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winning Branch = 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38708979229a48c78b440eacf89e960b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trimesh:face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--- Working on 1-to-1 correspondence-----\n",
      "max(original_labels),len(original_labels) = (2, 3)\n",
      "empty_indices % = 0.02508774210321071\n",
      " conflict_indices % = 0.02335456475583864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a4c14025894454952c1e3c0e0b19ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n",
      "Took 0 iterations to expand the label back\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb8a41d03a1467489fe857e8fb697e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#finding the branches where the new segment was added\n",
    "\n",
    "\"\"\"\n",
    "Pseudocode:\n",
    "1) Get the newly added branch (and the original vertex which is the first row)\n",
    "2) Find the branches that have that coordinate (could be multiple)\n",
    "2b) If multiple branches: Pick a winning branch that has the most of the\n",
    "    soma touching vertices\n",
    "3) Find the mesh and skeleton of the winning branch\n",
    "4) Add newly created branch to skeleton and divide the skeleton into branches (could make 2 or 3)\n",
    "5) Run Adaptive mesh correspondnece using branches and mesh\n",
    "6) Remove the original branch and mesh correspondence and replace with the multiples\n",
    "\n",
    "\"\"\"\n",
    "#maps the \n",
    "\n",
    "\n",
    "#for sm_ind in new_branch_info\n",
    "# sm_idx = 0\n",
    "# b_vert_idx = 0\n",
    "verbose = True\n",
    "\n",
    "for sm_idx in new_branch_info.keys():\n",
    "    for b_vert_idx in new_branch_info[sm_idx].keys():\n",
    "\n",
    "        #1) Get the newly added branch (and the original vertex which is the first row)\n",
    "        br_new,sm_bord_verts = new_branch_info[sm_idx][b_vert_idx].values()\n",
    "\n",
    "        curr_soma_to_piece_touching_vertices_MP = {sm_idx:[sm_bord_verts]}\n",
    "        endpoints_must_keep_MP = {sm_idx:[br_new[0][1]]}\n",
    "\n",
    "        orig_vertex = br_new[0][0]\n",
    "        \n",
    "        \n",
    "        #2) Find the branches that have that coordinate (could be multiple)\n",
    "        match_sk_branches = sk.find_branch_skeleton_with_specific_coordinate(segment_branches,\n",
    "            current_coordinate=orig_vertex)\n",
    "\n",
    "        #2b) If multiple branches: Pick a winning branch that has the most of the\n",
    "        #    soma touching vertices\n",
    "        if len(match_sk_branches) > 1:\n",
    "            bord_verts_tree = KDTree(sm_bord_verts.reshape(-1,3))\n",
    "            winning_branch = match_sk_branches[0]\n",
    "            dist,_ = bord_verts_tree.query(divided_submeshes[winning_branch].vertices)\n",
    "            winning_branch_n_bord_verts = np.sum(dist == 0)\n",
    "\n",
    "            for i in range(1,len(match_sk_branches)):\n",
    "                curr_branch = match_sk_branches[i]\n",
    "                dist,_ = bord_verts_tree.query(divided_submeshes[winning_branch].vertices)\n",
    "                n_bord_verts = np.sum(dist == 0)\n",
    "                if n_bord_verts>winning_branch_n_bord_verts:\n",
    "                    winning_branch_n_bord_verts = n_bord_verts\n",
    "                    winning_branch = curr_branch\n",
    "        elif len(match_sk_branches) == 1:\n",
    "            winning_branch = match_sk_branches[0]\n",
    "        else:\n",
    "            raise Exception(\"No matching branches found for soma extending point\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Winning Branch = {winning_branch}\")\n",
    "\n",
    "        #3) Find the mesh and skeleton of the winning branch\n",
    "        winning_branch_mesh = divided_submeshes[winning_branch]\n",
    "        winning_branch_mesh_idx = divided_submeshes_idx[winning_branch]\n",
    "        winning_branch_sk = segment_branches[winning_branch]\n",
    "\n",
    "        #4) Add newly created branch to skeleton and divide the skeleton into branches (could make 2 or 3)\n",
    "        extended_skeleton_to_soma = sk.stack_skeletons([winning_branch_sk,br_new])\n",
    "        soma_extending_branches = sk.decompose_skeleton_to_branches(extended_skeleton_to_soma)\n",
    "\n",
    "        #5) Run Adaptive mesh correspondnece using branches and mesh\n",
    "        local_correspondnece_MP = pre.mesh_correspondence_first_pass(mesh=winning_branch_mesh,\n",
    "                                      skeleton=extended_skeleton_to_soma)\n",
    "\n",
    "        local_correspondence_revised = pre.correspondence_1_to_1(mesh=winning_branch_mesh,\n",
    "                                                    local_correspondence=local_correspondnece_MP,\n",
    "                                                    curr_limb_endpoints_must_keep=endpoints_must_keep_MP,\n",
    "                                                    curr_soma_to_piece_touching_vertices=curr_soma_to_piece_touching_vertices_MP)\n",
    "\n",
    "        # All the things that should be revised:\n",
    "    #     segment_branches, #skeleton branches\n",
    "    #     divided_submeshes, divided_submeshes_idx, #mesh correspondence (mesh and indices)\n",
    "    #     segment_widths_median\n",
    "\n",
    "\n",
    "        new_submeshes = [k[\"branch_mesh\"] for k in local_correspondence_revised.values()]\n",
    "        new_submeshes_idx = [winning_branch_mesh_idx[k[\"branch_face_idx\"]] for k in local_correspondence_revised.values()]\n",
    "        new_skeletal_branches = [k[\"branch_skeleton\"] for k in local_correspondence_revised.values()]\n",
    "\n",
    "        #calculate the new width\n",
    "        ray_inter = tu.ray_pyembree.RayMeshIntersector(limb_mesh_mparty)\n",
    "        new_widths = []\n",
    "        for new_s_idx in new_submeshes_idx:\n",
    "            curr_ray_distance = tu.ray_trace_distance(mesh=limb_mesh_mparty, \n",
    "                                face_inds=new_s_idx,\n",
    "                               ray_inter=ray_inter)\n",
    "            new_widths.append(np.median(curr_ray_distance[curr_ray_distance!=0]))\n",
    "\n",
    "\n",
    "        #6) Remove the original branch and mesh correspondence and replace with the multiples\n",
    "    #     new_segment_branches = deepcopy(segment_branches)\n",
    "    #     new_divided_submeshes = deepcopy(divided_submeshes)\n",
    "    #     new_divided_submeshes_idx = deepcopy(divided_submeshes_idx)\n",
    "    #     new_segment_widths_median = deepcopy(segment_widths_median)\n",
    "\n",
    "        segment_branches = np.delete(segment_branches,winning_branch)\n",
    "        segment_branches = np.append(segment_branches,new_skeletal_branches,axis=0)\n",
    "\n",
    "        divided_submeshes = np.delete(divided_submeshes,winning_branch)\n",
    "        divided_submeshes = np.append(divided_submeshes,new_submeshes,axis=0)\n",
    "\n",
    "        divided_submeshes_idx = np.delete(divided_submeshes_idx,winning_branch)\n",
    "        divided_submeshes_idx = np.append(divided_submeshes_idx,new_submeshes_idx,axis=0)\n",
    "\n",
    "        segment_widths_median = np.delete(segment_widths_median,winning_branch)\n",
    "        segment_widths_median = np.append(segment_widths_median,new_widths,axis=0)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All the things that should be revised:\n",
    "# segment_branches, #skeleton branches\n",
    "# divided_submeshes, divided_submeshes_idx, #mesh correspondence (mesh and indices)\n",
    "# segment_widths_median\n",
    "\n",
    "\n",
    "# new_submeshes = [k[\"branch_mesh\"] for k in local_correspondence_revised.values()]\n",
    "# new_submeshes_idx = [winning_branch_mesh_idx[k[\"branch_face_idx\"]] for k in local_correspondence_revised.values()]\n",
    "# new_skeletal_branches = [k[\"branch_skeleton\"] for k in local_correspondence_revised.values()]\n",
    "\n",
    "# #calculate the new width\n",
    "# ray_inter = tu.ray_pyembree.RayMeshIntersector(limb_mesh_mparty)\n",
    "# new_widths = []\n",
    "# for new_s_idx in new_submeshes_idx:\n",
    "#     curr_ray_distance = tu.ray_trace_distance(mesh=limb_mesh_mparty, \n",
    "#                         face_inds=new_s_idx,\n",
    "#                        ray_inter=ray_inter)\n",
    "#     new_widths.append(np.median(curr_ray_distance[curr_ray_distance!=0]))\n",
    "\n",
    "\n",
    "# #Reassigning Now\n",
    "# new_segment_branches = deepcopy(segment_branches)\n",
    "# new_divided_submeshes = deepcopy(divided_submeshes)\n",
    "# new_divided_submeshes_idx = deepcopy(divided_submeshes_idx)\n",
    "# new_segment_widths_median = deepcopy(segment_widths_median)\n",
    "\n",
    "# new_segment_branches = np.delete(new_segment_branches,winning_branch)\n",
    "# new_segment_branches = np.append(new_segment_branches,new_skeletal_branches,axis=0)\n",
    "\n",
    "# new_divided_submeshes = np.delete(new_divided_submeshes,winning_branch)\n",
    "# new_divided_submeshes = np.append(new_divided_submeshes,new_submeshes,axis=0)\n",
    "\n",
    "# new_divided_submeshes_idx = np.delete(new_divided_submeshes_idx,winning_branch)\n",
    "# new_divided_submeshes_idx = np.append(new_divided_submeshes_idx,new_submeshes_idx,axis=0)\n",
    "\n",
    "# new_segment_widths_median = np.delete(new_segment_widths_median,winning_branch)\n",
    "# new_segment_widths_median = np.append(new_segment_widths_median,new_widths,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfbbced09604e03811155a1018b49bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nviz.plot_objects(meshes=divided_submeshes,\n",
    "                  meshes_colors=\"random\",\n",
    "                 skeletons=segment_branches,\n",
    "                  skeletons_colors=\"random\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfe26aef2904834be15f231c4ff2841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nviz.plot_objects(meshes=[k[\"branch_mesh\"] for k in local_correspondence_revised.values()],\n",
    "                  meshes_colors=\"random\",\n",
    "                skeletons=[k[\"branch_skeleton\"] for k in local_correspondence_revised.values()],\n",
    "                  skeletons_colors=\"random\",\n",
    "                 scatters=[orig_vertex],\n",
    "                 scatter_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the CGAL skeletonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Working on Soma Continaing Mesh 0--\n",
      "\n",
      "    -- Working on branch 0--\n",
      "\n",
      "    -- Working on branch 1--\n",
      "     Starting Screened Poisson\n",
      "xvfb-run -n 4715 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/temp/None.off -o /notebooks/Platinum_Decomp_Fusion/temp/None_poisson.off -s /notebooks/Platinum_Decomp_Fusion/temp/poisson_922211.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/temp/None.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/temp/None_poisson.off\n",
      "Using the close holes feature\n",
      "xvfb-run -n 6153 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/temp/neuron_14072.off -o /notebooks/Platinum_Decomp_Fusion/temp/neuron_14072_fill_holes.off -s /notebooks/Platinum_Decomp_Fusion/temp/fill_holes_360365.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/temp/neuron_14072.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/temp/neuron_14072_fill_holes.off\n",
      "-----Time for Screened Poisson= 34.262648582458496\n",
      "     Starting Calcification\n",
      "Before mesh subtraction number of skeleton edges = 3186\n",
      "Inside mesh subtraction, len(main_mesh_bbox_restricted.faces) = 215445\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a98f9d7d03c49b697689214f54e839b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trimesh:face_normals all zero, ignoring!\n",
      "WARNING:trimesh:face_normals all zero, ignoring!\n",
      "WARNING:trimesh:face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Mesh subtraction time = 43.4393\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b6a72befdd484e81a6cdefd12a869f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len_subgraphs AT BEGINNING of the loop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c4d20b0ffd48778b3a20adadfc7208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=168.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all graph is one component!\n",
      "\n",
      "Total time for skeleton stitching = 0.38226771354675293\n",
      "The process was using a temp folder\n",
      "    Total time for skeletonizing branch: 87.33870434761047\n",
      "skipping soma 0 because closest skeleton node was already end node\n",
      "Adding new branch to skeleton\n",
      "    Total time for Fixing Skeleton Soma Endpoint Extension : 15.409197807312012\n",
      "filter_end_node_length = 4001\n",
      "Using Distance measure skeletal_distance\n",
      "Going to ignore certain endnodes that are 5000 nm close to soma border vertices\n",
      "Number of end_nodes BEFORE filtering = 120\n",
      "Using an already specified end node: 1673 with index 59checking was correct node end_nodes[index] = 1673\n",
      "Using an already specified end node: 1571 with index 55checking was correct node end_nodes[index] = 1571\n",
      "May Eliminate end_node 59: 1673 because path_len to soma border was 2070.282670892157\n",
      "single_node_to_eliminate = 59\n",
      "all_single_nodes_to_eliminate = [59, 55, 59]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f935ab01cf4c6188912be2e7a36b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********COMBINING CLOSE SKELETON NODES WITHIN 700 DISTANCE**********\n",
      "Found 0 valid paths to replace\n",
      "valid_paths = []\n",
      "valid_path_lengths = []\n",
      "No valid paths found so just returning the original\n",
      "After skeletonization and cleaning skleelton size = (3308, 2, 3)\n",
      "    Total time for cleaning of branch 1: 7000.877564907074\n",
      "Working on limb correspondence for #1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7275dd7c7c6f4e7287f875ff0b4b3d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trimesh:face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time for decomposition = 44.051918745040894\n",
      "\n",
      "    -- Working on branch 2--\n",
      "\n",
      "    -- Working on branch 3--\n",
      "\n",
      "    -- Working on branch 4--\n",
      "\n",
      "    -- Working on branch 5--\n",
      "Total time for skeletonization = 154.63969540596008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_start_time = time.time()\n",
    "endpoints_must_keep = dict()\n",
    "\n",
    "perform_cleaning_checks = False\n",
    "\n",
    "\n",
    "#For the limb correspondence\n",
    "limb_correspondence = dict()\n",
    "soma_containing_idx= 0\n",
    "\n",
    "if branch_skeleton_data is None:\n",
    "    for j,(soma_containing_mesh_idx,mesh_data) in enumerate(soma_touching_mesh_data.items()):\n",
    "\n",
    "#                 sk_debug = True\n",
    "#                 if sk_debug:\n",
    "#                     import system_utils as su\n",
    "#                     su.compressed_pickle(mesh_data[\"branch_meshes\"],\n",
    "#                                         \"ordered_branch_meshes\")\n",
    "#                 raise Exception(\"Done exporting branches\")\n",
    "\n",
    "        \n",
    "        \n",
    "        print(f\"\\n-- Working on Soma Continaing Mesh {j}--\")\n",
    "        current_branches = mesh_data[\"branch_meshes\"]\n",
    "\n",
    "        #skeletonize each of the branches\n",
    "        total_skeletons = []\n",
    "\n",
    "        for z,branch in enumerate(current_branches):\n",
    "            print(f\"\\n    -- Working on branch {z}--\")\n",
    "            if z != 1:\n",
    "                continue\n",
    "            mesh_start_time = time.time()\n",
    "                \n",
    "            # ---- 0) Generating the Clean skeletons  -------------------------------------------#\n",
    "            total_border_vertices = dict()\n",
    "            for k in soma_to_piece_touching_vertices.keys():\n",
    "                if z in soma_to_piece_touching_vertices[k].keys():\n",
    "                    total_border_vertices[k] = np.vstack(soma_to_piece_touching_vertices[k][z])\n",
    "            \n",
    "            \n",
    "            \n",
    "            #build the soma to piece touching vertices dictionary for this neuorn\n",
    "            curr_soma_to_piece_touching_vertices = dict()\n",
    "            for s_index,v in soma_to_piece_touching_vertices.items():\n",
    "                if z not in v:\n",
    "                    continue\n",
    "                curr_soma_to_piece_touching_vertices[s_index] = soma_to_piece_touching_vertices[s_index][z]\n",
    "                \n",
    "            cleaned_branch,curr_limb_endpoints_must_keep = sk.skeletonize_and_clean_connected_branch_CGAL(\n",
    "                mesh=branch,\n",
    "                curr_soma_to_piece_touching_vertices=curr_soma_to_piece_touching_vertices,\n",
    "                total_border_vertices=total_border_vertices,\n",
    "                filter_end_node_length=filter_end_node_length,\n",
    "                perform_cleaning_checks=perform_cleaning_checks,\n",
    "                combine_close_skeleton_nodes = combine_close_skeleton_nodes,\n",
    "                combine_close_skeleton_nodes_threshold=combine_close_skeleton_nodes_threshold)\n",
    "            \n",
    "            \n",
    "            endpoints_must_keep[z] = curr_limb_endpoints_must_keep\n",
    "            \n",
    "            #do the cleanin ghtat removes loops from branches\n",
    "            print(f\"After skeletonization and cleaning skleelton size = {cleaned_branch.shape}\")\n",
    "            total_skeletons.append(cleaned_branch)\n",
    "\n",
    "\n",
    "            print(f\"    Total time for cleaning of branch {z}: {time.time() - clean_time}\")\n",
    "            if len(cleaned_branch) == 0:\n",
    "                raise Exception(f\"Found a zero length skeleton for limb {z} of trmesh {branch}\")\n",
    "            total_skeletons.append(cleaned_branch)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # ---- 1) Generating Initial Mesh Correspondence -------------------------------------------#\n",
    "            start_time = time.time()\n",
    "            limb_idx = z\n",
    "            \n",
    "            print(f\"Working on limb correspondence for #{limb_idx}\")\n",
    "            local_correspondence = pre.mesh_correspondence_first_pass(mesh=branch,\n",
    "                                                                 skeleton=cleaned_branch,\n",
    "                                                                 distance_by_mesh_center=distance_by_mesh_center)\n",
    "            limb_correspondence[limb_idx] = local_correspondence\n",
    "            \n",
    "            \n",
    "            print(f\"Total time for decomposition = {time.time() - start_time}\")\n",
    "            \n",
    "            #------------- 2) Doing Some checks on the initial corespondence -------- #\n",
    "            \n",
    "            perform_cleaning_checks = True       \n",
    "            if perform_cleaning_checks:\n",
    "                pre.check_skeletonization_and_decomp(skeleton=cleaned_branch,\n",
    "                                                local_correspondence=local_correspondence)\n",
    "                \n",
    "            # -------3) Finishing off the face correspondence so get 1-to-1 correspondence of mesh face to skeletal piece\n",
    "            local_correspondence_revised = pre.correspondence_1_to_1(mesh=branch,\n",
    "                                            local_correspondence=local_correspondence,\n",
    "                                            curr_limb_endpoints_must_keep=curr_limb_endpoints_must_keep,\n",
    "                                            curr_soma_to_piece_touching_vertices=curr_soma_to_piece_touching_vertices)\n",
    "\n",
    "            print(f\"Total time for limb mesh processing = {time.time() - mesh_start_time}\")\n",
    "\n",
    "        soma_touching_mesh_data[j][\"branch_skeletons_cleaned\"] = total_skeletons\n",
    "\n",
    "    print(f\"Total time for skeletonization and mesh corespondence = {time.time() - global_start_time}\")\n",
    "\n",
    "else:\n",
    "    print(\"****Skipping skeleton cleaning and USING THE PRE-COMPUTED SKELETONS ****\")\n",
    "    soma_touching_mesh_data[0][\"branch_skeletons_cleaned\"] =branch_skeleton_data\n",
    "    \n",
    "current_mesh_data = soma_touching_mesh_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [TrackedArray([[580478.6, 771669. , 885399.6],\n",
       "                [579293.8, 772063.6, 885518. ],\n",
       "                [580218.1, 771431.4, 885658.1],\n",
       "                [579967.5, 772038.8, 885108. ],\n",
       "                [579810. , 771918. , 885108. ],\n",
       "                [579749.8, 771992.9, 885150.2],\n",
       "                [580156.6, 771634.1, 885833.4],\n",
       "                [579800.9, 771569.4, 885956.9],\n",
       "                [579190.2, 771672.2, 885807.2],\n",
       "                [580298.4, 771508.4, 885558.6],\n",
       "                [579088.1, 771776.2, 885706. ],\n",
       "                [580237.9, 771731.7, 885095.4],\n",
       "                [580083. , 771839.2, 885097.5],\n",
       "                [579342.4, 771610.4, 885914.6],\n",
       "                [580324.5, 771687. , 885606.8],\n",
       "                [579682.6, 772060.9, 885191. ],\n",
       "                [579552.9, 771584.2, 885938.3],\n",
       "                [579148.5, 771750. , 885801. ],\n",
       "                [580399.5, 771822.4, 885275.6],\n",
       "                [580209.7, 771929.2, 885127.6],\n",
       "                [579396.1, 771929. , 885393.1],\n",
       "                [579529.8, 771752.9, 886008.8],\n",
       "                [579686.8, 771633.1, 885954.6],\n",
       "                [579159.5, 771898.9, 885562.4],\n",
       "                [579519.4, 772073. , 885284.4],\n",
       "                [580234.8, 771703.7, 885771.6],\n",
       "                [579957. , 771553.1, 885885. ]]),\n",
       "  TrackedArray([[572892.4, 784112.7, 890130.9],\n",
       "                [574874.6, 784909.7, 892020. ],\n",
       "                [573535. , 784217.2, 889387.8],\n",
       "                [573227.4, 784303.4, 889605. ],\n",
       "                [573390.1, 784125.6, 889503.9],\n",
       "                [572945. , 783962.2, 890017.7],\n",
       "                [575680.2, 785088.4, 890166.6],\n",
       "                [575244.9, 785021.9, 891612.5],\n",
       "                [574781.2, 785014.6, 888710.4],\n",
       "                [574374.5, 784522.3, 888754.2],\n",
       "                [572451.5, 784365.9, 891610.9],\n",
       "                [575645.8, 785055.8, 891261.9],\n",
       "                [573284.6, 784642.2, 892474.9],\n",
       "                [573875.2, 784410.9, 889063.9],\n",
       "                [575397.4, 784923.8, 891602.9],\n",
       "                [572451.1, 784301.3, 891217.1],\n",
       "                [575660.2, 785045. , 890456.9],\n",
       "                [575716.5, 785054.4, 889921.1],\n",
       "                [574571.1, 784848.5, 888726.6],\n",
       "                [575550.1, 785012.2, 891440.2],\n",
       "                [575446.9, 785097.5, 889604. ],\n",
       "                [575163.9, 784979.5, 891740.9],\n",
       "                [575772.1, 784957.9, 891203.4],\n",
       "                [574481.1, 784669.9, 888718.7],\n",
       "                [575778.2, 785050.3, 891067.9],\n",
       "                [573451.5, 784447.1, 892683.8],\n",
       "                [572657.1, 784240.8, 890560. ],\n",
       "                [572449.5, 784581. , 891891. ],\n",
       "                [575044.8, 785239.4, 888962.1],\n",
       "                [574150.2, 784656.4, 892523.2],\n",
       "                [574904.2, 784986.1, 888674.7],\n",
       "                [575730.8, 785021.4, 890313.5],\n",
       "                [572439. , 784528.5, 891796.5],\n",
       "                [574120.2, 784193.4, 888891.3],\n",
       "                [575780.2, 784941.8, 890518.7],\n",
       "                [575906.8, 784910. , 891063.8],\n",
       "                [572661. , 784409.8, 892315.4],\n",
       "                [573733.2, 784357.2, 889177.4],\n",
       "                [573951. , 784434. , 892741.5],\n",
       "                [574750.6, 784969.8, 891984.8],\n",
       "                [574051.3, 784446.8, 888936.8],\n",
       "                [573149.2, 784620.4, 892462.5],\n",
       "                [574959.9, 785028.8, 891839.9],\n",
       "                [573804. , 784329. , 892815. ],\n",
       "                [573987.8, 784218.8, 889003.5],\n",
       "                [573448.1, 784625.4, 892537.3],\n",
       "                [572449.8, 784457.8, 892073. ],\n",
       "                [572585.6, 784314.4, 890769.6],\n",
       "                [575158.9, 785280.8, 889281.2],\n",
       "                [575050.1, 785019.4, 888739.9],\n",
       "                [573330.7, 784528.3, 892585.9],\n",
       "                [573174. , 784213.5, 889717.5],\n",
       "                [573792.8, 784462.2, 892665.2],\n",
       "                [573026.2, 784141.7, 889863.1],\n",
       "                [574038.9, 784593.9, 892594.9],\n",
       "                [575259.2, 784887. , 891783.5],\n",
       "                [572946.3, 784218.8, 889985.4],\n",
       "                [572964. , 784518. , 892374. ],\n",
       "                [573616.5, 784216.3, 889309.5],\n",
       "                [575695. , 785054.2, 890671.8],\n",
       "                [574413.7, 784957. , 892146.2],\n",
       "                [575514.8, 785160.8, 889848.7],\n",
       "                [572829.1, 784141.8, 890264.2],\n",
       "                [574624.5, 784963.7, 888719.5],\n",
       "                [572730.3, 784174.5, 890428.4],\n",
       "                [575134.6, 785363.2, 889413.2],\n",
       "                [574170. , 784769.2, 892323.5],\n",
       "                [574324.9, 784765. , 892278.7],\n",
       "                [572870.8, 784425.9, 892437.6],\n",
       "                [575492.6, 784941.5, 891520.2],\n",
       "                [572565. , 784486.5, 892227. ],\n",
       "                [575305.5, 785232. , 889570.5],\n",
       "                [574955.1, 785224. , 888785.3],\n",
       "                [572544. , 784476. , 890909.2],\n",
       "                [573923.4, 784527.9, 892629. ],\n",
       "                [573150.6, 784446.4, 892538.2],\n",
       "                [573971.2, 784563.4, 889003.8],\n",
       "                [575842.1, 785020.5, 890883.1],\n",
       "                [572417.9, 784473.3, 891669.2],\n",
       "                [573972.2, 784679.5, 892465.9],\n",
       "                [572484.6, 784428.8, 891115.8],\n",
       "                [575047.4, 784896.4, 891946.6],\n",
       "                [572442.6, 784464.8, 891332.6],\n",
       "                [574286.4, 784457.6, 888801.9],\n",
       "                [574531.7, 784940.7, 892098.4],\n",
       "                [573689.1, 784479.2, 892636.6]])]}"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_soma_to_piece_touching_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[579716.        , 769970.        , 884968.        ],\n",
       "        [574112.13372093, 784665.63837209, 890838.61395349]])}"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_limb_endpoints_must_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7eb58d3cb804bb29b64cb1b1b4b0278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# curr_branch = local_correspondence[8]\n",
    "# nviz.plot_objects(meshes=curr_branch[\"correspondence_mesh\"],\n",
    "#                 skeletons=[curr_branch[\"branch_skeleton\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
