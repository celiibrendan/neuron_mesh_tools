{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo make sure the fusion decomposition works\\nup to the part where we would stitch the sublimbs together into one limb\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To make sure the fusion decomposition works\n",
    "up to the part where we would stitch the sublimbs together into one limb\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Need to pip install annotationframeworkclient to repair mesh with pychunkedgraph\n",
      "WARNING:root:Need to pip install annotationframeworkclient to use dataset_name parameters\n"
     ]
    }
   ],
   "source": [
    "import skeleton_utils as sk\n",
    "import soma_extraction_utils as sm\n",
    "import trimesh_utils as tu\n",
    "import trimesh\n",
    "import numpy_utils as nu\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import networkx as nx\n",
    "import time\n",
    "import compartment_utils as cu\n",
    "import networkx_utils as xu\n",
    "import matplotlib_utils as mu\n",
    "import neuron_utils as nru\n",
    "\n",
    "#importing at the bottom so don't get any conflicts\n",
    "import itertools\n",
    "from tqdm_utils import tqdm\n",
    "\n",
    "#for meshparty preprocessing\n",
    "import meshparty_skeletonize as m_sk\n",
    "import general_utils as gu\n",
    "import compartment_utils as cu\n",
    "from meshparty import trimesh_io\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from neuron_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datajoint.settings:Setting database.host to at-database.ad.bcm.edu\n",
      "INFO:datajoint.settings:Setting database.user to celiib\n",
      "INFO:datajoint.settings:Setting database.password to newceliipass\n",
      "INFO:datajoint.settings:Setting stores to {'minnie65': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65', 'stage': '/mnt/dj-stor01/platinum/minnie65'}, 'meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/meshes'}, 'decimated_meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes'}, 'skeletons': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/skeletons'}}\n",
      "INFO:datajoint.settings:Setting enable_python_native_blobs to True\n",
      "INFO:datajoint.settings:Setting enable_python_native_blobs to True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@at-database.ad.bcm.edu:3306\n"
     ]
    }
   ],
   "source": [
    "import datajoint_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_visualizations as nviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_segment_id = 864691135471571396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/traittypes/traittypes.py:101: UserWarning: Given trait value dtype \"float64\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  np.dtype(self.dtype).name))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1463cb975d1e4e7bb36b2079a1b195c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "segment_id = 864691135471571396\n",
    "\n",
    "print(f\"curr_segment_id = {segment_id}\")\n",
    "current_neuron = du.fetch_segment_id_mesh(segment_id)\n",
    "nviz.plot_objects(current_neuron)\n",
    "\n",
    "description = \"triple_soma_error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import trimesh_utils as tu\n",
    "# curent_neuron = tu.load_mesh_no_processing(\"/notebooks/test_neurons/Segmentation_2/864691135738362516_thin_long_down_axon.off\")\n",
    "# segment_id = 864691135738362516\n",
    "# description = \"thin_long_down_axon\"\n",
    "\n",
    "# import neuron_visualizations as nviz\n",
    "# nviz.plot_objects(main_mesh=curent_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Arguments that would be present inside a preprocessing function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predefined arguments for the Neuron constructor\n",
    "\n",
    "decomposition_type=\"meshafterparty\"\n",
    "mesh_correspondence=\"meshparty\" #meshafterparty_adaptive\n",
    "distance_by_mesh_center=True #how the distance is calculated for mesh correspondence\n",
    "meshparty_segment_size = 100\n",
    "meshparty_n_surface_downsampling = 2\n",
    "meshparty_adaptive_correspondence_after_creation=False\n",
    "suppress_preprocessing_print=True\n",
    "computed_attribute_dict=None\n",
    "somas = None\n",
    "branch_skeleton_data=None\n",
    "combine_close_skeleton_nodes = True\n",
    "combine_close_skeleton_nodes_threshold=700\n",
    "ignore_warnings=True\n",
    "suppress_output=False\n",
    "calculate_spines=True\n",
    "widths_to_calculate=[\"no_spine_median_mesh_center\"]\n",
    "fill_hole_size=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arguments for the preprocess neuron\n",
    "mesh = current_neuron\n",
    "segment_id=segment_id\n",
    "description=description\n",
    "\n",
    "sig_th_initial_split=15 #for significant splitting meshes in the intial mesh split\n",
    "limb_threshold = 2000 #the mesh faces threshold for a mesh to be qualified as a limb (otherwise too small)\n",
    "filter_end_node_length=4001 #used in cleaning the skeleton during skeletonizations\n",
    "return_no_somas = False\n",
    "\n",
    "decomposition_type=decomposition_type\n",
    "mesh_correspondence=mesh_correspondence\n",
    "distance_by_mesh_center=distance_by_mesh_center\n",
    "meshparty_segment_size =meshparty_segment_size\n",
    "meshparty_n_surface_downsampling = meshparty_n_surface_downsampling\n",
    "somas=somas\n",
    "branch_skeleton_data=branch_skeleton_data\n",
    "combine_close_skeleton_nodes = combine_close_skeleton_nodes\n",
    "combine_close_skeleton_nodes_threshold=combine_close_skeleton_nodes_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_processing_tiempo = time.time()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Purpose: To process the mesh into a format that can be loaded into the neuron class\n",
    "and used for higher order processing (how to visualize is included)\n",
    "\n",
    "\"\"\"\n",
    "if description is None:\n",
    "    description = \"no_description\"\n",
    "if segment_id is None:\n",
    "    #pick a random segment id\n",
    "    segment_id = np.random.randint(100000000)\n",
    "    print(f\"picking a random 7 digit segment id: {segment_id}\")\n",
    "    description += \"_random_id\"\n",
    "\n",
    "\n",
    "if mesh is None:\n",
    "    if current_mesh_file is None:\n",
    "        raise Exception(\"No mesh or mesh_file file were given\")\n",
    "    else:\n",
    "        current_neuron = trimesh.load_mesh(current_mesh_file)\n",
    "else:\n",
    "    current_neuron = mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************************ Phase A: Soma and Limb Identification ********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 15000.0 \n",
      "large_mesh_threshold_inner = 10000.0 \n",
      "soma_size_threshold = 937.5 \n",
      "soma_size_threshold_max = 12000.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "xvfb-run -n 9484 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/864691135471571396/neuron_864691135471571396.off -o /notebooks/Platinum_Decomp_Fusion/864691135471571396/neuron_864691135471571396_decimated.off -s /notebooks/Platinum_Decomp_Fusion/864691135471571396/decimation_meshlab_25160243.mls\n",
      "Total found significant pieces before Poisson = [<trimesh.Trimesh(vertices.shape=(9255, 3), faces.shape=(23395, 3))>]\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(9255, 3), faces.shape=(23395, 3))>\n",
      "pre_largest_mesh_path = /notebooks/Platinum_Decomp_Fusion/864691135471571396/neuron_864691135471571396_decimated_largest_piece.off\n",
      "xvfb-run -n 7957 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/864691135471571396/neuron_864691135471571396_decimated_largest_piece.off -o /notebooks/Platinum_Decomp_Fusion/864691135471571396/neuron_864691135471571396_decimated_largest_piece_poisson.off -s /notebooks/Platinum_Decomp_Fusion/864691135471571396/poisson_377857.mls\n",
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(44039, 3), faces.shape=(88526, 3))>]\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(44039, 3), faces.shape=(88526, 3))>\n",
      "xvfb-run -n 7381 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/864691135471571396/neuron_864691135471571396_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Platinum_Decomp_Fusion/864691135471571396/neuron_864691135471571396_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Platinum_Decomp_Fusion/864691135471571396/decimation_meshlab_2517357.mls\n",
      "done exporting decimated mesh: neuron_864691135471571396_decimated_largest_piece_poisson_largest_inner.off\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0003085136413574219\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Platinum_Decomp_Fusion/temp/86469113547157139600_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 3.0496089458465576\n",
      "2) Finished: Generating CGAL segmentation for neuron: 3.3968801498413086\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 1\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.008179187774658203\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 5.030632019042969e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.017111539840698242\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.21296262741088867\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.209122\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 8,  6,  1,  5,  9,  0,  3,  7,  4, 11, 10,  2]), array([0.523375  , 0.465381  , 0.209122  , 0.150633  , 0.103505  ,\n",
      "       0.0606204 , 0.04036385, 0.0392553 , 0.031434  , 0.02428165,\n",
      "       0.0202881 , 0.0120462 ]))\n",
      "Sizes = [2221, 1585, 13713, 3173, 460, 809, 54, 19, 13, 24, 7, 38]\n",
      "valid_soma_segments_width\n",
      "      ------ Found 2 viable somas: [8, 6]\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 9433 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_387808.off -o /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_387808_poisson.off -s /notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_399343.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_387808.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_387808_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_399343.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 4.147276828679378\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 9404 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_452192.off -o /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_452192_poisson.off -s /notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_258411.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_452192.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_452192_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_258411.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 3.7391501345711893\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 30.56484055519104\n",
      "Before Filtering the number of somas found = 2\n",
      "Performing Soma Mesh Backtracking to original mesh\n",
      "# total split meshes = 2\n",
      "viable_meshes = [0]\n",
      "There were 1 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0]}\n",
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "current_soma_mesh_list = [<trimesh.Trimesh(vertices.shape=(1127, 3), faces.shape=(2221, 3))>]\n",
      "current_mesh = <trimesh.Trimesh(vertices.shape=(9255, 3), faces.shape=(23345, 3))>\n",
      "\n",
      "inside Soma subtraction\n",
      "mesh pieces in subtact soma BEFORE the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(8538, 3), faces.shape=(21653, 3))>]\n",
      "There were 1 pieces found after size threshold\n",
      "mesh pieces in subtact soma AFTER the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(8538, 3), faces.shape=(21653, 3))>]\n",
      "Total Time for soma mesh cancellation = 0.13\n",
      "mesh_pieces_without_soma = [<trimesh.Trimesh(vertices.shape=(8538, 3), faces.shape=(21653, 3))>]\n",
      "Total time for Subtract Soam = 0.1305828094482422\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 0.047949790954589844\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 4894 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_139547.off -o /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_139547_poisson.off -s /notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_88934.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_139547.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_139547_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_88934.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 4.635592125096772\n",
      "Performing Soma Mesh Backtracking to original mesh\n",
      "# total split meshes = 2\n",
      "viable_meshes = [0]\n",
      "There were 1 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0]}\n",
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "current_soma_mesh_list = [<trimesh.Trimesh(vertices.shape=(826, 3), faces.shape=(1585, 3))>]\n",
      "current_mesh = <trimesh.Trimesh(vertices.shape=(9255, 3), faces.shape=(23345, 3))>\n",
      "\n",
      "inside Soma subtraction\n",
      "mesh pieces in subtact soma BEFORE the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(7724, 3), faces.shape=(19666, 3))>, <trimesh.Trimesh(vertices.shape=(974, 3), faces.shape=(2184, 3))>]\n",
      "There were 2 pieces found after size threshold\n",
      "mesh pieces in subtact soma AFTER the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(7724, 3), faces.shape=(19666, 3))>, <trimesh.Trimesh(vertices.shape=(974, 3), faces.shape=(2184, 3))>]\n",
      "Total Time for soma mesh cancellation = 0.096\n",
      "mesh_pieces_without_soma = [<trimesh.Trimesh(vertices.shape=(7724, 3), faces.shape=(19666, 3))>, <trimesh.Trimesh(vertices.shape=(974, 3), faces.shape=(2184, 3))>]\n",
      "Total time for Subtract Soam = 0.09658598899841309\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 0.031469106674194336\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 4057 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_470253.off -o /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_470253_poisson.off -s /notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_423445.mls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_470253.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_470253_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_423445.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 5.031424592205093\n",
      "Soma List = [<trimesh.Trimesh(vertices.shape=(748, 3), faces.shape=(1692, 3))>, <trimesh.Trimesh(vertices.shape=(667, 3), faces.shape=(1495, 3))>]\n",
      "soma_mesh_list_centers = [array([1255656.67112299,  801413.90748663,  988112.30080214]), array([1285277.6131934 ,  790706.47271364, 1011040.59070465])]\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Doing the soma detection\n",
    "if somas is None:\n",
    "    soma_mesh_list,run_time,total_soma_list_sdf = sm.extract_soma_center(segment_id,\n",
    "                                             current_neuron.vertices,\n",
    "                                             current_neuron.faces)\n",
    "else:\n",
    "    soma_mesh_list,run_time,total_soma_list_sdf = somas\n",
    "\n",
    "# geting the soma centers\n",
    "if len(soma_mesh_list) <= 0:\n",
    "    print(f\"**** No Somas Found for Mesh {segment_id} so just one mesh\")\n",
    "    soma_mesh_list_centers = []\n",
    "    if return_no_somas:\n",
    "        return_value= soma_mesh_list_centers\n",
    "    raise Exception(\"Processing of No Somas is not yet implemented yet\")\n",
    "else:\n",
    "    #compute the soma centers\n",
    "    print(f\"Soma List = {soma_mesh_list}\")\n",
    "\n",
    "    soma_mesh_list_centers = sm.find_soma_centroids(soma_mesh_list)\n",
    "    print(f\"soma_mesh_list_centers = {soma_mesh_list_centers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total split meshes = 3\n",
      "There were 2 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0, 1]}\n"
     ]
    }
   ],
   "source": [
    "#--- 2) getting the soma submeshes that are connected to each soma and identifiying those that aren't (and eliminating any mesh pieces inside the soma)\n",
    "\n",
    "main_mesh_total = current_neuron\n",
    "\n",
    "\n",
    "#finding the mesh pieces that contain the soma\n",
    "#splitting the current neuron into distinct pieces\n",
    "split_meshes = tu.split_significant_pieces(\n",
    "                            main_mesh_total,\n",
    "                            significance_threshold=sig_th_initial_split,\n",
    "                            print_flag=False)\n",
    "\n",
    "print(f\"# total split meshes = {len(split_meshes)}\")\n",
    "\n",
    "\n",
    "#returns the index of the split_meshes index that contains each soma    \n",
    "containing_mesh_indices = sm.find_soma_centroid_containing_meshes(soma_mesh_list,\n",
    "                                        split_meshes)\n",
    "\n",
    "# filtering away any of the inside floating pieces: \n",
    "non_soma_touching_meshes = [m for i,m in enumerate(split_meshes)\n",
    "                 if i not in list(containing_mesh_indices.values())]\n",
    "\n",
    "\n",
    "#Adding the step that will filter away any pieces that are inside the soma\n",
    "if len(non_soma_touching_meshes) > 0 and len(soma_mesh_list) > 0:\n",
    "    \"\"\"\n",
    "    *** want to save these pieces that are inside of the soma***\n",
    "    \"\"\"\n",
    "\n",
    "    non_soma_touching_meshes,inside_pieces = sm.filter_away_inside_soma_pieces(soma_mesh_list,non_soma_touching_meshes,\n",
    "                                    significance_threshold=sig_th_initial_split,\n",
    "                                    return_inside_pieces = True)                                                      \n",
    "\n",
    "\n",
    "split_meshes # the meshes of the original mesh\n",
    "containing_mesh_indices #the mapping of each soma centroid to the correct split mesh\n",
    "soma_containing_meshes = sm.grouping_containing_mesh_indices(containing_mesh_indices)\n",
    "\n",
    "soma_touching_meshes = [split_meshes[k] for k in soma_containing_meshes.keys()]\n",
    "\n",
    "\n",
    "#     print(f\"# of non soma touching seperate meshes = {len(non_soma_touching_meshes)}\")\n",
    "#     print(f\"# of inside pieces = {len(inside_pieces)}\")\n",
    "print(f\"# of soma containing seperate meshes = {len(soma_touching_meshes)}\")\n",
    "print(f\"meshes with somas = {soma_containing_meshes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "\n",
      "inside Soma subtraction\n",
      "mesh pieces in subtact soma BEFORE the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(31420, 3), faces.shape=(66700, 3))>, <trimesh.Trimesh(vertices.shape=(5171, 3), faces.shape=(10371, 3))>]\n",
      "There were 2 pieces found after size threshold\n",
      "mesh pieces in subtact soma AFTER the filtering inside pieces = [<trimesh.Trimesh(vertices.shape=(31420, 3), faces.shape=(66700, 3))>, <trimesh.Trimesh(vertices.shape=(5171, 3), faces.shape=(10371, 3))>]\n",
      "Total Time for soma mesh cancellation = 0.213\n",
      "Total time for Subtract Soam = 0.21416258811950684\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 0.06694221496582031\n",
      "Total time for Original_mesh_faces_map for somas= 0.05743718147277832\n",
      "Total time for sig_non_soma_pieces= 0.07738685607910156\n",
      "Total time for split= 0.019165754318237305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:318: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for mesh_pieces_connectivity= 0.8974361419677734\n",
      "# of insignificant_limbs = 0 with trimesh : []\n"
     ]
    }
   ],
   "source": [
    "tu = reload(tu)\n",
    "#--- 3)  Soma Extraction was great (but it wasn't the original soma faces), so now need to get the original soma faces and the original non-soma faces of original pieces\n",
    "\n",
    "#     sk.graph_skeleton_and_mesh(other_meshes=[soma_meshes])\n",
    "\n",
    "\"\"\"\n",
    "for each soma touching mesh get the following:\n",
    "1) original soma meshes\n",
    "2) significant mesh pieces touching these somas\n",
    "3) The soma connectivity to each of the significant mesh pieces\n",
    "-- later will just translate the \n",
    "\n",
    "\n",
    "Process: \n",
    "\n",
    "1) Final all soma faces (through soma extraction and then soma original faces function)\n",
    "2) Subtact all soma faces from original mesh\n",
    "3) Find all significant mesh pieces\n",
    "4) Backtrack significant mesh pieces to orignal mesh and find connectivity of each to all\n",
    "   the available somas\n",
    "Conclusion: Will have connectivity map\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "soma_touching_mesh_data = dict()\n",
    "\n",
    "for z,(mesh_idx, soma_idxes) in enumerate(soma_containing_meshes.items()):\n",
    "    soma_touching_mesh_data[z] = dict()\n",
    "    print(f\"\\n\\n----Working on soma-containing mesh piece {z}----\")\n",
    "\n",
    "    #1) Final all soma faces (through soma extraction and then soma original faces function)\n",
    "    current_mesh = split_meshes[mesh_idx]\n",
    "\n",
    "    current_soma_mesh_list = [soma_mesh_list[k] for k in soma_idxes]\n",
    "\n",
    "    current_time = time.time()\n",
    "    mesh_pieces_without_soma = sm.subtract_soma(current_soma_mesh_list,current_mesh,\n",
    "                                                significance_threshold=250)\n",
    "    print(f\"Total time for Subtract Soam = {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "\n",
    "    mesh_pieces_without_soma_stacked = tu.combine_meshes(mesh_pieces_without_soma)\n",
    "\n",
    "    # find the original soma faces of mesh\n",
    "    soma_faces = tu.original_mesh_faces_map(current_mesh,mesh_pieces_without_soma_stacked,matching=False)\n",
    "    print(f\"Total time for Original_mesh_faces_map for mesh_pieces without soma= {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "    soma_meshes = current_mesh.submesh([soma_faces],append=True,repair=False)\n",
    "\n",
    "    # finding the non-soma original faces\n",
    "    non_soma_faces = tu.original_mesh_faces_map(current_mesh,soma_meshes,matching=False)\n",
    "    non_soma_stacked_mesh = current_mesh.submesh([non_soma_faces],append=True,repair=False)\n",
    "\n",
    "    print(f\"Total time for Original_mesh_faces_map for somas= {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "\n",
    "    # 3) Find all significant mesh pieces\n",
    "    sig_non_soma_pieces,insignificant_limbs = tu.split_significant_pieces(non_soma_stacked_mesh,significance_threshold=limb_threshold,\n",
    "                                                     return_insignificant_pieces=True)\n",
    "\n",
    "    print(f\"Total time for sig_non_soma_pieces= {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "\n",
    "    soma_touching_mesh_data[z][\"branch_meshes\"] = sig_non_soma_pieces\n",
    "\n",
    "    #4) Backtrack significant mesh pieces to orignal mesh and find connectivity of each to all the available somas\n",
    "    # get all the seperate mesh faces\n",
    "\n",
    "    #How to seperate the mesh faces\n",
    "    seperate_soma_meshes,soma_face_components = tu.split(soma_meshes,only_watertight=False)\n",
    "    #take the top largest ones depending how many were originally in the soma list\n",
    "    seperate_soma_meshes = seperate_soma_meshes[:len(soma_mesh_list)]\n",
    "    soma_face_components = soma_face_components[:len(soma_mesh_list)]\n",
    "\n",
    "    soma_touching_mesh_data[z][\"soma_meshes\"] = seperate_soma_meshes\n",
    "\n",
    "    print(f\"Total time for split= {time.time() - current_time}\")\n",
    "    current_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    soma_to_piece_connectivity = dict()\n",
    "    soma_to_piece_touching_vertices = dict()\n",
    "    soma_to_piece_touching_vertices_idx = dict()\n",
    "    limb_root_nodes = dict()\n",
    "    \n",
    "    m_vert_graph = tu.mesh_vertex_graph(current_mesh)\n",
    "    \n",
    "    for i,curr_soma in enumerate(seperate_soma_meshes):\n",
    "        (connected_mesh_pieces,\n",
    "         connected_mesh_pieces_vertices,\n",
    "         connected_mesh_pieces_vertices_idx) = tu.mesh_pieces_connectivity(\n",
    "                        main_mesh=current_mesh,\n",
    "                        central_piece=curr_soma,\n",
    "                        periphery_pieces = sig_non_soma_pieces,\n",
    "                        return_vertices = True,\n",
    "                        return_vertices_idx=True)\n",
    "        #print(f\"soma {i}: connected_mesh_pieces = {connected_mesh_pieces}\")\n",
    "        soma_to_piece_connectivity[i] = connected_mesh_pieces\n",
    "\n",
    "        soma_to_piece_touching_vertices[i] = dict()\n",
    "        for piece_index,piece_idx in enumerate(connected_mesh_pieces):\n",
    "            limb_root_nodes[piece_idx] = connected_mesh_pieces_vertices[piece_index][0]\n",
    "            \n",
    "            \"\"\" Old way of finding vertex connected components on a mesh without trimesh function\n",
    "            #find the number of touching groups and save those \n",
    "            soma_touching_graph = m_vert_graph.subgraph(connected_mesh_pieces_vertices_idx[piece_index])\n",
    "            soma_con_comp = [current_mesh.vertices[np.array(list(k)).astype(\"int\")] for k in list(nx.connected_components(soma_touching_graph))]\n",
    "            soma_to_piece_touching_vertices[i][piece_idx] = soma_con_comp\n",
    "            \"\"\"\n",
    "            \n",
    "            soma_to_piece_touching_vertices[i][piece_idx] = tu.split_vertex_list_into_connected_components(\n",
    "                                                vertex_indices_list=connected_mesh_pieces_vertices_idx[piece_index],\n",
    "                                                mesh=current_mesh, \n",
    "                                                vertex_graph=m_vert_graph, \n",
    "                                                return_coordinates=True\n",
    "                                               )\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "#         border_debug = False\n",
    "#         if border_debug:\n",
    "#             print(f\"soma_to_piece_connectivity = {soma_to_piece_connectivity}\")\n",
    "#             print(f\"soma_to_piece_touching_vertices = {soma_to_piece_touching_vertices}\")\n",
    "\n",
    "\n",
    "    print(f\"Total time for mesh_pieces_connectivity= {time.time() - current_time}\")\n",
    "\n",
    "    soma_touching_mesh_data[z][\"soma_to_piece_connectivity\"] = soma_to_piece_connectivity\n",
    "\n",
    "print(f\"# of insignificant_limbs = {len(insignificant_limbs)} with trimesh : {insignificant_limbs}\")\n",
    "\n",
    "\n",
    "\n",
    "# Lets have an alert if there was more than one soma disconnected meshes\n",
    "if len(soma_touching_mesh_data.keys()) > 1:\n",
    "    raise Exception(\"More than 1 disconnected meshes that contain somas\")\n",
    "\n",
    "current_mesh_data = soma_touching_mesh_data\n",
    "soma_containing_idx = 0\n",
    "\n",
    "#doing inversion of the connectivity and touching vertices\n",
    "piece_to_soma_touching_vertices = gu.flip_key_orders_for_dict(soma_to_piece_touching_vertices)\n",
    "\n",
    "\n",
    "# ****Soma Touching mesh Data has the branches and the connectivity (So this is where you end up skipping if you don't have somas)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process that will start for each limb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----- Working on Proper Limb # 0 ---------\n",
      "Time for preparing soma vertices and root: 1.2636184692382812e-05\n",
      "cc_vertex_thresh = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfb33ad8c494e3585d25f9f6e3bdb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=31419.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for 1st pass MP skeletonization: 0.6433067321777344\n",
      "connecting at the root\n",
      "branches_touching_root = [14]\n",
      "length of Graph = 1117\n",
      "max(kept_branches_idx) = 14, len(kept_branches_idx) = 15\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.02977511244377811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19a063898334fd889f5cd3ed7d9f698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cbb3c23b1e41ea9ab5bdb318aaaebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decomposing first pass: 1.4453108310699463\n",
      "Attempting to use MeshAfterParty Skeletonization and Mesh Correspondence\n",
      "Another print\n",
      "Divinding into MP and MAP pieces: 6.437301635742188e-05\n",
      "Total time for MAP sublimb processing 2.86102294921875e-06\n",
      "---- Working on MP Decomposition #0 ----\n",
      "Using Quicker soma_to_piece_touching_vertices because no MAP and only one sublimb_mesh piece \n",
      "MP filtering soma verts: 5.2928924560546875e-05\n",
      "Fixing Possible Soma Extension Branch for Sublimb 0\n",
      "Total time for mesh KDTree = 0.3566622734069824\n",
      "sbv[0].reshape(-1,3) = [[1258379.   799888.   989992.9]]\n",
      "closest_sk_pt_coord BEFORE = [1259682.12401294  799081.64086113  986567.81558316]\n",
      "current_skeleton.shape = (1116, 2, 3)\n",
      "Current stitch point was not a branch or endpoint, shortest_path_length to one = 1299.8458944601148\n",
      "Changing the stitch point becasue the distance to end or branch node was 1299.8458944601148\n",
      "New stitch point has degree 1\n",
      "change_status for create soma extending pieces = True\n",
      "closest_sk_pt_coord AFTER = [1258490.5794517   798666.61569431  986279.60434233]\n",
      "skipping soma 0 because closest skeleton node was already end node\n",
      "sbv[0].reshape(-1,3) = [[1285439.   791562.1 1007778. ]]\n",
      "closest_sk_pt_coord BEFORE = [1284194.79560791  793905.42551066 1006316.58095651]\n",
      "current_skeleton.shape = (1116, 2, 3)\n",
      "Current stitch point was not a branch or endpoint, shortest_path_length to one = 5899.4150065368085\n",
      "change_status for create soma extending pieces = False\n",
      "closest_sk_pt_coord AFTER = [1284194.79560791  793905.42551066 1006316.58095651]\n",
      "Adding new branch to skeleton\n",
      "border_average_coordinate = [1284925.60416667  792732.54583333 1008043.375     ]\n",
      "endpoints_must_keep = {0: array([[1258490.5794517 ,  798666.61569431,  986279.60434233]]), 1: array([[1284925.60416667,  792732.54583333, 1008043.375     ]])}\n",
      "MP (because soma touching verts) create_soma_extending_branches: 0.844430685043335\n",
      "orig_vertex = [1284194.79560791  793905.42551066 1006316.58095651]\n",
      "match_sk_branches = [5]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bbea40cb9e49fa996306d96bf6940b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--- Working on 1-to-1 correspondence-----\n",
      "max(original_labels),len(original_labels) = (2, 3)\n",
      "empty_indices % = 0.05250965250965251\n",
      " conflict_indices % = 0.26975546975546977\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8946e85fe07c4fc3aafa4e3d1dbf160a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n",
      "Took 0 iterations to expand the label back\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133d9770525143cd9b061c8df61fff08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9b599cb0254f0b8c3a38f198e0e835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "curr_width_median = 179.73534412291542\n",
      "curr_width_median = 393.3486822690925\n",
      "curr_width_median = 282.6155655436335\n",
      "checked segment branches after soma add on\n",
      "MP (because soma touching verts) soma extension add: 0.7569689750671387\n",
      "There were not both MAP and MP pieces so skipping the stitch resolving phase\n",
      "Time for decomp of Limb = 3.6904990673065186\n",
      "\n",
      "\n",
      "----- Working on Proper Limb # 1 ---------\n",
      "Time for preparing soma vertices and root: 7.62939453125e-06\n",
      "cc_vertex_thresh = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fb90b731fb401188c93a08f371da10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5170.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for 1st pass MP skeletonization: 0.14141249656677246\n",
      "connecting at the root\n",
      "branches_touching_root = [0]\n",
      "length of Graph = 110\n",
      "max(kept_branches_idx) = 0, len(kept_branches_idx) = 1\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7ddc38c3b04f96a586962b61c878b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b54ec98a7f5428a9c9594ae5d004c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decomposing first pass: 0.2119460105895996\n",
      "Attempting to use MeshAfterParty Skeletonization and Mesh Correspondence\n",
      "Another print\n",
      "Divinding into MP and MAP pieces: 6.079673767089844e-05\n",
      "Total time for MAP sublimb processing 2.86102294921875e-06\n",
      "---- Working on MP Decomposition #0 ----\n",
      "Using Quicker soma_to_piece_touching_vertices because no MAP and only one sublimb_mesh piece \n",
      "MP filtering soma verts: 6.508827209472656e-05\n",
      "Fixing Possible Soma Extension Branch for Sublimb 0\n",
      "Total time for mesh KDTree = 0.05324196815490723\n",
      "sbv[0].reshape(-1,3) = [[1283541.   788733.4 1011086. ]]\n",
      "closest_sk_pt_coord BEFORE = [1282152.75805681  789395.08520355 1011977.1396222 ]\n",
      "current_skeleton.shape = (109, 2, 3)\n",
      "Current stitch point was not a branch or endpoint, shortest_path_length to one = 4799.305420143581\n",
      "change_status for create soma extending pieces = False\n",
      "closest_sk_pt_coord AFTER = [1282152.75805681  789395.08520355 1011977.1396222 ]\n",
      "Adding new branch to skeleton\n",
      "border_average_coordinate = [1284060.63793103  788160.40689655 1012104.72413793]\n",
      "sbv[0].reshape(-1,3) = [[1286509.   790761.4 1013819. ]]\n",
      "closest_sk_pt_coord BEFORE = [1285893.02413496  791081.97020098 1014502.89558998]\n",
      "current_skeleton.shape = (110, 2, 3)\n",
      "Current stitch point was not a branch or endpoint, shortest_path_length to one = 414.61257967178636\n",
      "Changing the stitch point becasue the distance to end or branch node was 414.61257967178636\n",
      "New stitch point has degree 1\n",
      "change_status for create soma extending pieces = True\n",
      "closest_sk_pt_coord AFTER = [1286176.32233104  790783.9497128  1014502.11270705]\n",
      "skipping soma 1 because closest skeleton node was already end node\n",
      "sbv[0].reshape(-1,3) = [[1283352.  789999. 1011297.]]\n",
      "closest_sk_pt_coord BEFORE = [1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      "current_skeleton.shape = (110, 2, 3)\n",
      "Current stitch point was not a branch or endpoint, shortest_path_length to one = 2676.0629935752477\n",
      "change_status for create soma extending pieces = False\n",
      "closest_sk_pt_coord AFTER = [1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      "Adding new branch to skeleton\n",
      "border_average_coordinate = [1283328.2   790319.53 1011364.65]\n",
      "sbv[0].reshape(-1,3) = [[1283294.   789637.4 1011175. ]]\n",
      "closest_sk_pt_coord BEFORE = [1283328.2   790319.53 1011364.65]\n",
      "current_skeleton.shape = (111, 2, 3)\n",
      "Current stitch point was a branch or endpoint\n",
      "change_status for create soma extending pieces = False\n",
      "closest_sk_pt_coord AFTER = [1283328.2   790319.53 1011364.65]\n",
      "skipping soma 1 because closest skeleton node was already end node\n",
      "endpoints_must_keep = {1: array([[1284060.63793103,  788160.40689655, 1012104.72413793],\n",
      "       [1286176.32233104,  790783.9497128 , 1014502.11270705],\n",
      "       [1283328.2       ,  790319.53      , 1011364.65      ],\n",
      "       [1283328.2       ,  790319.53      , 1011364.65      ]])}\n",
      "MP (because soma touching verts) create_soma_extending_branches: 0.3682365417480469\n",
      "orig_vertex = [1282152.75805681  789395.08520355 1011977.1396222 ]\n",
      "match_sk_branches = [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2f6adc3eaa4cbaa2c3b96f090b2f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--- Working on 1-to-1 correspondence-----\n",
      "max(original_labels),len(original_labels) = (2, 3)\n",
      "empty_indices % = 0.3232089480281554\n",
      " conflict_indices % = 0.17944267669462927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd894b8f30ce47049675b8039ce6396d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n",
      "Took 19 iterations to expand the label back\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3392d838de6645dba034398a88b0a47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc86e063897479baad83c0c70763bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "curr_width_median = 186.78649637826263\n",
      "curr_width_median = 202.53942187517265\n",
      "curr_width_median = 297.0480439177468\n",
      "checked segment branches after soma add on\n",
      "orig_vertex = [1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      "match_sk_branches = [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6743573bbf415b94c44af2568f650e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--- Working on 1-to-1 correspondence-----\n",
      "max(original_labels),len(original_labels) = (2, 3)\n",
      "empty_indices % = 0.5089974293059126\n",
      " conflict_indices % = 0.07969151670951156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429a8bbff5634a4c8064bea613ae3ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n",
      "Took 1 iterations to expand the label back\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc77b1ec44c4c32ac7cfca595ba8643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e819ee6503e9440184b697c7894379a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "curr_width_median = 234.49402376267165\n",
      "curr_width_median = 197.79403995527989\n",
      "curr_width_median = 302.5253011179024\n",
      "checked segment branches after soma add on\n",
      "MP (because soma touching verts) soma extension add: 2.663644313812256\n",
      "There were not both MAP and MP pieces so skipping the stitch resolving phase\n",
      "Time for decomp of Limb = 3.3857522010803223\n",
      "Total time for Skeletonization and Mesh Correspondence = 7.076906681060791\n",
      "Total time for stitching floating pieces = 0.00011134147644042969\n",
      "Starting_edge inside branches_to_conept = [[1258490.5794517   798666.61569431  986279.60434233]\n",
      " [1281187.37809427  792659.11311247  991031.06609014]]\n",
      "At the start, starting_node (in terms of the skeleton, that shouldn't match the starting edge) = [0]\n",
      "printing out current edge:\n",
      "[[1258490.5794517   798666.61569431  986279.60434233]\n",
      " [1281187.37809427  792659.11311247  991031.06609014]]\n",
      "edge_endpoints_to_process was empty so exiting loop after 17 iterations\n",
      "starting_node in concept map (that should match the starting edge) = 13\n",
      "Total time for branches to concept conversion = 0.11651396751403809\n",
      "\n",
      "Done generating concept network \n",
      "\n",
      "\n",
      "recovered_touching_piece = [13]\n",
      "Starting_edge inside branches_to_conept = [[1284194.79560791  793905.42551066 1006316.58095651]\n",
      " [1284925.60416667  792732.54583333 1008043.375     ]]\n",
      "At the start, starting_node (in terms of the skeleton, that shouldn't match the starting edge) = [12]\n",
      "printing out current edge:\n",
      "[[1284925.60416667  792732.54583333 1008043.375     ]\n",
      " [1284194.79560791  793905.42551066 1006316.58095651]]\n",
      "edge_endpoints_to_process was empty so exiting loop after 17 iterations\n",
      "starting_node in concept map (that should match the starting edge) = 16\n",
      "Total time for branches to concept conversion = 0.09605956077575684\n",
      "\n",
      "Done generating concept network \n",
      "\n",
      "\n",
      "recovered_touching_piece = [16]\n",
      "Starting_edge inside branches_to_conept = [[1282152.75805681  789395.08520355 1011977.1396222 ]\n",
      " [1284060.63793103  788160.40689655 1012104.72413793]]\n",
      "At the start, starting_node (in terms of the skeleton, that shouldn't match the starting edge) = [4]\n",
      "printing out current edge:\n",
      "[[1284060.63793103  788160.40689655 1012104.72413793]\n",
      " [1282152.75805681  789395.08520355 1011977.1396222 ]]\n",
      "edge_endpoints_to_process was empty so exiting loop after 5 iterations\n",
      "starting_node in concept map (that should match the starting edge) = 1\n",
      "Total time for branches to concept conversion = 0.014457225799560547\n",
      "\n",
      "Done generating concept network \n",
      "\n",
      "\n",
      "recovered_touching_piece = [1]\n",
      "Starting_edge inside branches_to_conept = [[1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      " [1286176.32233104  790783.9497128  1014502.11270705]]\n",
      "At the start, starting_node (in terms of the skeleton, that shouldn't match the starting edge) = [5]\n",
      "printing out current edge:\n",
      "[[1286176.32233104  790783.9497128  1014502.11270705]\n",
      " [1282288.50847186  789605.39727109 1012288.8885627 ]]\n",
      "edge_endpoints_to_process was empty so exiting loop after 5 iterations\n",
      "starting_node in concept map (that should match the starting edge) = 3\n",
      "Total time for branches to concept conversion = 0.01388859748840332\n",
      "\n",
      "Done generating concept network \n",
      "\n",
      "\n",
      "recovered_touching_piece = [3]\n",
      "Starting_edge inside branches_to_conept = [[1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      " [1283328.2         790319.53       1011364.65      ]]\n",
      "At the start, starting_node (in terms of the skeleton, that shouldn't match the starting edge) = [3]\n",
      "printing out current edge:\n",
      "[[1283328.2         790319.53       1011364.65      ]\n",
      " [1282288.50847186  789605.39727109 1012288.8885627 ]]\n",
      "edge_endpoints_to_process was empty so exiting loop after 5 iterations\n",
      "starting_node in concept map (that should match the starting edge) = 4\n",
      "Total time for branches to concept conversion = 0.015752315521240234\n",
      "\n",
      "Done generating concept network \n",
      "\n",
      "\n",
      "recovered_touching_piece = [4]\n",
      "Starting_edge inside branches_to_conept = [[1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      " [1283328.2         790319.53       1011364.65      ]]\n",
      "At the start, starting_node (in terms of the skeleton, that shouldn't match the starting edge) = [3]\n",
      "printing out current edge:\n",
      "[[1283328.2         790319.53       1011364.65      ]\n",
      " [1282288.50847186  789605.39727109 1012288.8885627 ]]\n",
      "edge_endpoints_to_process was empty so exiting loop after 5 iterations\n",
      "starting_node in concept map (that should match the starting edge) = 4\n",
      "Total time for branches to concept conversion = 0.015377998352050781\n",
      "\n",
      "Done generating concept network \n",
      "\n",
      "\n",
      "recovered_touching_piece = [4]\n",
      "Total time for Concept Networks = 0.42244958877563477\n",
      "Total time for all limb decomps = 7.502451658248901\n",
      "Already have preprocessed data\n",
      "--- 1) Finished unpacking preprocessed materials: 2.5510787963867188e-05\n",
      "total_edges = [['S0', 'L0'], ['S1', 'L0'], ['S1', 'L1']]\n",
      "--- 2) Finished creating neuron connectivity graph: 8.440017700195312e-05\n",
      "Having to generate soma_meshes_face_idx because none in preprocessed data\n",
      "--- 3a) Finshed generating soma_meshes_face_idx: 0.053128957748413086\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 5803 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_939214.off -o /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_939214_poisson.off -s /notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_106957.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_939214.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_939214_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_106957.mls is being deleted....\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8101 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_820890.off -o /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_820890_poisson.off -s /notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_767526.mls\n",
      "removed temporary input file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_820890.off\n",
      "removed temporary output file: /notebooks/Platinum_Decomp_Fusion/Poisson_temp/neuron_820890_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Platinum_Decomp_Fusion/Poisson_temp/poisson_767526.mls is being deleted....\n",
      "--- 3) Finshed generating soma objects and adding them to concept graph: 12.029300689697266\n",
      "--- 4a) Finshed generating curr_limb_meshes_face_idx: 0.05119585990905762\n",
      "curr_limb_concept_networks= {0: [<networkx_utils.GraphOrderedEdges object at 0x7f439b587630>], 1: [<networkx_utils.GraphOrderedEdges object at 0x7f439b5870b8>]}\n",
      "concept_network_dict = {0: [<networkx_utils.GraphOrderedEdges object at 0x7f439b587630>], 1: [<networkx_utils.GraphOrderedEdges object at 0x7f439b5870b8>]}\n",
      "checking and resolving cycles\n",
      "No cycles to fix\n",
      "curr_limb_concept_networks= {1: [<networkx_utils.GraphOrderedEdges object at 0x7f439b587b38>, <networkx_utils.GraphOrderedEdges object at 0x7f439b587400>, <networkx_utils.GraphOrderedEdges object at 0x7f439b587f28>, <networkx_utils.GraphOrderedEdges object at 0x7f439b5878d0>]}\n",
      "concept_network_dict = {1: [<networkx_utils.GraphOrderedEdges object at 0x7f439b587b38>, <networkx_utils.GraphOrderedEdges object at 0x7f439b587400>, <networkx_utils.GraphOrderedEdges object at 0x7f439b587f28>, <networkx_utils.GraphOrderedEdges object at 0x7f439b5878d0>]}\n",
      "checking and resolving cycles\n",
      "No cycles to fix\n",
      "--- 4) Finshed generating Limb objects and adding them to concept graph: 0.06631183624267578\n",
      "--- 5) SKIPPING Doing the adaptive mesh correspondence on the meshparty preprocessing ---\n",
      "--- 6) SKIPPING Using the computed_attribute_dict to populate neuron attributes ---\n",
      "7) Calculating the spines for the neuorn if do not already exist\n",
      "7a) calculating spines because didn't exist\n",
      "query = median_mesh_center > 200 and n_faces_branch>100\n",
      "smoothness_threshold = 0.08\n",
      "The median_mesh_center was requested but has not already been calculated so calculating now.... \n",
      "Working on limb L0 branch 13\n",
      "Working on limb L0 branch 4\n",
      "Working on limb L0 branch 12\n",
      "Working on limb L0 branch 3\n",
      "Working on limb L0 branch 6\n",
      "Working on limb L0 branch 2\n",
      "Working on limb L0 branch 9\n",
      "Working on limb L0 branch 5\n",
      "Working on limb L0 branch 11\n",
      "Working on limb L0 branch 1\n",
      "Working on limb L0 branch 15\n",
      "Working on limb L0 branch 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on limb L0 branch 10\n",
      "Working on limb L0 branch 0\n",
      "Working on limb L0 branch 7\n",
      "Working on limb L0 branch 14\n",
      "Working on limb L0 branch 16\n",
      "Working on limb L1 branch 1\n",
      "Assigning the old width calculation because no valid new widths\n",
      "Working on limb L1 branch 0\n",
      "Working on limb L1 branch 2\n",
      "Working on limb L1 branch 3\n",
      "Working on limb L1 branch 4\n",
      "functions_list = [<function median_mesh_center at 0x7f43bad6c488>, <function n_faces_branch at 0x7f43bad6c048>]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419cd6f1fdf74587a4169e8b060c56ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on limb L0 branch 13\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 4\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 12\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 3\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 6\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 2\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 9\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 5\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 11\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 1\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 15\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 8\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 10\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 0\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 7\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 14\n",
      "No spines and using precomputed width\n",
      "Working on limb L0 branch 16\n",
      "No spines and using precomputed width\n",
      "Working on limb L1 branch 1\n",
      "No spines and using precomputed width\n",
      "Working on limb L1 branch 0\n",
      "No spines and using precomputed width\n",
      "Working on limb L1 branch 2\n",
      "No spines and using precomputed width\n",
      "Working on limb L1 branch 3\n",
      "No spines and using precomputed width\n",
      "Working on limb L1 branch 4\n",
      "No spines and using precomputed width\n",
      "Total time for neuron instance creation = 13.818000078201294\n"
     ]
    }
   ],
   "source": [
    "sk = reload(sk)\n",
    "tu = reload(tu)\n",
    "cu = reload(cu)\n",
    "m_sk = reload(m_sk)\n",
    "pre = reload(pre)\n",
    "xu = reload(xu)\n",
    "nu = reload(nu)\n",
    "gu = reload(gu)\n",
    "\n",
    "try: \n",
    "    proper_time = time.time()\n",
    "\n",
    "    #The containers that will hold the final data for the preprocessed neuron\n",
    "    limb_correspondence=dict()\n",
    "    limb_network_stating_info = dict()\n",
    "\n",
    "    # ---------- Part A: skeletonization and mesh decomposition --------- #\n",
    "    skeleton_time = time.time()\n",
    "\n",
    "    for curr_limb_idx,limb_mesh_mparty in enumerate(current_mesh_data[0][\"branch_meshes\"]):\n",
    "\n",
    "        #Arguments to pass to the specific function (when working with a limb)\n",
    "        soma_touching_vertices_dict = piece_to_soma_touching_vertices[curr_limb_idx]\n",
    "\n",
    "    #     if curr_limb_idx != 10:\n",
    "    #         continue\n",
    "\n",
    "        curr_limb_time = time.time()\n",
    "        print(f\"\\n\\n----- Working on Proper Limb # {curr_limb_idx} ---------\")\n",
    "\n",
    "\n",
    "        limb_correspondence_individual,network_starting_info = pre.preprocess_limb(mesh=limb_mesh_mparty,\n",
    "                       soma_touching_vertices_dict = soma_touching_vertices_dict,\n",
    "                       return_concept_network = False, \n",
    "                       return_concept_network_starting_info=True,\n",
    "                       width_threshold_MAP=500,\n",
    "                       size_threshold_MAP=2000,\n",
    "                       surface_reconstruction_size=1000,                                                            \n",
    "                       )\n",
    "        #Storing all of the data to be sent to \n",
    "\n",
    "        limb_correspondence[curr_limb_idx] = limb_correspondence_individual\n",
    "        limb_network_stating_info[curr_limb_idx] = network_starting_info\n",
    "\n",
    "    #     raise Exception(\"Done with #10\")\n",
    "\n",
    "\n",
    "    print(f\"Total time for Skeletonization and Mesh Correspondence = {time.time() - skeleton_time}\")\n",
    "\n",
    "    pre=reload(pre)\n",
    "    neuron=reload(neuron)\n",
    "    nru = reload(nru)\n",
    "    # ---------- Part B: Stitching on floating pieces --------- #\n",
    "    floating_stitching_time = time.time()\n",
    "\n",
    "    limb_correspondence_with_floating_pieces = pre.attach_floating_pieces_to_limb_correspondence(\n",
    "            limb_correspondence,\n",
    "            floating_meshes=non_soma_touching_meshes,\n",
    "            floating_piece_face_threshold = 600,\n",
    "            max_stitch_distance=8000,\n",
    "            distance_to_move_point_threshold = 4000,\n",
    "            verbose = False)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Total time for stitching floating pieces = {time.time() - floating_stitching_time}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- Part C: Computing Concept Networks --------- #\n",
    "    concept_network_time = time.time()\n",
    "\n",
    "    limb_concept_networks=dict()\n",
    "    limb_labels=dict()\n",
    "\n",
    "    for curr_limb_idx,limb_mesh_mparty in enumerate(current_mesh_data[0][\"branch_meshes\"]):\n",
    "        limb_to_soma_concept_networks = pre.calculate_limb_concept_networks(limb_correspondence_with_floating_pieces[curr_limb_idx],\n",
    "                                                                            run_concept_network_checks=True,\n",
    "                                                                           **limb_network_stating_info[curr_limb_idx])   \n",
    "\n",
    "\n",
    "\n",
    "        limb_concept_networks[curr_limb_idx] = limb_to_soma_concept_networks\n",
    "        limb_labels[curr_limb_idx]= \"Unlabeled\"\n",
    "\n",
    "    print(f\"Total time for Concept Networks = {time.time() - concept_network_time}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    preprocessed_data= dict(\n",
    "        soma_meshes = current_mesh_data[0][\"soma_meshes\"],\n",
    "        soma_to_piece_connectivity = current_mesh_data[0][\"soma_to_piece_connectivity\"],\n",
    "        soma_sdfs = total_soma_list_sdf,\n",
    "        insignificant_limbs=insignificant_limbs,\n",
    "        non_soma_touching_meshes=non_soma_touching_meshes,\n",
    "        inside_pieces=inside_pieces,\n",
    "        limb_correspondence=limb_correspondence_with_floating_pieces,\n",
    "        limb_concept_networks=limb_concept_networks,\n",
    "        limb_network_stating_info=limb_network_stating_info,\n",
    "        limb_labels=limb_labels,\n",
    "        limb_meshes=current_mesh_data[0][\"branch_meshes\"],\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Total time for all limb decomps = {time.time() - proper_time}\")\n",
    "\n",
    "    #864049.29563888 1004924.982468    884750.28428994\n",
    "\n",
    "    neuron_obj = neuron.Neuron(\n",
    "                    mesh=current_neuron,\n",
    "                     segment_id=segment_id,\n",
    "                     description=description,\n",
    "                     preprocessed_data=preprocessed_data,\n",
    "    #                  decomposition_type=\"meshafterparty\",\n",
    "    #                  mesh_correspondence=\"meshparty\", #meshafterparty_adaptive\n",
    "    #                  distance_by_mesh_center=True, #how the distance is calculated for mesh correspondence\n",
    "    #                  meshparty_segment_size = 0,\n",
    "    #                  meshparty_n_surface_downsampling = 0,\n",
    "    #                  meshparty_adaptive_correspondence_after_creation=False,\n",
    "    #                 suppress_preprocessing_print=True,\n",
    "    #                  computed_attribute_dict=None,\n",
    "    #                  somas = None,\n",
    "    #                  branch_skeleton_data=None,\n",
    "    #                  combine_close_skeleton_nodes = True,\n",
    "    #                 combine_close_skeleton_nodes_threshold=700,\n",
    "\n",
    "\n",
    "                    ignore_warnings=True,\n",
    "                    suppress_output=False,\n",
    "                    calculate_spines=True,\n",
    "                    widths_to_calculate=[\"no_spine_median_mesh_center\"]\n",
    "                    )\n",
    "except gu.CGAL_skel_error as error: \n",
    "    print(\"The CGAL error was encountered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Working on visualization type: mesh\n",
      "\n",
      " Working on visualization type: skeleton\n",
      "working on soma border vertices\n",
      "Working on  new stand alone scatter points\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4df09f1b7b422b9cf9c29ceb25a65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nviz.visualize_neuron(neuron_obj,\n",
    "                     visualize_type=[\"mesh\",\"skeleton\"],\n",
    "                     limb_branch_dict=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the limb decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for preparing soma vertices and root: 0.00010514259338378906\n",
      "cc_vertex_thresh = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84ffec5b9a346d582299f3a10c0f397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5170.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for 1st pass MP skeletonization: 0.14657306671142578\n",
      "connecting at the root\n",
      "branches_touching_root = [0]\n",
      "length of Graph = 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/meshAfterParty/meshparty_skeletonize.py:895: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  segment_branches = np.array([sk_meshparty_obj.vertices[np.vstack([k[:-1],k[1:]]).T] for k in segments])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(kept_branches_idx) = 0, len(kept_branches_idx) = 1\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e12f53e3f145b3b5647eda50e87c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c5a92ab940483db4024ba1440a2423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decomposing first pass: 0.21408605575561523\n",
      "Attempting to use MeshAfterParty Skeletonization and Mesh Correspondence\n",
      "Another print\n",
      "Divinding into MP and MAP pieces: 0.0009737014770507812\n",
      "Total time for MAP sublimb processing 0.0004305839538574219\n",
      "---- Working on MP Decomposition #0 ----\n",
      "Using Quicker soma_to_piece_touching_vertices because no MAP and only one sublimb_mesh piece \n",
      "MP filtering soma verts: 0.001611471176147461\n",
      "Fixing Possible Soma Extension Branch for Sublimb 0\n",
      "Total time for mesh KDTree = 0.052979230880737305\n",
      "sbv[0].reshape(-1,3) = [[1283541.   788733.4 1011086. ]]\n",
      "closest_sk_pt_coord BEFORE = [1282152.75805681  789395.08520355 1011977.1396222 ]\n",
      "current_skeleton.shape = (109, 2, 3)\n",
      "Current stitch point was not a branch or endpoint, shortest_path_length to one = 4799.305420143581\n",
      "change_status for create soma extending pieces = False\n",
      "closest_sk_pt_coord AFTER = [1282152.75805681  789395.08520355 1011977.1396222 ]\n",
      "Adding new branch to skeleton\n",
      "border_average_coordinate = [1284060.63793103  788160.40689655 1012104.72413793]\n",
      "sbv[0].reshape(-1,3) = [[1286509.   790761.4 1013819. ]]\n",
      "closest_sk_pt_coord BEFORE = [1285893.02413496  791081.97020098 1014502.89558998]\n",
      "current_skeleton.shape = (110, 2, 3)\n",
      "Current stitch point was not a branch or endpoint, shortest_path_length to one = 414.61257967178636\n",
      "Changing the stitch point becasue the distance to end or branch node was 414.61257967178636\n",
      "New stitch point has degree 1\n",
      "change_status for create soma extending pieces = True\n",
      "closest_sk_pt_coord AFTER = [1286176.32233104  790783.9497128  1014502.11270705]\n",
      "skipping soma 1 because closest skeleton node was already end node\n",
      "sbv[0].reshape(-1,3) = [[1283352.  789999. 1011297.]]\n",
      "closest_sk_pt_coord BEFORE = [1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      "current_skeleton.shape = (110, 2, 3)\n",
      "Current stitch point was not a branch or endpoint, shortest_path_length to one = 2676.0629935752477\n",
      "change_status for create soma extending pieces = False\n",
      "closest_sk_pt_coord AFTER = [1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      "Adding new branch to skeleton\n",
      "border_average_coordinate = [1283328.2   790319.53 1011364.65]\n",
      "sbv[0].reshape(-1,3) = [[1283294.   789637.4 1011175. ]]\n",
      "closest_sk_pt_coord BEFORE = [1283328.2   790319.53 1011364.65]\n",
      "current_skeleton.shape = (111, 2, 3)\n",
      "Current stitch point was a branch or endpoint\n",
      "change_status for create soma extending pieces = False\n",
      "closest_sk_pt_coord AFTER = [1283328.2   790319.53 1011364.65]\n",
      "skipping soma 1 because closest skeleton node was already end node\n",
      "endpoints_must_keep = {1: array([[1284060.63793103,  788160.40689655, 1012104.72413793],\n",
      "       [1286176.32233104,  790783.9497128 , 1014502.11270705],\n",
      "       [1283328.2       ,  790319.53      , 1011364.65      ],\n",
      "       [1283328.2       ,  790319.53      , 1011364.65      ]])}\n",
      "MP (because soma touching verts) create_soma_extending_branches: 0.39687156677246094\n",
      "new_branch_info = {1: [{'new_branch': array([[[1282152.75805681,  789395.08520355, 1011977.1396222 ],\n",
      "        [1284060.63793103,  788160.40689655, 1012104.72413793]]]), 'border_verts': TrackedArray([[1283541. ,  788733.4, 1011086. ],\n",
      "              [1284102. ,  787829.5, 1011834. ],\n",
      "              [1283604. ,  788487. , 1011213. ],\n",
      "              [1283474. ,  788941.9, 1011262. ],\n",
      "              [1283360. ,  789030.4, 1011141. ],\n",
      "              [1284862. ,  787667.8, 1013174. ],\n",
      "              [1284951. ,  787755.1, 1013175. ],\n",
      "              [1283666. ,  788451.6, 1011539. ],\n",
      "              [1283690. ,  788095.1, 1011736. ],\n",
      "              [1283669. ,  788655.3, 1011438. ],\n",
      "              [1284954. ,  787688.9, 1013399. ],\n",
      "              [1284501. ,  788019.2, 1012964. ],\n",
      "              [1283645. ,  788346.2, 1011339. ],\n",
      "              [1284228. ,  788050.1, 1012381. ],\n",
      "              [1283986. ,  788042. , 1012311. ],\n",
      "              [1284748. ,  788088. , 1013302. ],\n",
      "              [1284497. ,  787673.5, 1012413. ],\n",
      "              [1283763. ,  788144.8, 1011517. ],\n",
      "              [1284255. ,  788102.2, 1012623. ],\n",
      "              [1285045. ,  787759.8, 1013532. ],\n",
      "              [1284806. ,  787817.8, 1013410. ],\n",
      "              [1284977. ,  787940.8, 1013394. ],\n",
      "              [1284706. ,  787668. , 1012977. ],\n",
      "              [1284348. ,  788017.6, 1012850. ],\n",
      "              [1283777. ,  788051.5, 1011917. ],\n",
      "              [1283693. ,  788586.8, 1011150. ],\n",
      "              [1283772. ,  788067. , 1011549. ],\n",
      "              [1284549. ,  788017.2, 1013104. ],\n",
      "              [1285077. ,  787912.1, 1013526. ],\n",
      "              [1283234. ,  789153.1, 1011119. ],\n",
      "              [1283394. ,  788886. , 1011024. ],\n",
      "              [1283812. ,  787776.2, 1011846. ],\n",
      "              [1283772. ,  787815. , 1011738. ],\n",
      "              [1283634. ,  787923.8, 1011666. ],\n",
      "              [1283654. ,  788214.8, 1011422. ],\n",
      "              [1283364. ,  788995.8, 1011017. ],\n",
      "              [1284120. ,  788097.9, 1012177. ],\n",
      "              [1283361. ,  788849.1, 1011067. ],\n",
      "              [1284603. ,  787627.3, 1012840. ],\n",
      "              [1284948. ,  787773. , 1013502. ],\n",
      "              [1284526. ,  787619.2, 1012686. ],\n",
      "              [1284244. ,  787797.2, 1012011. ],\n",
      "              [1283346. ,  788900.2, 1011244. ],\n",
      "              [1283712. ,  788214.8, 1011618. ],\n",
      "              [1283205. ,  789138. , 1010961. ],\n",
      "              [1283768. ,  787947.4, 1011677. ],\n",
      "              [1283635. ,  788076.1, 1011535. ],\n",
      "              [1284420. ,  787731.4, 1012302. ],\n",
      "              [1283656. ,  788298. , 1011549. ],\n",
      "              [1284822. ,  787993.5, 1013355. ],\n",
      "              [1284124. ,  787979.4, 1012579. ],\n",
      "              [1283497. ,  788579.1, 1011467. ],\n",
      "              [1283955. ,  788054.7, 1012001. ],\n",
      "              [1284063. ,  787773.9, 1011988. ],\n",
      "              [1283456. ,  788763.5, 1011403. ],\n",
      "              [1283922. ,  787882.9, 1011718. ],\n",
      "              [1284619. ,  787997.8, 1013172. ],\n",
      "              [1284405. ,  787803.9, 1012134. ]])}, None, {'new_branch': array([[[1282288.50847186,  789605.39727109, 1012288.8885627 ],\n",
      "        [1283328.2       ,  790319.53      , 1011364.65      ]]]), 'border_verts': TrackedArray([[1283352. ,  789999. , 1011297. ],\n",
      "              [1283328. ,  789939.9, 1011177. ],\n",
      "              [1283147. ,  790032.9, 1011089. ],\n",
      "              [1283394. ,  790282.5, 1011434. ],\n",
      "              [1283090. ,  790124.2, 1011245. ],\n",
      "              [1283226. ,  789999. , 1010972. ],\n",
      "              [1283300. ,  790261.5, 1011570. ],\n",
      "              [1283436. ,  790792.4, 1011595. ],\n",
      "              [1283224. ,  790463. , 1011469. ],\n",
      "              [1283330. ,  790611.9, 1011561. ],\n",
      "              [1283457. ,  790671. , 1011591. ],\n",
      "              [1283284. ,  789927. , 1010947. ],\n",
      "              [1283576. ,  790635.1, 1011537. ],\n",
      "              [1283356. ,  790643.2, 1011423. ],\n",
      "              [1283558. ,  790888.7, 1011562. ],\n",
      "              [1283350. ,  790098.4, 1011412. ],\n",
      "              [1283458. ,  790540.9, 1011545. ],\n",
      "              [1283184. ,  789820.5, 1010961. ],\n",
      "              [1283380. ,  790401.3, 1011509. ],\n",
      "              [1283134. ,  790258.2, 1011397. ]])}, None]}\n",
      "sm_idx = 1\n",
      "orig_vertex = [1282152.75805681  789395.08520355 1011977.1396222 ]\n",
      "match_sk_branches = [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad96c698ca9a4c4d9067fca9508e6a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:318: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--- Working on 1-to-1 correspondence-----\n",
      "max(original_labels),len(original_labels) = (2, 3)\n",
      "empty_indices % = 0.3232089480281554\n",
      " conflict_indices % = 0.17944267669462927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4ea6bf694a49c19419d972c6379da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n",
      "Took 19 iterations to expand the label back\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd6ea786b7c4429afb2dc7a78278119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5388e0c436364c108a97048e61556511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "curr_width_median = 186.5058490761475\n",
      "curr_width_median = 202.32564922662314\n",
      "curr_width_median = 295.3992209889738\n",
      "PASSED DIVIDED SUBMESH\n",
      "checked segment branches after soma add on\n",
      "\n",
      "\n",
      " No new branch created so continuing \n",
      "\n",
      "\n",
      "orig_vertex = [1282288.50847186  789605.39727109 1012288.8885627 ]\n",
      "match_sk_branches = [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:576: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:588: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a89c44323d04afa8c68cb7b4b11b73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--- Working on 1-to-1 correspondence-----\n",
      "max(original_labels),len(original_labels) = (2, 3)\n",
      "empty_indices % = 0.5099727364040751\n",
      " conflict_indices % = 0.07892093557181805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f6a225fdf244a08b0f994253cf72ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n",
      "Took 1 iterations to expand the label back\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12b7f31da574488aeceda35577c8f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13a9b6f450f42fab98b11a7bc3cfbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "curr_width_median = 237.96686662938578\n",
      "curr_width_median = 197.85630130658947\n",
      "curr_width_median = 306.0616296628673\n",
      "PASSED DIVIDED SUBMESH\n",
      "checked segment branches after soma add on\n",
      "\n",
      "\n",
      " No new branch created so continuing \n",
      "\n",
      "\n",
      "MP (because soma touching verts) soma extension add: 2.6748437881469727\n",
      "There were not both MAP and MP pieces so skipping the stitch resolving phase\n",
      "Time for decomp of Limb = 3.4381229877471924\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'limb_to_soma_concept_networks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-c9243a26a678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimb_correspondence_individual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimb_to_soma_concept_networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'limb_to_soma_concept_networks' is not defined"
     ]
    }
   ],
   "source": [
    "from preprocessing_vp2 import *\n",
    "\n",
    "mesh=limb_mesh_mparty\n",
    "soma_touching_vertices_dict = soma_touching_vertices_dict\n",
    "\n",
    "\n",
    "distance_by_mesh_center=True #how the distance is calculated for mesh correspondence\n",
    "meshparty_segment_size = 100\n",
    "meshparty_n_surface_downsampling = 2\n",
    "combine_close_skeleton_nodes=True\n",
    "combine_close_skeleton_nodes_threshold=700\n",
    "filter_end_node_length=4001\n",
    "use_meshafterparty=True\n",
    "perform_cleaning_checks = True\n",
    "\n",
    "#for controlling the pieces processed by MAP\n",
    "width_threshold_MAP = 450\n",
    "size_threshold_MAP = 1000\n",
    "\n",
    "#parameters for MP skeletonization,\n",
    "\n",
    "#Parameters for setting how the MAP skeletonization takes place\n",
    "use_surface_after_CGAL=False\n",
    "surface_reconstruction_size = 500\n",
    "\n",
    "#parametrers for stitching the MAP and MP pieces together\n",
    "move_MAP_stitch_to_end_or_branch = True\n",
    "distance_to_move_point_threshold=500\n",
    "\n",
    "#concept_network parameters\n",
    "run_concept_network_checks = True\n",
    "return_concept_network = True\n",
    "return_concept_network_starting_info=False\n",
    "\n",
    "#printing controls\n",
    "verbose = True\n",
    "print_fusion_steps=True\n",
    "\n",
    "\n",
    "#arguments\n",
    "return_concept_network = False\n",
    "return_concept_network_starting_info=True\n",
    "width_threshold_MAP=500\n",
    "size_threshold_MAP=2000\n",
    "surface_reconstruction_size=1000\n",
    "                    \n",
    "                   \n",
    "    \n",
    "curr_limb_time = time.time()\n",
    "\n",
    "limb_mesh_mparty = mesh\n",
    "\n",
    "\n",
    "#will store a list of all the endpoints tha tmust be kept:\n",
    "limb_to_endpoints_must_keep_list = []\n",
    "limb_to_soma_touching_vertices_list = []\n",
    "\n",
    "# --------------- Part 1 and 2: Getting Border Vertices and Setting the Root------------- #\n",
    "fusion_time = time.time()\n",
    "#will eventually get the current root from soma_to_piece_touching_vertices[i]\n",
    "if not soma_touching_vertices_dict is None:\n",
    "    root_curr = soma_touching_vertices_dict[list(soma_touching_vertices_dict.keys())[0]][0][0]\n",
    "else:\n",
    "    root_curr = None\n",
    "\n",
    "if print_fusion_steps:\n",
    "    print(f\"Time for preparing soma vertices and root: {time.time() - fusion_time }\")\n",
    "    fusion_time = time.time()\n",
    "\n",
    "# --------------- Part 3: Meshparty skeletonization and Decomposition ------------- #\n",
    "sk_meshparty_obj = m_sk.skeletonize_mesh_largest_component(limb_mesh_mparty,\n",
    "                                                        root=root_curr,\n",
    "                                                          filter_mesh=False)\n",
    "\n",
    "if print_fusion_steps:\n",
    "    print(f\"Time for 1st pass MP skeletonization: {time.time() - fusion_time }\")\n",
    "    fusion_time = time.time()\n",
    "\n",
    "(segment_branches, #skeleton branches\n",
    "divided_submeshes, divided_submeshes_idx, #mesh correspondence (mesh and indices)\n",
    "segment_widths_median) = m_sk.skeleton_obj_to_branches(sk_meshparty_obj,\n",
    "                                                      mesh = limb_mesh_mparty,\n",
    "                                                      meshparty_segment_size=meshparty_segment_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if print_fusion_steps:\n",
    "    print(f\"Decomposing first pass: {time.time() - fusion_time }\")\n",
    "    fusion_time = time.time()\n",
    "\n",
    "\n",
    "if use_meshafterparty:\n",
    "    print(\"Attempting to use MeshAfterParty Skeletonization and Mesh Correspondence\")\n",
    "    # --------------- Part 4: Find Individual Branches that could be MAP processed because of width ------------- #\n",
    "    #gettin the branches that should be passed through MAP skeletonization\n",
    "    pieces_above_threshold = np.where(segment_widths_median>width_threshold_MAP)[0]\n",
    "\n",
    "    #getting the correspondnece info for those MAP qualifying\n",
    "    width_large = segment_widths_median[pieces_above_threshold]\n",
    "    sk_large = [segment_branches[k] for k in pieces_above_threshold]\n",
    "    mesh_large_idx = [divided_submeshes_idx[k] for k in pieces_above_threshold]\n",
    "else:\n",
    "    print(\"Only Using MeshParty Skeletonization and Mesh Correspondence\")\n",
    "    mesh_large_idx = []\n",
    "    width_large = []\n",
    "    sk_large = []\n",
    "\n",
    "\n",
    "print(\"Another print\")\n",
    "mesh_pieces_for_MAP = []\n",
    "mesh_pieces_for_MAP_face_idx = []\n",
    "\n",
    "\n",
    "if len(mesh_large_idx) > 0: #will only continue processing if found MAP candidates\n",
    "\n",
    "    # --------------- Part 5: Find mesh connectivity and group MAP branch candidates into MAP sublimbs ------------- #\n",
    "    print(f\"Found len(mesh_large_idx) MAP candidates: {[len(k) for k in mesh_large_idx]}\")\n",
    "\n",
    "    #finds the connectivity edges of all the MAP candidates\n",
    "    mesh_large_connectivity = tu.mesh_list_connectivity(meshes = mesh_large_idx,\n",
    "                            main_mesh = limb_mesh_mparty,\n",
    "                            print_flag = False)\n",
    "    if print_fusion_steps:\n",
    "        print(f\"mesh_large_connectivity: {time.time() - fusion_time }\")\n",
    "        fusion_time = time.time()\n",
    "    \"\"\"\n",
    "    --------------- Grouping MAP candidates ----------------\n",
    "    Purpose: Will see what mesh pieces should be grouped together\n",
    "    to pass through CGAL skeletonization\n",
    "\n",
    "\n",
    "    Pseudocode: \n",
    "    1) build a networkx graph with all nodes for mesh_large_idx indexes\n",
    "    2) Add the edges\n",
    "    3) Find the connected components\n",
    "    4) Find sizes of connected components\n",
    "    5) For all those connected components that are of a large enough size, \n",
    "    add the mesh branches and skeletons to the final list\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(np.arange(len(mesh_large_idx)))\n",
    "    G.add_edges_from(mesh_large_connectivity)\n",
    "    conn_comp = list(nx.connected_components(G))\n",
    "\n",
    "    filtered_pieces = []\n",
    "\n",
    "    sk_large_size_filt = []\n",
    "    mesh_large_idx_size_filt = []\n",
    "    width_large_size_filt = []\n",
    "\n",
    "    for cc in conn_comp:\n",
    "        total_cc_size = np.sum([len(mesh_large_idx[k]) for k in cc])\n",
    "        if total_cc_size>size_threshold_MAP:\n",
    "            #print(f\"cc ({cc}) passed the size threshold because size was {total_cc_size}\")\n",
    "            filtered_pieces.append(pieces_above_threshold[list(cc)])\n",
    "\n",
    "    if print_fusion_steps:\n",
    "        print(f\"Finding MAP candidates connected components: {time.time() - fusion_time }\")\n",
    "        fusion_time = time.time()\n",
    "\n",
    "    #filtered_pieces: will have the indexes of all the branch candidates that should  be \n",
    "    #grouped together and passed through MAP skeletonization\n",
    "\n",
    "    if len(filtered_pieces) > 0:\n",
    "        # --------------- Part 6: If Found MAP sublimbs, Get the meshes and mesh_idxs of the sublimbs ------------- #\n",
    "        print(f\"len(filtered_pieces) = {len(filtered_pieces)}\")\n",
    "        #all the pieces that will require MAP mesh correspondence and skeletonization\n",
    "        #(already organized into their components)\n",
    "        mesh_pieces_for_MAP = [limb_mesh_mparty.submesh([np.concatenate(divided_submeshes_idx[k])],append=True,repair=False) for k in filtered_pieces]\n",
    "        mesh_pieces_for_MAP_face_idx = [np.concatenate(divided_submeshes_idx[k]) for k in filtered_pieces]\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Old Way: Finding connectivity of pieces through\n",
    "        mesh_idx_MP = [divided_submeshes_idx[k] for k in pieces_idx_MP]\n",
    "\n",
    "        mesh_large_connectivity_MP = tu.mesh_list_connectivity(meshes = mesh_idx_MP,\n",
    "                                main_mesh = limb_mesh_mparty,\n",
    "                                print_flag = False)\n",
    "\n",
    "        New Way: going to use skeleton connectivity to determine\n",
    "        connectivity of pieces\n",
    "\n",
    "        Pseudocode: \n",
    "        1)\n",
    "\n",
    "        \"\"\"\n",
    "        # --------------- Part 7: If Found MAP sublimbs, Get the meshes and mesh_idxs of the sublimbs ------------- #\n",
    "        # ********* if there are no pieces leftover then will automatically make all the lists below just empty (don't need to if.. else.. the case)****\n",
    "        pieces_idx_MP = np.setdiff1d(np.arange(len(divided_submeshes_idx)),np.concatenate(filtered_pieces))\n",
    "\n",
    "        skeleton_MP = [segment_branches[k] for k in pieces_idx_MP]\n",
    "        skeleton_connectivity_MP = sk.skeleton_list_connectivity(\n",
    "                                        skeletons=skeleton_MP\n",
    "                                        )\n",
    "        if print_fusion_steps:\n",
    "            print(f\"skeleton_connectivity_MP : {time.time() - fusion_time }\")\n",
    "            fusion_time = time.time()\n",
    "\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(np.arange(len(skeleton_MP)))\n",
    "        G.add_edges_from(skeleton_connectivity_MP)\n",
    "        sublimbs_MP = list(nx.connected_components(G))\n",
    "        sublimbs_MP_orig_idx = [pieces_idx_MP[list(k)] for k in sublimbs_MP]\n",
    "\n",
    "\n",
    "        #concatenate into sublimbs the skeletons and meshes\n",
    "        sublimb_mesh_idx_branches_MP = [divided_submeshes_idx[k] for k in sublimbs_MP_orig_idx]\n",
    "        sublimb_mesh_branches_MP = [[limb_mesh_mparty.submesh([ki],append=True,repair=False)\n",
    "                                    for ki in k] for k in sublimb_mesh_idx_branches_MP]\n",
    "        sublimb_meshes_MP = [limb_mesh_mparty.submesh([np.concatenate(k)],append=True,repair=False)\n",
    "                                                     for k in sublimb_mesh_idx_branches_MP]\n",
    "        sublimb_meshes_MP_face_idx = [np.concatenate(k)\n",
    "                                                     for k in sublimb_mesh_idx_branches_MP]\n",
    "        sublimb_skeleton_branches = [segment_branches[k] for k in sublimbs_MP_orig_idx]\n",
    "        widths_MP = [segment_widths_median[k] for k in sublimbs_MP_orig_idx]\n",
    "\n",
    "        if print_fusion_steps:\n",
    "            print(f\"Grouping MP Sublimbs by Graph: {time.time() - fusion_time }\")\n",
    "            fusion_time = time.time()\n",
    "\n",
    "\n",
    "# else: #if no pieces were determine to need MAP processing\n",
    "#     print(\"No MAP processing needed: just returning the Meshparty skeletonization and mesh correspondence\")\n",
    "#     raise Exception(\"Returning MP correspondence\")\n",
    "\n",
    "\n",
    "# nviz.plot_objects(main_mesh=tu.combine_meshes([limb_mesh_mparty,current_neuron[\"S0\"].mesh]),\n",
    "#                   main_mesh_color=\"green\",\n",
    "#     skeletons=sk_large_size_filt,\n",
    "#      meshes=[limb_mesh_mparty.submesh([k],append=True) for k in mesh_large_idx_size_filt],\n",
    "#       meshes_colors=\"red\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------- Part 8: If No MAP sublimbs found, set the MP sublimb lists to just the whole MP branch decomposition ------------- #\n",
    "\n",
    "#if no sublimbs need to be decomposed with MAP then just reassign all of the previous MP processing to the sublimb_MPs\n",
    "if len(mesh_pieces_for_MAP) == 0:\n",
    "    sublimb_meshes_MP = [limb_mesh_mparty] #trimesh pieces that have already been passed through MP skeletonization (may not need)\n",
    "    # -- the decomposition information ---\n",
    "    sublimb_mesh_branches_MP = [divided_submeshes] #the mesh branches for all the disconnected sublimbs\n",
    "    sublimb_mesh_idx_branches_MP = [divided_submeshes_idx] #The mesh branches idx that have already passed through MP skeletonization\n",
    "    sublimb_skeleton_branches = [segment_branches]#the skeleton bnraches for all the sublimbs\n",
    "    widths_MP = [segment_widths_median] #the mesh branches widths for all the disconnected groups\n",
    "\n",
    "    MAP_flag = False\n",
    "else:\n",
    "    MAP_flag = True\n",
    "\n",
    "\n",
    "\n",
    "mesh_pieces_for_MAP #trimesh pieces that should go through CGAL skeletonization\n",
    "sublimb_meshes_MP #trimesh pieces that have already been passed through MP skeletonization (may not need)\n",
    "\n",
    "# -- the decomposition information ---\n",
    "sublimb_mesh_branches_MP #the mesh branches for all the disconnected sublimbs\n",
    "sublimb_mesh_idx_branches_MP #The mesh branches idx that have already passed through MP skeletonization\n",
    "sublimb_skeleton_branches #the skeleton bnraches for all the sublimbs\n",
    "widths_MP #the mesh branches widths for all the disconnected groups\n",
    "\n",
    "if print_fusion_steps:\n",
    "    print(f\"Divinding into MP and MAP pieces: {time.time() - fusion_time }\")\n",
    "    fusion_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- At this point have the correct division between MAP and MP ------------------------\n",
    "\n",
    "# -------------- Part 9: Doing the MAP decomposition ------------------ #\n",
    "global_start_time = time.time()\n",
    "endpoints_must_keep = dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "limb_correspondence_MAP = dict()\n",
    "\n",
    "for sublimb_idx,(mesh,mesh_idx) in enumerate(zip(mesh_pieces_for_MAP,mesh_pieces_for_MAP_face_idx)):\n",
    "    print(f\"--- Working on MAP piece {sublimb_idx}---\")\n",
    "    mesh_start_time = time.time()\n",
    "    curr_soma_to_piece_touching_vertices = filter_soma_touching_vertices_dict_by_mesh(\n",
    "    mesh = mesh,\n",
    "    curr_piece_to_soma_touching_vertices = soma_touching_vertices_dict\n",
    "    )\n",
    "\n",
    "    if print_fusion_steps:\n",
    "        print(f\"MAP Filtering Soma Pieces: {time.time() - fusion_time }\")\n",
    "        fusion_time = time.time()\n",
    "\n",
    "    # ---- 0) Generating the Clean skeletons  -------------------------------------------#\n",
    "    if not curr_soma_to_piece_touching_vertices is None:\n",
    "        curr_total_border_vertices = dict([(k,np.vstack(v)) for k,v in curr_soma_to_piece_touching_vertices.items()])\n",
    "    else:\n",
    "        curr_total_border_vertices = None\n",
    "\n",
    "\n",
    "    cleaned_branch,curr_limb_endpoints_must_keep = sk.skeletonize_and_clean_connected_branch_CGAL(\n",
    "        mesh=mesh,\n",
    "        curr_soma_to_piece_touching_vertices=curr_soma_to_piece_touching_vertices,\n",
    "        total_border_vertices=curr_total_border_vertices,\n",
    "        filter_end_node_length=filter_end_node_length,\n",
    "        perform_cleaning_checks=perform_cleaning_checks,\n",
    "        combine_close_skeleton_nodes = combine_close_skeleton_nodes,\n",
    "        combine_close_skeleton_nodes_threshold=combine_close_skeleton_nodes_threshold,\n",
    "    use_surface_after_CGAL=use_surface_after_CGAL,\n",
    "    surface_reconstruction_size=surface_reconstruction_size)\n",
    "\n",
    "    if not curr_limb_endpoints_must_keep is None:\n",
    "        limb_to_endpoints_must_keep_list.append(curr_limb_endpoints_must_keep)\n",
    "        limb_to_soma_touching_vertices_list.append(curr_soma_to_piece_touching_vertices)\n",
    "    else:\n",
    "        print(\"Inside MAP decomposition and curr_limb_endpoints_must_keep was None\")\n",
    "\n",
    "    if len(cleaned_branch) == 0:\n",
    "        raise Exception(f\"Found a zero length skeleton for limb {z} of trmesh {branch}\")\n",
    "\n",
    "    if print_fusion_steps:\n",
    "        print(f\"skeletonize_and_clean_connected_branch_CGAL: {time.time() - fusion_time }\")\n",
    "        fusion_time = time.time()\n",
    "\n",
    "    # ---- 1) Generating Initial Mesh Correspondence -------------------------------------------#\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Working on limb correspondence for #{sublimb_idx} MAP piece\")\n",
    "    local_correspondence = mesh_correspondence_first_pass(mesh=mesh,\n",
    "                                                         skeleton=cleaned_branch,\n",
    "                                                         distance_by_mesh_center=distance_by_mesh_center)\n",
    "\n",
    "\n",
    "    print(f\"Total time for decomposition = {time.time() - start_time}\")\n",
    "    if print_fusion_steps:\n",
    "        print(f\"mesh_correspondence_first_pass: {time.time() - fusion_time }\")\n",
    "        fusion_time = time.time()\n",
    "\n",
    "\n",
    "    #------------- 2) Doing Some checks on the initial corespondence -------- #\n",
    "\n",
    "\n",
    "    if perform_cleaning_checks:\n",
    "        check_skeletonization_and_decomp(skeleton=cleaned_branch,\n",
    "                                        local_correspondence=local_correspondence)\n",
    "\n",
    "    # -------3) Finishing off the face correspondence so get 1-to-1 correspondence of mesh face to skeletal piece\n",
    "    local_correspondence_revised = correspondence_1_to_1(mesh=mesh,\n",
    "                                    local_correspondence=local_correspondence,\n",
    "                                    curr_limb_endpoints_must_keep=curr_limb_endpoints_must_keep,\n",
    "                                    curr_soma_to_piece_touching_vertices=curr_soma_to_piece_touching_vertices)\n",
    "\n",
    "    # -------3b) Fixing the mesh indices to correspond to the larger mesh as a whole\n",
    "    for k,v in local_correspondence_revised.items():\n",
    "        local_correspondence_revised[k][\"branch_face_idx\"] = mesh_idx[local_correspondence_revised[k][\"branch_face_idx\"]]\n",
    "\n",
    "    print(f\"Total time for MAP sublimb #{sublimb_idx} mesh processing = {time.time() - mesh_start_time}\")\n",
    "\n",
    "    if print_fusion_steps:\n",
    "        print(f\"correspondence_1_to_1: {time.time() - fusion_time }\")\n",
    "        fusion_time = time.time()\n",
    "\n",
    "    limb_correspondence_MAP[sublimb_idx] = local_correspondence_revised\n",
    "\n",
    "print(f\"Total time for MAP sublimb processing {time.time() - global_start_time}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- Part 10: Doing the MP Decomposition ---------------------- #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sublimb_meshes_MP #trimesh pieces that have already been passed through MP skeletonization (may not need)\n",
    "# -- the decomposition information ---\n",
    "sublimb_mesh_branches_MP #the mesh branches for all the disconnected sublimbs\n",
    "sublimb_mesh_idx_branches_MP #The mesh branches idx that have already passed through MP skeletonization\n",
    "sublimb_skeleton_branches #the skeleton bnraches for all the sublimbs\n",
    "widths_MP #the mesh branches widths for all the disconnected groups\n",
    "\n",
    "limb_correspondence_MP = dict()\n",
    "\n",
    "for sublimb_idx,mesh in enumerate(sublimb_meshes_MP):\n",
    "    print(f\"---- Working on MP Decomposition #{sublimb_idx} ----\")\n",
    "    mesh_start_time = time.time()\n",
    "\n",
    "    if len(sublimb_meshes_MP) == 1 and MAP_flag == False:\n",
    "        print(\"Using Quicker soma_to_piece_touching_vertices because no MAP and only one sublimb_mesh piece \")\n",
    "        curr_soma_to_piece_touching_vertices = soma_touching_vertices_dict\n",
    "    else:\n",
    "        if not soma_touching_vertices_dict is None:\n",
    "            curr_soma_to_piece_touching_vertices = filter_soma_touching_vertices_dict_by_mesh(\n",
    "                                                mesh = mesh,\n",
    "                                                curr_piece_to_soma_touching_vertices = soma_touching_vertices_dict\n",
    "                                                )\n",
    "        else:\n",
    "            curr_soma_to_piece_touching_vertices = None\n",
    "\n",
    "    if print_fusion_steps:\n",
    "        print(f\"MP filtering soma verts: {time.time() - fusion_time }\")\n",
    "        fusion_time = time.time()\n",
    "\n",
    "    #creating all of the sublimb groups\n",
    "    segment_branches = sublimb_skeleton_branches[sublimb_idx]\n",
    "    whole_sk_MP = sk.stack_skeletons(segment_branches)\n",
    "    branch = mesh\n",
    "    divided_submeshes = sublimb_mesh_branches_MP[sublimb_idx]\n",
    "    divided_submeshes_idx = sublimb_mesh_idx_branches_MP[sublimb_idx]\n",
    "    segment_widths_median = widths_MP[sublimb_idx]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    if curr_soma_to_piece_touching_vertices is None:\n",
    "        print(f\"Do Not Need to Fix MP Decomposition {sublimb_idx} so just continuing\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Fixing Possible Soma Extension Branch for Sublimb {sublimb_idx}\")\n",
    "\n",
    "        #If there is some soma touching then need to see if have to fix soma extending pieces\n",
    "        return_info = sk.create_soma_extending_branches(current_skeleton=whole_sk_MP,\n",
    "                                  skeleton_mesh=branch,\n",
    "                                  soma_to_piece_touching_vertices=curr_soma_to_piece_touching_vertices,\n",
    "                                  return_endpoints_must_keep=True,\n",
    "                                     return_created_branch_info=True)\n",
    "        new_sk,endpts,new_branch_info = return_info\n",
    "\n",
    "        if print_fusion_steps:\n",
    "            print(f\"MP (because soma touching verts) create_soma_extending_branches: {time.time() - fusion_time }\")\n",
    "            fusion_time = time.time()\n",
    "\n",
    "        no_soma_extension_add = True\n",
    "        \n",
    "        \n",
    "\n",
    "        if not endpts is None:\n",
    "            limb_to_endpoints_must_keep_list.append(endpts)\n",
    "            limb_to_soma_touching_vertices_list.append(curr_soma_to_piece_touching_vertices)\n",
    "        \n",
    "        print(f\"new_branch_info = {new_branch_info}\")\n",
    "        process_counter=0\n",
    "        for sm_idx in new_branch_info.keys(): \n",
    "            print(f\"sm_idx = {sm_idx}\")\n",
    "            for b_vert_idx,br_info in enumerate(new_branch_info[sm_idx]):\n",
    "                if br_info is None:\n",
    "                    print(\"\\n\\n No new branch created so continuing \\n\\n\")\n",
    "                    continue\n",
    "                no_soma_extension_add=False\n",
    "                process_counter+=1\n",
    "\n",
    "\n",
    "                #1) Get the newly added branch (and the original vertex which is the first row)\n",
    "                br_new,sm_bord_verts = br_info[\"new_branch\"],br_info[\"border_verts\"] #this will hold the new branch and the border vertices corresponding to it\n",
    "\n",
    "                curr_soma_to_piece_touching_vertices_MP = {sm_idx:[sm_bord_verts]}\n",
    "                endpoints_must_keep_MP = {sm_idx:[br_new[0][1]]}\n",
    "\n",
    "\n",
    "                orig_vertex = br_new[0][0]\n",
    "                print(f\"orig_vertex = {orig_vertex}\")\n",
    "\n",
    "                #2) Find the branches that have that coordinate (could be multiple)\n",
    "                match_sk_branches = sk.find_branch_skeleton_with_specific_coordinate(segment_branches,\n",
    "                    current_coordinate=orig_vertex)\n",
    "\n",
    "                print(f\"match_sk_branches = {match_sk_branches}\")\n",
    "\n",
    "\n",
    "\n",
    "                \"\"\" ******************* THIS NEEDS TO BE FIXED WITH THE SAME METHOD OF STITCHING ********************  \"\"\"\n",
    "                \"\"\"\n",
    "                Pseudocode:\n",
    "                1) Find if branch point will require split or not\n",
    "                2) If does require split then split the skeleton\n",
    "                3) Gather mesh pieces for correspondence and the skeletons\n",
    "                4) Run the mesh correspondence\n",
    "                - this case calculate the new widths after run \n",
    "                5) Replace the old branch parts with the new ones\n",
    "\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "                stitch_point_on_end_or_branch = find_if_stitch_point_on_end_or_branch(\n",
    "                                                        matched_branches_skeletons= segment_branches[match_sk_branches],\n",
    "                                                         stitch_coordinate=orig_vertex,\n",
    "                                                          verbose=False)\n",
    "\n",
    "\n",
    "                if not stitch_point_on_end_or_branch:\n",
    "                    matching_branch_sk = sk.cut_skeleton_at_coordinate(skeleton=segment_branches[match_sk_branches][0],\n",
    "                                                                      cut_coordinate = orig_vertex)\n",
    "                else:\n",
    "                    matching_branch_sk = segment_branches[match_sk_branches]\n",
    "\n",
    "\n",
    "                #3) Find the mesh and skeleton of the winning branch\n",
    "                matching_branch_meshes = np.array(divided_submeshes)[match_sk_branches]\n",
    "                matching_branch_mesh_idx = np.array(divided_submeshes_idx)[match_sk_branches]\n",
    "                extend_soma_mesh_idx = np.concatenate(matching_branch_mesh_idx)\n",
    "                extend_soma_mesh = limb_mesh_mparty.submesh([extend_soma_mesh_idx ],append=True,repair=False)\n",
    "\n",
    "                #4) Add newly created branch to skeleton and divide the skeleton into branches (could make 2 or 3)\n",
    "                #extended_skeleton_to_soma = sk.stack_skeletons([list(matching_branch_sk),br_new])\n",
    "\n",
    "                sk.check_skeleton_connected_component(sk.stack_skeletons(list(matching_branch_sk) + [br_new]))\n",
    "\n",
    "                #5) Run Adaptive mesh correspondnece using branches and mesh\n",
    "                local_correspondnece_MP = mesh_correspondence_first_pass(mesh=extend_soma_mesh,\n",
    "                                                                         skeleton_branches = list(matching_branch_sk) + [br_new]\n",
    "                                              #skeleton=extended_skeleton_to_soma\n",
    "                                                                        )\n",
    "                \n",
    "                new_submeshes_test = [k[\"correspondence_mesh\"] for k in local_correspondnece_MP.values()]\n",
    "                for n_t in new_submeshes_test:\n",
    "                    if len(tu.split(n_t)[0]) > 1:\n",
    "                        raise Exception(\"Not connected mesh\")\n",
    "\n",
    "                # GETTING MESHES THAT ARE NOT FULLY CONNECTED!!\n",
    "                local_correspondence_revised = correspondence_1_to_1(mesh=extend_soma_mesh,\n",
    "                                                            local_correspondence=local_correspondnece_MP,\n",
    "                                                            curr_limb_endpoints_must_keep=endpoints_must_keep_MP,\n",
    "                                                            curr_soma_to_piece_touching_vertices=curr_soma_to_piece_touching_vertices_MP)\n",
    "                \n",
    "                new_submeshes_test = [k[\"branch_mesh\"] for k in local_correspondence_revised.values()]\n",
    "                for n_t in new_submeshes_test:\n",
    "                    if len(tu.split(n_t)[0]) > 1:\n",
    "                        raise Exception(\"Not connected mesh\")\n",
    "                \n",
    "                \n",
    "\n",
    "                # All the things that should be revised:\n",
    "            #     segment_branches, #skeleton branches\n",
    "            #     divided_submeshes, divided_submeshes_idx, #mesh correspondence (mesh and indices)\n",
    "            #     segment_widths_median\n",
    "\n",
    "\n",
    "                new_submeshes = [k[\"branch_mesh\"] for k in local_correspondence_revised.values()]\n",
    "                new_submeshes_idx = [extend_soma_mesh_idx[k[\"branch_face_idx\"]] for k in local_correspondence_revised.values()]\n",
    "                new_skeletal_branches = [k[\"branch_skeleton\"] for k in local_correspondence_revised.values()]\n",
    "\n",
    "                #calculate the new width\n",
    "                ray_inter = tu.ray_pyembree.RayMeshIntersector(limb_mesh_mparty)\n",
    "                new_widths = []\n",
    "                for new_s_idx in new_submeshes_idx:\n",
    "                    curr_ray_distance = tu.ray_trace_distance(mesh=limb_mesh_mparty, \n",
    "                                        face_inds=new_s_idx,\n",
    "                                       ray_inter=ray_inter)\n",
    "                    curr_width_median = np.median(curr_ray_distance[curr_ray_distance!=0])\n",
    "                    print(f\"curr_width_median = {curr_width_median}\")\n",
    "                    if (not np.isnan(curr_width_median)) and (curr_width_median > 0):\n",
    "                        new_widths.append(curr_width_median)\n",
    "                    else:\n",
    "                        print(f\"USING A DEFAULT WIDTH BECAUSE THE NEWLY COMPUTED ONE WAS {curr_width_median}: {segment_widths_median[match_sk_branches[0]]}\")\n",
    "                        new_widths.append(segment_widths_median[match_sk_branches[0]])\n",
    "\n",
    "\n",
    "                #6) Remove the original branch and mesh correspondence and replace with the multiples\n",
    "#                     print(f\"match_sk_branches BEFORE = {match_sk_branches}\")\n",
    "#                     print(f\"segment_branches BEFORE = {segment_branches}\")\n",
    "#                     print(f\"len(new_skeletal_branches) = {len(new_skeletal_branches)}\")\n",
    "#                     print(f\"new_skeletal_branches BEFORE= {new_skeletal_branches}\")\n",
    "\n",
    "\n",
    "                #segment_branches = np.delete(segment_branches,match_sk_branches,axis=0)\n",
    "                #segment_branches = np.append(segment_branches,new_skeletal_branches,axis=0)\n",
    "\n",
    "                segment_branches = np.array([k for i,k in enumerate(segment_branches) if i not in match_sk_branches] + new_skeletal_branches)\n",
    "\n",
    "\n",
    "                divided_submeshes = np.delete(divided_submeshes,match_sk_branches,axis=0)\n",
    "                divided_submeshes = np.append(divided_submeshes,new_submeshes,axis=0)\n",
    "                for d_sm in divided_submeshes:\n",
    "                    if len(tu.split(d_sm)[0]) > 1:\n",
    "                        raise Exception(f\"Divided submesh was more than 2 pieces during {process_counter}\")\n",
    "                print(\"PASSED DIVIDED SUBMESH\")\n",
    "\n",
    "                #divided_submeshes_idx = np.delete(divided_submeshes_idx,match_sk_branches,axis=0)\n",
    "                #divided_submeshes_idx = np.append(divided_submeshes_idx,new_submeshes_idx,axis=0)\n",
    "                divided_submeshes_idx = np.array([k for i,k in enumerate(divided_submeshes_idx) if i not in match_sk_branches] + new_submeshes_idx)\n",
    "                \n",
    "                \n",
    "\n",
    "                segment_widths_median = np.delete(segment_widths_median,match_sk_branches,axis=0)\n",
    "                segment_widths_median = np.append(segment_widths_median,new_widths,axis=0)\n",
    "\n",
    "                try:\n",
    "                    debug = False\n",
    "                    if debug:\n",
    "                        print(f\"segment_branches.shape = {segment_branches.shape}\")\n",
    "                        print(f\"segment_branches = {segment_branches}\")\n",
    "                        print(f\"new_skeletal_branches = {new_skeletal_branches}\")\n",
    "                    sk.check_skeleton_connected_component(sk.stack_skeletons(segment_branches))\n",
    "                except:\n",
    "                    su.compressed_pickle(local_correspondence_revised,\"local_correspondence_revised\")\n",
    "                print(\"checked segment branches after soma add on\")\n",
    "                return_find = sk.find_branch_skeleton_with_specific_coordinate(segment_branches,\n",
    "                                             orig_vertex)\n",
    "\n",
    "\n",
    "\n",
    "                \"\"\" ******************* END OF HOW CAN DO STITCHING ********************  \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if no_soma_extension_add:\n",
    "            print(\"No soma extending branch was added for this sublimb even though it had a soma border (means they already existed)\")\n",
    "\n",
    "        if print_fusion_steps:\n",
    "            print(f\"MP (because soma touching verts) soma extension add: {time.time() - fusion_time }\")\n",
    "            fusion_time = time.time()\n",
    "\n",
    "    #building the limb correspondence\n",
    "    limb_correspondence_MP[sublimb_idx] = dict()\n",
    "\n",
    "    for zz,b_sk in enumerate(segment_branches):\n",
    "        limb_correspondence_MP[sublimb_idx][zz] = dict(\n",
    "            branch_skeleton = b_sk,\n",
    "            width_from_skeleton = segment_widths_median[zz],\n",
    "            branch_mesh = divided_submeshes[zz],\n",
    "            branch_face_idx = divided_submeshes_idx[zz]\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------- Part C: Will make sure the correspondences can all be stitched together --------------- #\n",
    "\n",
    "# Only want to perform this step if both MP and MAP pieces\n",
    "if len(limb_correspondence_MAP)>0 and len(limb_correspondence_MP)>0:\n",
    "\n",
    "    # -------------- Part 11: Getting Sublimb Mesh and Skeletons and Gets connectivitiy by Mesh -------#\n",
    "    # -------------(filtering connections to only MP to MAP edges)--------------- #\n",
    "\n",
    "    # ---- Doing the mesh connectivity ---------#\n",
    "    sublimb_meshes_MP = []\n",
    "    sublimb_skeletons_MP = []\n",
    "\n",
    "    for sublimb_key,sublimb_v in limb_correspondence_MP.items():\n",
    "        sublimb_meshes_MP.append(tu.combine_meshes([branch_v[\"branch_mesh\"] for branch_v in sublimb_v.values()]))\n",
    "        sublimb_skeletons_MP.append(sk.stack_skeletons([branch_v[\"branch_skeleton\"] for branch_v in sublimb_v.values()]))\n",
    "\n",
    "    sublimb_meshes_MAP = []\n",
    "    sublimb_skeletons_MAP = []\n",
    "\n",
    "    for sublimb_key,sublimb_v in limb_correspondence_MAP.items():\n",
    "        sublimb_meshes_MAP.append(tu.combine_meshes([branch_v[\"branch_mesh\"] for branch_v in sublimb_v.values()]))\n",
    "        sublimb_skeletons_MAP.append(sk.stack_skeletons([branch_v[\"branch_skeleton\"] for branch_v in sublimb_v.values()]))\n",
    "\n",
    "\n",
    "\n",
    "    mesh_conn,mesh_conn_vertex_groups = tu.mesh_list_connectivity(meshes = sublimb_meshes_MP + sublimb_meshes_MAP,\n",
    "                                        main_mesh = limb_mesh_mparty,\n",
    "                                        min_common_vertices=1,\n",
    "                                        return_vertex_connection_groups=True,\n",
    "                                        return_largest_vertex_connection_group=True,\n",
    "                                        print_flag = False)\n",
    "    mesh_conn_old = copy.deepcopy(mesh_conn)\n",
    "\n",
    "\n",
    "\n",
    "    #check that every MAP piece mapped to a MP piece\n",
    "    mesh_conn_filt = []\n",
    "    mesh_conn_vertex_groups_filt = []\n",
    "    for j,(m1,m2) in enumerate(mesh_conn):\n",
    "        if m1 < len(sublimb_meshes_MP) and m2 >=len(sublimb_meshes_MP):\n",
    "            mesh_conn_filt.append([m1,m2])\n",
    "            mesh_conn_vertex_groups_filt.append(mesh_conn_vertex_groups[j])\n",
    "    mesh_conn_filt = np.array(mesh_conn_filt)\n",
    "\n",
    "    mesh_conn = mesh_conn_filt\n",
    "    mesh_conn_vertex_groups = mesh_conn_vertex_groups_filt\n",
    "\n",
    "    #check that the mapping should create only one connected component\n",
    "    G = nx.from_edgelist(mesh_conn)\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        if len(G) != len(sublimb_meshes_MP) + len(sublimb_meshes_MAP):\n",
    "            raise Exception(\"Number of nodes in mesh connectivity graph is not equal to number of  MAP and MP sublimbs\")\n",
    "\n",
    "        connect_comp = list(nx.connected_components(G))\n",
    "        if len(connect_comp)>1:\n",
    "            raise Exception(f\"Mesh connectivity was not one component, instead it was ({len(connect_comp)}): {connect_comp} \")\n",
    "    except:\n",
    "        print(f\"mesh_conn_filt = {mesh_conn_filt}\")\n",
    "        print(f\"mesh_conn_old = {mesh_conn_old}\")\n",
    "        mesh_conn_adjusted = np.vstack([mesh_conn[:,0],mesh_conn[:,1]-len(sublimb_meshes_MP)]).T\n",
    "        print(f\"mesh_conn_adjusted = {mesh_conn_adjusted}\")\n",
    "        print(f\"len(sublimb_meshes_MP) = {len(sublimb_meshes_MP)}\")\n",
    "        print(f\"len(sublimb_meshes_MAP) = {len(sublimb_meshes_MAP)}\")\n",
    "        meshes = sublimb_meshes_MP + sublimb_meshes_MAP\n",
    "        #su.compressed_pickle(meshes,\"meshes\")\n",
    "        su.compressed_pickle(sublimb_meshes_MP,\"sublimb_meshes_MP\")\n",
    "        su.compressed_pickle(sublimb_meshes_MAP,\"sublimb_meshes_MAP\")\n",
    "        su.compressed_pickle(limb_mesh_mparty,\"limb_mesh_mparty\")\n",
    "        su.compressed_pickle(sublimb_skeletons_MP,\"sublimb_skeletons_MP\")\n",
    "        su.compressed_pickle(sublimb_skeletons_MAP,\"sublimb_skeletons_MAP\")\n",
    "\n",
    "\n",
    "        raise Exception(\"Something went wrong in the connectivity\")\n",
    "\n",
    "\n",
    "    #adjust the connection indices for MP and MAP indices\n",
    "    mesh_conn_adjusted = np.vstack([mesh_conn[:,0],mesh_conn[:,1]-len(sublimb_meshes_MP)]).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Pseudocode:\n",
    "    For each connection edge:\n",
    "        For each vertex connection group:\n",
    "            1) Get the endpoint vertices of the MP skeleton\n",
    "            2) Find the closest endpoint vertex to the vertex connection group (this is MP stitch point)\n",
    "            3) Find the closest skeletal point on MAP pairing (MAP stitch) \n",
    "            4) Find the branches that have that MAP stitch point:\n",
    "            5A) If the number of branches corresponding to stitch point is multipled\n",
    "                --> then we are stitching at a branching oint\n",
    "                i) Just add the skeletal segment from MP_stitch to MAP stitch to the MP skeletal segment\n",
    "                ii) \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------- STITCHING PHASE -------#\n",
    "    for (MP_idx,MAP_idx),v_g in zip(mesh_conn_adjusted,mesh_conn_vertex_groups):\n",
    "        print(f\"\\n---- Working on {(MP_idx,MAP_idx)} connection-----\")\n",
    "\n",
    "        \"\"\"\n",
    "        This old way of getting the endpoints was not good because could possibly just need\n",
    "        a stitching done between original branch junction\n",
    "\n",
    "        skeleton_MP_graph = sk.convert_skeleton_to_graph(curr_skeleton_MP)\n",
    "        endpoint_nodes = xu.get_nodes_of_degree_k(skeleton_MP_graph,1)\n",
    "        endpoint_nodes_coordinates = xu.get_node_attributes(skeleton_MP_graph,node_list=endpoint_nodes)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # -------------- Part 12: Find the MP and MAP stitching point and branches that contain the stitching point-------#\n",
    "\n",
    "\n",
    "        #getting the skeletons that should be stitched\n",
    "        curr_skeleton_MP = sk.stack_skeletons([branch_v[\"branch_skeleton\"] for branch_v in limb_correspondence_MP[MP_idx].values()])\n",
    "        curr_skeleton_MAP = sk.stack_skeletons([branch_v[\"branch_skeleton\"] for branch_v in limb_correspondence_MAP[MAP_idx].values()])\n",
    "\n",
    "        #1) Get the endpoint vertices of the MP skeleton branches (so every endpoint or high degree node)\n",
    "        #(needs to be inside loop because limb correspondence will change)\n",
    "        curr_MP_branch_skeletons = [limb_correspondence_MP[MP_idx][k][\"branch_skeleton\"] for k in np.sort(list(limb_correspondence_MP[MP_idx].keys()))]\n",
    "        endpoint_nodes_coordinates = np.array([sk.find_branch_endpoints(k) for k in curr_MP_branch_skeletons])\n",
    "        endpoint_nodes_coordinates = np.unique(endpoint_nodes_coordinates.reshape(-1,3),axis=0)\n",
    "\n",
    "        #2) Find the closest endpoint vertex to the vertex connection group (this is MP stitch point)\n",
    "        av_vert = np.mean(v_g,axis=0)\n",
    "        winning_vertex = endpoint_nodes_coordinates[np.argmin(np.linalg.norm(endpoint_nodes_coordinates-av_vert,axis=1))]\n",
    "        print(f\"winning_vertex = {winning_vertex}\")\n",
    "\n",
    "\n",
    "        #2b) Find the branch points where the winning vertex is located\n",
    "        MP_branches_with_stitch_point = sk.find_branch_skeleton_with_specific_coordinate(\n",
    "            divded_skeleton=curr_MP_branch_skeletons,\n",
    "            current_coordinate = winning_vertex\n",
    "        )\n",
    "        print(f\"MP_branches_with_stitch_point = {MP_branches_with_stitch_point}\")\n",
    "\n",
    "\n",
    "        #3) Find the closest skeletal point on MAP pairing (MAP stitch)\n",
    "        MAP_skeleton_coords = np.unique(curr_skeleton_MAP.reshape(-1,3),axis=0)\n",
    "        MAP_stitch_point = MAP_skeleton_coords[np.argmin(np.linalg.norm(MAP_skeleton_coords-winning_vertex,axis=1))]\n",
    "\n",
    "\n",
    "        #3b) Consider if the stitch point is close enough to end or branch node in skeleton:\n",
    "        # and if so then reassign\n",
    "        if move_MAP_stitch_to_end_or_branch:\n",
    "            MAP_stitch_point_new,change_status = sk.move_point_to_nearest_branch_end_point_within_threshold(\n",
    "                                                    skeleton=curr_skeleton_MAP,\n",
    "                                                    coordinate=MAP_stitch_point,\n",
    "                                                    distance_to_move_point_threshold = distance_to_move_point_threshold,\n",
    "                                                    verbose=True\n",
    "\n",
    "                                                    )\n",
    "            MAP_stitch_point=MAP_stitch_point_new\n",
    "\n",
    "\n",
    "        #4) Find the branches that have that MAP stitch point:\n",
    "        curr_MAP_branch_skeletons = [limb_correspondence_MAP[MAP_idx][k][\"branch_skeleton\"]\n",
    "                                         for k in np.sort(list(limb_correspondence_MAP[MAP_idx].keys()))]\n",
    "\n",
    "        MAP_branches_with_stitch_point = sk.find_branch_skeleton_with_specific_coordinate(\n",
    "            divded_skeleton=curr_MAP_branch_skeletons,\n",
    "            current_coordinate = MAP_stitch_point\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        MAP_stitch_point_on_end_or_branch = False\n",
    "        if len(MAP_branches_with_stitch_point)>1:\n",
    "            MAP_stitch_point_on_end_or_branch = True\n",
    "        elif len(MAP_branches_with_stitch_point)==1:\n",
    "            if len(nu.matching_rows(sk.find_branch_endpoints(curr_MAP_branch_skeletons[MAP_branches_with_stitch_point[0]]),\n",
    "                                    MAP_stitch_point))>0:\n",
    "                MAP_stitch_point_on_end_or_branch=True\n",
    "        else:\n",
    "            raise Exception(\"No matching MAP values\")\n",
    "\n",
    "\n",
    "        print(f\"MAP_branches_with_stitch_point = {MAP_branches_with_stitch_point}\")\n",
    "        print(f\"MAP_stitch_point_on_end_or_branch = {MAP_stitch_point_on_end_or_branch}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ------------------------- This part does the stitching -------------------- #\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Pseudocode:\n",
    "        1) For all MP branches\n",
    "            a) Get neighbor coordinates to MP stitch points\n",
    "            b) Delete the MP Stitch points on each \n",
    "            c) Add skeleton segment from neighbor to MAP stitch point\n",
    "        2) Get skeletons and meshes from MP and MAP pieces\n",
    "        3) Run mesh correspondence to get new meshes and mesh_idx and widths\n",
    "        4a) If MAP_stitch_point_on_end_or_branch is False\n",
    "        - Delete the old MAP branch parts and replace with new MAP ones\n",
    "        4b) Revise the meshes,  mesh_idx, and widths of the MAP pieces\n",
    "        5) Revise the meshes,  mesh_idx, and widths of the MP pieces\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # -------------- Part 13: Will Adjust the MP branches that have the stitch point so extends to the MAP stitch point -------#\n",
    "        curr_MP_sk = []\n",
    "        for b_idx in MP_branches_with_stitch_point:\n",
    "\n",
    "            #a) Get neighbor coordinates to MP stitch points\n",
    "            MP_stitch_branch_graph = sk.convert_skeleton_to_graph(curr_MP_branch_skeletons[b_idx])\n",
    "            stitch_node = xu.get_nodes_with_attributes_dict(MP_stitch_branch_graph,dict(coordinates=winning_vertex))[0]\n",
    "            stitch_neighbors = xu.get_neighbors(MP_stitch_branch_graph,stitch_node)\n",
    "\n",
    "            if len(stitch_neighbors) != 1:\n",
    "                raise Exception(\"Not just one neighbor for stitch point of MP branch\")\n",
    "            keep_neighbor = stitch_neighbors[0]  \n",
    "            keep_neighbor_coordinates = xu.get_node_attributes(MP_stitch_branch_graph,node_list=[keep_neighbor])[0]\n",
    "\n",
    "            #b) Delete the MP Stitch points on each \n",
    "            MP_stitch_branch_graph.remove_node(stitch_node)\n",
    "\n",
    "            \"\"\" Old way that does not do smoothing\n",
    "\n",
    "            #c) Add skeleton segment from neighbor to MAP stitch point\n",
    "            new_node_name = np.max(MP_stitch_branch_graph.nodes())+1\n",
    "\n",
    "            MP_stitch_branch_graph.add_nodes_from([(int(new_node_name),{\"coordinates\":MAP_stitch_point})])\n",
    "            MP_stitch_branch_graph.add_weighted_edges_from([(keep_neighbor,new_node_name,np.linalg.norm(MAP_stitch_point - keep_neighbor_coordinates))])\n",
    "\n",
    "            new_MP_skeleton = sk.convert_graph_to_skeleton(MP_stitch_branch_graph)\n",
    "\n",
    "            \"\"\"\n",
    "            try:\n",
    "                if len(MP_stitch_branch_graph)>1:\n",
    "                    new_MP_skeleton = sk.add_and_smooth_segment_to_branch(skeleton=sk.convert_graph_to_skeleton(MP_stitch_branch_graph),\n",
    "                                                    skeleton_stitch_point=keep_neighbor_coordinates,\n",
    "                                                     new_stitch_point=MAP_stitch_point)\n",
    "                else:\n",
    "                    print(\"Not even attempting smoothing segment because once keep_neighbor_coordinates\")\n",
    "                    new_MP_skeleton = np.vstack([keep_neighbor_coordinates,MAP_stitch_point]).reshape(-1,2,3)\n",
    "            except:\n",
    "                su.compressed_pickle(MP_stitch_branch_graph,\"MP_stitch_branch_graph\")\n",
    "                su.compressed_pickle(keep_neighbor_coordinates,\"keep_neighbor_coordinates\")\n",
    "                su.compressed_pickle(MAP_stitch_point,\"MAP_stitch_point\")\n",
    "\n",
    "\n",
    "                raise Exception(\"Something went wrong with add_and_smooth_segment_to_branch\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #smooth over the new skeleton\n",
    "            new_MP_skeleton_smooth = sk.resize_skeleton_branch(new_MP_skeleton,\n",
    "                                                              segment_width=meshparty_segment_size)\n",
    "\n",
    "            curr_MP_sk.append(new_MP_skeleton_smooth)\n",
    "\n",
    "\n",
    "\n",
    "        #2) Get skeletons and meshes from MP and MAP pieces\n",
    "        curr_MAP_sk = [limb_correspondence_MAP[MAP_idx][k][\"branch_skeleton\"] for k in MAP_branches_with_stitch_point]\n",
    "\n",
    "        #2.1) Going to break up the MAP skeleton if need be\n",
    "        \"\"\"\n",
    "        Pseudocode:\n",
    "        a) check to see if it needs to be broken up\n",
    "        If it does:\n",
    "        b) Convert the skeleton into a graph\n",
    "        c) Find the node of the MAP stitch point (where need to do the breaking)\n",
    "        d) Find the degree one nodes\n",
    "        e) For each degree one node:\n",
    "        - Find shortest path from stitch node to end node\n",
    "        - get a subgraph from that path\n",
    "        - convert graph to a skeleton and save as new skeletons\n",
    "\n",
    "        \"\"\"\n",
    "        # -------------- Part 14: Breaks Up MAP skeleton into 2 pieces if Needs (because MAP stitch point not on endpoint or branch point)  -------#\n",
    "\n",
    "        #a) check to see if it needs to be broken up\n",
    "        if not MAP_stitch_point_on_end_or_branch:\n",
    "            if len(curr_MAP_sk) > 1:\n",
    "                raise Exception(f\"There was more than one skeleton for MAP skeletons even though MAP_stitch_point_on_end_or_branch = {MAP_stitch_point_on_end_or_branch}\")\n",
    "\n",
    "\n",
    "            skeleton_to_cut = curr_MAP_sk[0]\n",
    "            curr_MAP_sk = sk.cut_skeleton_at_coordinate(skeleton=skeleton_to_cut,\n",
    "                                                        cut_coordinate=MAP_stitch_point)\n",
    "\n",
    "\n",
    "        # -------------- Part 15: Gets all of the skeletons and Mesh to divide u and does mesh correspondence -------#\n",
    "        # ------------- revise IDX so still references the whole limb mesh -----------#\n",
    "        curr_MAP_meshes_idx = [limb_correspondence_MAP[MAP_idx][k][\"branch_face_idx\"] for k in MAP_branches_with_stitch_point]\n",
    "\n",
    "        curr_MP_sk\n",
    "        curr_MP_meshes_idx = [limb_correspondence_MP[MP_idx][k][\"branch_face_idx\"] for k in MP_branches_with_stitch_point]\n",
    "\n",
    "        stitching_mesh_idx = np.concatenate(curr_MAP_meshes_idx + curr_MP_meshes_idx)\n",
    "        stitching_mesh = limb_mesh_mparty.submesh([stitching_mesh_idx],append=True,repair=False)\n",
    "        stitching_skeleton_branches = curr_MAP_sk + curr_MP_sk\n",
    "        \"\"\"\n",
    "\n",
    "        ****** NEED TO GET THE RIGHT MESH TO RUN HE IDX ON SO GETS A GOOD MESH (CAN'T BE LIMB_MESH_MPARTY)\n",
    "        BUT MUST BE THE ORIGINAL MAP MESH\n",
    "\n",
    "        mesh_pieces_for_MAP\n",
    "        sublimb_meshes_MP\n",
    "\n",
    "        mesh_pieces_for_MAP_face_idx\n",
    "        sublimb_meshes_MP_face_idx\n",
    "\n",
    "        stitching_mesh = tu.combine_meshes(curr_MAP_meshes + curr_MP_meshes)\n",
    "        stitching_skeleton_branches = curr_MAP_sk + curr_MP_sk\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #3) Run mesh correspondence to get new meshes and mesh_idx and widths\n",
    "        local_correspondnece_stitch = mesh_correspondence_first_pass(mesh=stitching_mesh,\n",
    "                                      skeleton_branches=stitching_skeleton_branches)\n",
    "\n",
    "        try:\n",
    "\n",
    "            local_correspondence_stitch_revised = correspondence_1_to_1(mesh=stitching_mesh,\n",
    "                                                        local_correspondence=local_correspondnece_stitch,\n",
    "                                                        curr_limb_endpoints_must_keep=None,\n",
    "                                                        curr_soma_to_piece_touching_vertices=None)\n",
    "        except:\n",
    "            su.compressed_pickle(stitching_skeleton_branches,\"stitching_skeleton_branches\")\n",
    "            su.compressed_pickle(stitching_mesh,\"stitching_mesh\")\n",
    "            su.compressed_pickle(local_correspondnece_stitch,\"local_correspondnece_stitch\")\n",
    "            raise Exception(\"Something went wrong with 1 to 1 correspondence\")\n",
    "\n",
    "\n",
    "        #Need to readjust the mesh correspondence idx\n",
    "        for k,v in local_correspondence_stitch_revised.items():\n",
    "            local_correspondence_stitch_revised[k][\"branch_face_idx\"] = stitching_mesh_idx[local_correspondence_stitch_revised[k][\"branch_face_idx\"]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # -------------- Part 16: Overwrite old branch entries (and add on one new to MAP if required a split) -------#\n",
    "\n",
    "\n",
    "        #4a) If MAP_stitch_point_on_end_or_branch is False\n",
    "        #- Delete the old MAP branch parts and replace with new MAP ones\n",
    "        if not MAP_stitch_point_on_end_or_branch:\n",
    "            print(\"Deleting branches from dictionary\")\n",
    "            del limb_correspondence_MAP[MAP_idx][MAP_branches_with_stitch_point[0]]\n",
    "            #adding the two new branches created from the stitching\n",
    "            limb_correspondence_MAP[MAP_idx][MAP_branches_with_stitch_point[0]] = local_correspondence_stitch_revised[0]\n",
    "            limb_correspondence_MAP[MAP_idx][np.max(list(limb_correspondence_MAP[MAP_idx].keys()))+1] = local_correspondence_stitch_revised[1]\n",
    "\n",
    "            #have to reorder the keys\n",
    "            #limb_correspondence_MAP[MAP_idx] = dict([(k,limb_correspondence_MAP[MAP_idx][k]) for k in np.sort(list(limb_correspondence_MAP[MAP_idx].keys()))])\n",
    "            limb_correspondence_MAP[MAP_idx] = gu.order_dict_by_keys(limb_correspondence_MAP[MAP_idx])\n",
    "\n",
    "        else: #4b) Revise the meshes,  mesh_idx, and widths of the MAP pieces if weren't broken up\n",
    "            for j,curr_MAP_idx_fixed in enumerate(MAP_branches_with_stitch_point):\n",
    "                limb_correspondence_MAP[MAP_idx][curr_MAP_idx_fixed] = local_correspondence_stitch_revised[j]\n",
    "\n",
    "        #5) Revise the meshes,  mesh_idx, and widths of the MP pieces\n",
    "        for j,curr_MP_idx_fixed in enumerate(MP_branches_with_stitch_point):\n",
    "            limb_correspondence_MP[MP_idx][curr_MP_idx_fixed] = local_correspondence_stitch_revised[j+len(curr_MAP_sk)]\n",
    "\n",
    "\n",
    "        print(f\" Finished with {(MP_idx,MAP_idx)} \\n\\n\\n\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"There were not both MAP and MP pieces so skipping the stitch resolving phase\")\n",
    "\n",
    "print(f\"Time for decomp of Limb = {time.time() - curr_limb_time}\")\n",
    "#     # ------------- Saving the MAP and MP Decompositions ---------------- #\n",
    "#     proper_limb_mesh_correspondence_MAP[curr_limb_idx] = limb_correspondence_MAP\n",
    "#     proper_limb_mesh_correspondence_MP[curr_limb_idx] = limb_correspondence_MP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------- Part 17: Grouping the MP and MAP Correspondence into one correspondence dictionary -------#\n",
    "limb_correspondence_individual = dict()\n",
    "counter = 0\n",
    "\n",
    "for sublimb_idx,sublimb_branches in limb_correspondence_MAP.items():\n",
    "    for branch_dict in sublimb_branches.values():\n",
    "        limb_correspondence_individual[counter]= branch_dict\n",
    "        counter += 1\n",
    "for sublimb_idx,sublimb_branches in limb_correspondence_MP.items():\n",
    "    for branch_dict in sublimb_branches.values():\n",
    "        limb_correspondence_individual[counter]= branch_dict\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "#info that may be used for concept networks\n",
    "network_starting_info = dict(\n",
    "            touching_verts_list = limb_to_soma_touching_vertices_list,\n",
    "            endpoints_must_keep = limb_to_endpoints_must_keep_list\n",
    ")\n",
    "\n",
    "if not return_concept_network:\n",
    "    if return_concept_network_starting_info: #because may want to calculate the concept networks later\n",
    "        return_values= limb_correspondence_individual,network_starting_info\n",
    "    else:\n",
    "        return_value = limb_correspondence_individual\n",
    "else:\n",
    "    limb_to_soma_concept_networks = calculate_limb_concept_networks(limb_correspondence_individual,\n",
    "                                                                    run_concept_network_checks=run_concept_network_checks,\n",
    "                                                                   **network_starting_info)\n",
    "\n",
    "\n",
    "return_value = limb_correspondence_individual,limb_to_soma_concept_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_correspondence_revised = correspondence_1_to_1(mesh=extend_soma_mesh,\n",
    "                                                    local_correspondence=local_correspondnece_MP,\n",
    "                                                    curr_limb_endpoints_must_keep=endpoints_must_keep_MP,\n",
    "                                                    curr_soma_to_piece_touching_vertices=curr_soma_to_piece_touching_vertices_MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Working on 1-to-1 correspondence-----\n",
      "max(original_labels),len(original_labels) = (2, 3)\n",
      "empty_indices % = 0.3232089480281554\n",
      " conflict_indices % = 0.17944267669462927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599416fe81f84c579f6c718ae3fd3bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n",
      "Took 19 iterations to expand the label back\n",
      "empty_indices % = 0.0\n",
      " conflict_indices % = 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adeff712fbd34db1b35b26e8a80d8020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER face_lookup_resolved_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c89f499e94f4780aadf11e471871982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "cu = reload(cu)\n",
    "mesh=copy.deepcopy(extend_soma_mesh)\n",
    "local_correspondence=copy.deepcopy(local_correspondnece_MP)\n",
    "curr_limb_endpoints_must_keep=copy.deepcopy(endpoints_must_keep_MP)\n",
    "curr_soma_to_piece_touching_vertices=copy.deepcopy(curr_soma_to_piece_touching_vertices_MP)\n",
    "\n",
    "\"\"\"\n",
    "Will Fix the 1-to-1 Correspondence of the mesh\n",
    "correspondence for the limbs and make sure that the\n",
    "endpoints that are designated as touching the soma then \n",
    "make sure the mesh correspondnece reaches the soma limb border\n",
    "\n",
    "\"\"\"\n",
    "mesh_start_time = time.time()\n",
    "print(f\"\\n\\n--- Working on 1-to-1 correspondence-----\")\n",
    "\n",
    "#geting the current limb mesh\n",
    "\n",
    "no_missing_labels = list(local_correspondence.keys()) #counts the number of divided branches which should be the total number of labels\n",
    "curr_limb_mesh = mesh\n",
    "\n",
    "#set up the face dictionary\n",
    "face_lookup = dict([(j,[]) for j in range(0,len(curr_limb_mesh.faces))])\n",
    "\n",
    "for j,branch_piece in local_correspondence.items():\n",
    "    curr_faces_corresponded = branch_piece[\"correspondence_face_idx\"]\n",
    "\n",
    "    for c in curr_faces_corresponded:\n",
    "        face_lookup[c].append(j)\n",
    "\n",
    "original_labels = set(list(itertools.chain.from_iterable(list(face_lookup.values()))))\n",
    "print(f\"max(original_labels),len(original_labels) = {(max(original_labels),len(original_labels))}\")\n",
    "\n",
    "\n",
    "if len(original_labels) != len(no_missing_labels):\n",
    "    raise Exception(f\"len(original_labels) != len(no_missing_labels) for original_labels = {len(original_labels)},no_missing_labels = {len(no_missing_labels)}\")\n",
    "\n",
    "if max(original_labels) + 1 > len(original_labels):\n",
    "    raise Exception(\"There are some missing labels in the initial labeling\")\n",
    "\n",
    "\n",
    "\n",
    "#here is where can call the function that resolves the face labels\n",
    "face_coloring_copy = cu.resolve_empty_conflicting_face_labels(\n",
    "                 curr_limb_mesh = curr_limb_mesh,\n",
    "                 face_lookup=face_lookup,\n",
    "                 no_missing_labels = list(original_labels)\n",
    ")\n",
    "\n",
    "\"\"\"  9/17 Addition: Will make sure that the desired starting node is touching the soma border \"\"\"\n",
    "\"\"\"\n",
    "Pseudocode:\n",
    "For each soma it is touching\n",
    "0) Get the soma border\n",
    "1) Find the label_to_expand based on the starting coordinate\n",
    "a. Get the starting coordinate\n",
    "\n",
    "soma_to_piece_touching_vertices=None\n",
    "endpoints_must_keep\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#curr_limb_endpoints_must_keep --> stores the endpoints that should be connected to the soma\n",
    "#curr_soma_to_piece_touching_vertices --> maps soma to  a list of grouped touching vertices\n",
    "\n",
    "if (not curr_limb_endpoints_must_keep is None) and (not curr_soma_to_piece_touching_vertices is None):\n",
    "    for sm,soma_border_list in curr_soma_to_piece_touching_vertices.items():\n",
    "        for curr_soma_border,st_coord in zip(soma_border_list,curr_limb_endpoints_must_keep[sm]):\n",
    "\n",
    "            #1) Find the label_to_expand based on the starting coordinate\n",
    "            divided_branches = [v[\"branch_skeleton\"] for v in local_correspondence.values()]\n",
    "            #print(f\"st_coord = {st_coord}\")\n",
    "            label_to_expand = sk.find_branch_skeleton_with_specific_coordinate(divded_skeleton=divided_branches,\n",
    "                                                                               current_coordinate=st_coord)[0]\n",
    "\n",
    "\n",
    "            face_coloring_copy = cu.waterfill_starting_label_to_soma_border(curr_limb_mesh,\n",
    "                                               border_vertices=curr_soma_border,\n",
    "                                                label_to_expand=label_to_expand,\n",
    "                                               total_face_labels=face_coloring_copy,\n",
    "                                               print_flag=True)\n",
    "\n",
    "\n",
    "# -- splitting the mesh pieces into individual pieces\n",
    "divided_submeshes,divided_submeshes_idx = tu.split_mesh_into_face_groups(curr_limb_mesh,face_coloring_copy)\n",
    "\n",
    "#-- check that all the split mesh pieces are one component --#\n",
    "local_correspondence_revised = deepcopy(local_correspondence)\n",
    "#save off the new data as branch mesh\n",
    "for k in local_correspondence_revised.keys():\n",
    "    local_correspondence_revised[k][\"branch_mesh\"] = divided_submeshes[k]\n",
    "    local_correspondence_revised[k][\"branch_face_idx\"] = divided_submeshes_idx[k]\n",
    "\n",
    "    #clean the limb correspondence that we do not need\n",
    "    del local_correspondence_revised[k][\"correspondence_mesh\"]\n",
    "    del local_correspondence_revised[k][\"correspondence_face_idx\"]\n",
    "\n",
    "return_value  = local_correspondence_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([<trimesh.Trimesh(vertices.shape=(861, 3), faces.shape=(1541, 3))>],\n",
       "       dtype=object),\n",
       " array([[   0, 1032, 1031, ...,  506,  576, 1540]]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu.split(return_value[2][\"branch_mesh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:318: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([<trimesh.Trimesh(vertices.shape=(3593, 3), faces.shape=(6980, 3))>,\n",
       "        <trimesh.Trimesh(vertices.shape=(15, 3), faces.shape=(22, 3))>,\n",
       "        <trimesh.Trimesh(vertices.shape=(3, 3), faces.shape=(1, 3))>,\n",
       "        <trimesh.Trimesh(vertices.shape=(3, 3), faces.shape=(1, 3))>],\n",
       "       dtype=object),\n",
       " array([array([   0, 4674, 4671, ..., 2335, 2334, 2333]),\n",
       "        array([4579, 4685, 4627, 4640, 2984, 4686, 4684, 4574, 4682, 4573, 4637,\n",
       "        4571, 4679, 4678, 4636, 4635, 4673, 4661, 4662, 2975, 4683, 4672]),\n",
       "        array([5276]), array([2910])], dtype=object))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu.split(curr_limb_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/traittypes/traittypes.py:101: UserWarning: Given trait value dtype \"float64\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  np.dtype(self.dtype).name))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e185aadeceac46cead8ddb3a394650c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curr_limb_mesh = su.decompress_pickle(\"curr_limb_mesh\")\n",
    "nviz.plot_objects(curr_limb_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/drawing/layout.py\u001b[0m in \u001b[0;36mfruchterman_reingold_layout\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[1;32m    487\u001b[0m                                            \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                                            dim, seed)\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-833>\u001b[0m in \u001b[0;36m_sparse_fruchterman_reingold\u001b[0;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_random_state\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mnew_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_state_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/drawing/layout.py\u001b[0m in \u001b[0;36m_sparse_fruchterman_reingold\u001b[0;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mdisplacement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mAi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;31m# update positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6ba2ba48ce12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_mesh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_mesh_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_mesh_graph\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_mesh_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'with_labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'labels'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mdraw_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_if_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx\u001b[0;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspring_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# default to spring layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mnode_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_networkx_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-829>\u001b[0m in \u001b[0;36mfruchterman_reingold_layout\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_random_state\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mnew_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_state_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/drawing/layout.py\u001b[0m in \u001b[0;36mfruchterman_reingold_layout\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdom_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         pos = _fruchterman_reingold(A, k, pos_arr, fixed, iterations,\n\u001b[0;32m--> 496\u001b[0;31m                                     threshold, dim, seed)\n\u001b[0m\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfixed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrescale_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-831>\u001b[0m in \u001b[0;36m_fruchterman_reingold\u001b[0;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_random_state\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mnew_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_state_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/drawing/layout.py\u001b[0m in \u001b[0;36m_fruchterman_reingold\u001b[0;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;31m# matrix of difference between points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;31m# distance between points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_mesh = su.decompress_pickle(\"total_mesh\")\n",
    "total_mesh_graph = su.decompress_pickle(\"total_mesh_graph\")\n",
    "total_mesh_correspondence = su.decompress_pickle(\"total_mesh_correspondence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_unmarked_faces = su.decompress_pickle(\"curr_unmarked_faces\")\n",
    "total_mesh_graph = su.decompress_pickle(\"total_mesh_graph\")\n",
    "total_mesh_correspondence = su.decompress_pickle(\"total_mesh_correspondence\")\n",
    "submesh_indices = su.decompress_pickle(\"submesh_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "new exception",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bafed5642542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new exception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mgu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCGAL_skel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No CGAL skeleton\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: new exception"
     ]
    }
   ],
   "source": [
    "import general_utils as gu\n",
    "gu = reload(gu)\n",
    "\n",
    "try:\n",
    "    raise Exception(\"new exception\")\n",
    "    raise gu.CGAL_skel_error(\"No CGAL skeleton\")\n",
    "    \n",
    "except gu.CGAL_skel_error as error: \n",
    "    print(error.msg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nviz.plot_soma_limb_concept_network(neuron_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nviz.visualize_neuron(neuron_obj,\n",
    "                      visualize_type=[\"mesh\",\"skeleton\"],\n",
    "                     limb_branch_dict=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sending the data to the Neuron Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nru = reload(nru)\n",
    "save_time = time.time()\n",
    "neuron_obj.save_compressed_neuron(output_folder=\"/notebooks/test_neurons/Fusion_decomp/\",\n",
    "                                 export_mesh=True,\n",
    "                                 suppress_output=True)\n",
    "print(f\"Save time = {time.time() - save_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_visualizations as nviz\n",
    "returned_colors = nviz.visualize_neuron(neuron_obj,\n",
    "                     visualize_type=[\"mesh\",\"skeleton\"],\n",
    "                     limb_branch_dict=dict(L6=\"all\"),\n",
    "                                       return_color_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
