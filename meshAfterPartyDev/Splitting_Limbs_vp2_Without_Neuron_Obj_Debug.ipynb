{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Clean implmentation that \n",
    "takes a fully processed neuron and is able to \n",
    "split the limbs that were accidentally included together\n",
    "\n",
    "Pseudocode\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "#sys.path.append(\"../../meshAfterParty/meshAfterParty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Need to pip install annotationframeworkclient to repair mesh with pychunkedgraph\n",
      "WARNING:root:Need to pip install annotationframeworkclient to use dataset_name parameters\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import os\n",
    "from pathlib import Path\n",
    "os.getcwd()\n",
    "\n",
    "import neuron_utils as nru\n",
    "nru = reload(nru)\n",
    "import neuron\n",
    "neuron=reload(neuron)\n",
    "import neuron_visualizations as nviz\n",
    "import time\n",
    "import system_utils as su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Test Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_file = \"/notebooks/test_neurons/meshafterparty_processed/12345_double_soma_meshafterparty\"\n",
    "neuron_obj = nru.decompress_neuron(filepath=current_file,\n",
    "                                  original_mesh=current_file,\n",
    "                                  minimal_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skeleton_utils as sk\n",
    "from pykdtree.kdtree import KDTree\n",
    "import trimesh_utils as tu\n",
    "import numpy as np\n",
    "import networkx_utils as xu\n",
    "import compartment_utils as cu\n",
    "import networkx as nx\n",
    "import numpy_utils as nu\n",
    "import copy\n",
    "import general_utils as gu\n",
    "import system_utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_neuron = neuron_obj\n",
    "nru = reload(nru)\n",
    "neuron = reload(neuron)\n",
    "current_neuron = neuron.Neuron(current_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_neuron[0].labels,current_neuron[1].labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Step 0: Prepare the containers to store the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_branch_needs_splitting(curr_limb,soma_idx,curr_soma_mesh,\n",
    "                                    significant_skeleton_threshold=30000,\n",
    "                                   print_flag=False):\n",
    "    \n",
    "    \n",
    "    #1) get the starting node:\n",
    "    curr_starting_branch_idx = curr_limb.get_starting_branch_by_soma(soma_idx)\n",
    "    #a. Get the staring branch skeleton and find the closest skeleton point to the soma border\n",
    "    curr_branch = curr_limb[curr_starting_branch_idx]\n",
    "    curr_branch_sk = curr_branch.skeleton\n",
    "    #b. find the closest skeleton point to the soma border\n",
    "    curr_soma_border = curr_limb.get_concept_network_data_by_soma(soma_idx)[\"touching_soma_vertices\"]\n",
    "    unique_skeleton_nodes = np.unique(curr_branch_sk.reshape(-1,3),axis=0)\n",
    "    \n",
    "    curr_soma_border_kdtree = KDTree(curr_soma_border)\n",
    "    distances,closest_node = curr_soma_border_kdtree.query(unique_skeleton_nodes)\n",
    "    cut_coordinate = unique_skeleton_nodes[np.argmin(distances),:]\n",
    "    \n",
    "    #c. cut the limb skeleton at that point\n",
    "    curr_limb_sk_graph = sk.convert_skeleton_to_graph(curr_limb.skeleton)\n",
    "\n",
    "    node_to_cut = xu.get_nodes_with_attributes_dict(curr_limb_sk_graph,dict(coordinates=cut_coordinate))\n",
    "\n",
    "    if len(node_to_cut) != 1:\n",
    "        raise Exception(\"Node to cut was not of length 1\")\n",
    "\n",
    "    node_to_cut = node_to_cut[0]\n",
    "    \n",
    "    #c. Seperate the graph into 2 components, If there are 2 connected components after cut, are the connected components both significant\n",
    "    curr_limb_sk_graph.remove_node(node_to_cut)\n",
    "\n",
    "    seperated_components = list(nx.connected_components(curr_limb_sk_graph))\n",
    "\n",
    "    if len(seperated_components) <= 1:\n",
    "        return None\n",
    "        #raise Exception(f\"Continue to next limb because number of seperated_components = {len(seperated_components)}\")\n",
    "\n",
    "    #c1. Seperate the graph into subgraph based on components and output the skeletons from each\n",
    "    seperated_skeletons = [sk.convert_graph_to_skeleton(curr_limb_sk_graph.subgraph(k)) for k in seperated_components]\n",
    "    skeleton_lengths = np.array([sk.calculate_skeleton_distance(k) for k in seperated_skeletons])\n",
    "\n",
    "\n",
    "    n_significant_skeletons = np.sum(skeleton_lengths>significant_skeleton_threshold)\n",
    "    if print_flag:\n",
    "        print(f\"n_significant_skeletons={n_significant_skeletons}\")\n",
    "        print(f\"skeleton_lengths = {skeleton_lengths}\")\n",
    "\n",
    "    if n_significant_skeletons < 2:\n",
    "        return None\n",
    "    else:\n",
    "        return cut_coordinate\n",
    "        #raise Exception(f\"Continue to next limb because n_significant_skeletons = {n_significant_skeletons} with lengths {skeleton_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_limb_on_soma(curr_limb,soma_idx,curr_soma_mesh,\n",
    "                       current_neuron_mesh,\n",
    "                       soma_meshes,\n",
    "                       cut_coordinate=None,\n",
    "                      print_flag=False):\n",
    "    #1) get the starting node:\n",
    "    curr_starting_branch_idx = curr_limb.get_starting_branch_by_soma(soma_idx)\n",
    "    #a. Get the staring branch skeleton and find the closest skeleton point to the soma border\n",
    "    curr_branch = curr_limb[curr_starting_branch_idx]\n",
    "    curr_branch_sk = curr_branch.skeleton\n",
    "    \n",
    "    if cut_coordinate is None:\n",
    "        if print_flag:\n",
    "            print(\"Having to recalculate the cut coordinate\")\n",
    "        \n",
    "        #b. find the closest skeleton point to the soma border\n",
    "        curr_soma_border = curr_limb.get_concept_network_data_by_soma(soma_idx)[\"touching_soma_vertices\"]\n",
    "        unique_skeleton_nodes = np.unique(curr_branch_sk.reshape(-1,3),axis=0)\n",
    "\n",
    "        curr_soma_border_kdtree = KDTree(curr_soma_border)\n",
    "        distances,closest_node = curr_soma_border_kdtree.query(unique_skeleton_nodes)\n",
    "        cut_coordinate = unique_skeleton_nodes[np.argmin(distances),:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # --------------- Part A: Finding the new split skeletons ------------------------- #\n",
    "    # Finding the node to cut on the BRANCH skeleton\n",
    "    curr_branch_sk_graph = sk.convert_skeleton_to_graph(curr_branch_sk)\n",
    "\n",
    "    if print_flag:\n",
    "        print(f\"cut_coordinate={cut_coordinate}\")\n",
    "    node_to_cut = xu.get_nodes_with_attributes_dict(curr_branch_sk_graph,dict(coordinates=cut_coordinate))\n",
    "    if len(node_to_cut) != 1:\n",
    "        raise Exception(\"Node to cut was not of length 1\")\n",
    "\n",
    "    node_to_cut = node_to_cut[0]\n",
    "    \n",
    "    G = curr_branch_sk_graph\n",
    "    endpoint_nodes = xu.get_nodes_of_degree_k(G,1)\n",
    "    endpoint_nodes_coord = xu.get_node_attributes(curr_branch_sk_graph,node_list=endpoint_nodes)\n",
    "    paths_to_endpt = [nx.dijkstra_path(G,node_to_cut,k) for k in endpoint_nodes]\n",
    "    path_lengths = np.array([len(k) for k in paths_to_endpt])\n",
    "    closest_endpoint = np.argmin(path_lengths)\n",
    "    farthest_endpoint = 1 - closest_endpoint\n",
    "    closest_endpoint_len = path_lengths[closest_endpoint]\n",
    "\n",
    "    if closest_endpoint_len <= 1:\n",
    "        #need to readjust the node_to_cut and paths\n",
    "        print(\"Having to readjust endpoint\")\n",
    "        node_to_cut = paths_to_endpt[farthest_endpoint][1]\n",
    "        paths_to_endpt = [nx.dijkstra_path(G,node_to_cut,k) for k in endpoint_nodes]\n",
    "\n",
    "    #generate the subnode in each graph\n",
    "    paths_to_endpt[farthest_endpoint].remove(node_to_cut)\n",
    "\n",
    "    subgraph_list = [G.subgraph(k) for k in paths_to_endpt]\n",
    "\n",
    "    starting_endpoints = xu.get_node_attributes(curr_branch_sk_graph,node_list=[k[0] for k in paths_to_endpt])\n",
    "\n",
    "\n",
    "    #export the skeletons of the subgraphs\n",
    "    exported_skeletons = [sk.convert_graph_to_skeleton(s) for s in subgraph_list]\n",
    "    endpoint_nodes_coord #will have the endpoints belonging to each split\n",
    "    \n",
    "    \n",
    "    # --------------- Part B: Getting Initial Mesh Correspondence ------------------------- #\n",
    "    \"\"\"\n",
    "    3) Do Mesh correspondnece to get new branch meshes for the split skeleton\n",
    "    - where have to do face resolving as well\n",
    "    4) Check that both of the meshes are touching the soma\n",
    "    5) If one of them is not touching the soma\n",
    "    --> do water growing algorithm until it is\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    div_st_branch_face_corr = []\n",
    "    div_st_branch_width = []\n",
    "    for sub_sk in exported_skeletons:\n",
    "        curr_branch_face_correspondence, width_from_skeleton = cu.mesh_correspondence_adaptive_distance(sub_sk,\n",
    "                                                  curr_branch.mesh,\n",
    "                                                 skeleton_segment_width = 1000,\n",
    "                                                 distance_by_mesh_center=True)\n",
    "        div_st_branch_face_corr.append(curr_branch_face_correspondence)\n",
    "        div_st_branch_width.append(width_from_skeleton)\n",
    "    \n",
    "    \n",
    "    divided_submeshes,divided_submeshes_idx = cu.groups_of_labels_to_resolved_labels(current_mesh = curr_branch.mesh,\n",
    "                                          face_correspondence_lists=div_st_branch_face_corr)\n",
    "    \n",
    "    \n",
    "#     # ------------ Intermediate part where intentionally messing up --------------- #    \n",
    "#     label_to_expand = 1\n",
    "\n",
    "#     #0) Turn the mesh into a graph\n",
    "#     total_mesh_graph = nx.from_edgelist(curr_branch.mesh.face_adjacency)\n",
    "\n",
    "#     #1) Get the nodes that represent the border\n",
    "#     border_vertices =  curr_limb.get_concept_network_data_by_soma(soma_idx)[\"touching_soma_vertices\"]\n",
    "#     border_faces = tu.vertices_coordinates_to_faces(curr_branch.mesh,border_vertices)\n",
    "\n",
    "#     label_face_idx = divided_submeshes_idx[label_to_expand]\n",
    "\n",
    "#     final_faces = label_face_idx.copy()\n",
    "\n",
    "#     for i in range(0,40):\n",
    "#         final_faces = np.unique(np.concatenate([xu.get_neighbors(total_mesh_graph,k) for k in final_faces]))\n",
    "\n",
    "#     other_mesh_faces = np.setdiff1d(np.arange(0,len(curr_branch.mesh.faces)),final_faces)\n",
    "    \n",
    "#     divided_submeshes_idx = [other_mesh_faces,final_faces]\n",
    "#     divided_submeshes = [curr_branch.mesh.submesh([k],append=True) for k in divided_submeshes_idx]\n",
    "    \n",
    "#     sk.graph_skeleton_and_mesh(other_meshes=list(divided_submeshes),\n",
    "#                            other_meshes_colors=[\"black\",\"red\"],\n",
    "#                           other_skeletons=exported_skeletons,\n",
    "#                           other_skeletons_colors=[\"black\",\"red\"],)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---------------- Part C: Checking that both pieces are touching the soma ------------- #\n",
    "    touching_pieces,touching_pieces_verts = tu.mesh_pieces_connectivity(main_mesh=tu.combine_meshes([curr_branch.mesh,curr_soma_mesh]),\n",
    "                           central_piece=curr_soma_mesh,\n",
    "                           periphery_pieces=divided_submeshes,\n",
    "                           merge_vertices=True,\n",
    "                            return_vertices=True,\n",
    "                           print_flag=False)\n",
    "    if print_flag:\n",
    "        print(f\"touching_pieces = {touching_pieces}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    # --------------- Part D: Doing Waterfilling Unitl Both Pieces are Touching Soma ------------- #\n",
    "    if len(touching_pieces) == 0:\n",
    "        raise Exception(\"There were none of the new meshes that were touching the soma\")\n",
    "    if len(touching_pieces) < 2:\n",
    "        #find which piece was not touching\n",
    "        label_to_expand = 1 - touching_pieces[0]\n",
    "        print(f\"new_mesh {label_to_expand} was not touching the mesh so need to expand until touches soma\")\n",
    "\n",
    "        #0) Turn the mesh into a graph\n",
    "        total_mesh_graph = nx.from_edgelist(curr_branch.mesh.face_adjacency)\n",
    "\n",
    "        #1) Get the nodes that represent the border\n",
    "        border_vertices =  curr_limb.get_concept_network_data_by_soma(soma_idx)[\"touching_soma_vertices\"]\n",
    "        border_faces = set(tu.vertices_coordinates_to_faces(curr_branch.mesh,border_vertices))\n",
    "\n",
    "        label_face_idx = divided_submeshes_idx[label_to_expand]\n",
    "\n",
    "        final_faces = label_face_idx.copy()\n",
    "\n",
    "        n_touching_soma = 0\n",
    "        counter = 0\n",
    "        while n_touching_soma < 10:\n",
    "            final_faces = np.unique(np.concatenate([xu.get_neighbors(total_mesh_graph,k) for k in final_faces]))\n",
    "            n_touching_soma = len(border_faces.intersection(set(final_faces)))\n",
    "            counter+= 1\n",
    "\n",
    "\n",
    "        other_mesh_faces = np.setdiff1d(np.arange(0,len(curr_branch.mesh.faces)),final_faces)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        print(f\"Took {counter} iterations to expand the label back\")\n",
    "\n",
    "        divided_submeshes_idx[label_to_expand] = final_faces\n",
    "        divided_submeshes_idx[touching_pieces[0]] = other_mesh_faces\n",
    "\n",
    "        #Need to fix the labels one more time to make sure the expansion did not cut off one of the labels\n",
    "        print(f\"divided_submeshes_idx = {divided_submeshes_idx}\")\n",
    "        divided_submeshes,divided_submeshes_idx = cu.groups_of_labels_to_resolved_labels(curr_branch.mesh,divided_submeshes_idx)\n",
    "\n",
    "        print(f\"divided_submeshes_idx = {divided_submeshes_idx}\")\n",
    "\n",
    "        divided_submeshes = [curr_branch.mesh.submesh([k],append=True) for k in divided_submeshes_idx]\n",
    "\n",
    "        #recalculate the border vertices and the list should be 2\n",
    "        touching_pieces,touching_pieces_verts = tu.mesh_pieces_connectivity(main_mesh=tu.combine_meshes([curr_branch.mesh,curr_soma_mesh]),\n",
    "                               central_piece=curr_soma_mesh,\n",
    "                               periphery_pieces=divided_submeshes,\n",
    "                               merge_vertices=True,\n",
    "                                return_vertices=True,\n",
    "                               print_flag=False)\n",
    "        if len(touching_pieces) != 2:\n",
    "            raise Exception(f\"Number of touching pieces not equal to 2 even after correction: {touching_pieces}\")\n",
    "\n",
    "    soma_border_verts = touching_pieces_verts\n",
    "\n",
    "#     sk.graph_skeleton_and_mesh(other_meshes=list(divided_submeshes),\n",
    "#                                other_meshes_colors=[\"black\",\"red\"],\n",
    "#                               other_skeletons=exported_skeletons,\n",
    "#                               other_skeletons_colors=[\"black\",\"red\"],\n",
    "#                               other_scatter=[endpoint_nodes_coord[0].reshape(-1,3),endpoint_nodes_coord[1].reshape(-1,3)],\n",
    "#                                other_scatter_colors=[\"black\",\"red\"],\n",
    "#                               scatter_size=1)\n",
    "\n",
    "\n",
    "    \n",
    "    # ----------------- Part E: Check that the mesh can't be split ----------------- #\n",
    "    \n",
    "    # check that the mesh can't be split\n",
    "    for j,sub in enumerate(divided_submeshes):\n",
    "        c_mesh,c_indic = tu.split(sub)\n",
    "        if len(c_mesh) > 1:\n",
    "            raise Exception(f\"New Mesh {j} had {len(c_mesh)} pieces after split\")\n",
    "\n",
    "            \n",
    "    # ----------------- Part F: Reorganize the Concept Network ----------------- #\n",
    "    neighbors_to_starting_node = xu.get_neighbors(curr_limb.concept_network,curr_starting_branch_idx)\n",
    "    \"\"\"\n",
    "    sk.graph_skeleton_and_mesh(other_meshes=[curr_limb[k].mesh for k in neighbors_to_starting_node + [curr_starting_branch_idx]],\n",
    "                              other_meshes_colors=\"random\")\n",
    "    \"\"\"\n",
    "    \n",
    "    match=dict([(k,[]) for k in neighbors_to_starting_node])\n",
    "    for ex_neighbor in neighbors_to_starting_node:\n",
    "        ex_neighbor_branch = curr_limb[ex_neighbor]\n",
    "        for j,endpt in enumerate(endpoint_nodes_coord):\n",
    "            if len(nu.matching_rows(ex_neighbor_branch.endpoints,endpt))>0:\n",
    "                match[ex_neighbor].append(j)\n",
    "\n",
    "    print(f\"match = {match}\")\n",
    "    #make sure that there was only one match\n",
    "    for k,v in match.items():\n",
    "        if len(v) != 1:\n",
    "            raise Exception(f\"Neighbor {k} did not have one matching but instead had {v}\")\n",
    "    \n",
    "   \n",
    "    concept_network_copy = copy.deepcopy(curr_limb.concept_network)\n",
    "    concept_network_copy.remove_node(curr_starting_branch_idx)\n",
    "    concept_conn_comp = list(nx.connected_components(concept_network_copy))\n",
    "\n",
    "    print(f\"match = {concept_conn_comp}\")\n",
    "    #divide up the connected components into the groups they belong to\n",
    "    new_branch_groups = [[],[]]\n",
    "    for c in concept_conn_comp:\n",
    "        #find the matching neighbor in that\n",
    "        matching_neighbor = c.intersection(set(neighbors_to_starting_node))\n",
    "        if len(matching_neighbor) != 1:\n",
    "            raise Exception(f\"matching_neighbor was not size 1 : {matching_neighbor}\")\n",
    "        matching_neighbor = list(matching_neighbor)[0]\n",
    "        new_branch_groups[match[matching_neighbor][0]].extend(list(c))\n",
    "\n",
    "    print(f\"new_branch_groups = {new_branch_groups}\")\n",
    "    if print_flag:\n",
    "        print(f\"new_branch_groups = {new_branch_groups}\")\n",
    "#     #check that the lists are not empty\n",
    "#     for i,g in enumerate(new_branch_groups):\n",
    "#         if len(g) == 0:\n",
    "#             raise Exception(f\"New branch group {i} was empty after dividing the rest of the nodes\")\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Visualize that correctly split\n",
    "#     divided_neighbor_meshes = [tu.combine_meshes([curr_limb[k].mesh for k in curr_group]) for curr_group in new_branch_groups]\n",
    "#     divided_neighbor_meshes_with_original = [tu.combine_meshes([k,v]) for k,v in zip(divided_neighbor_meshes,divided_submeshes)]\n",
    "#     #sk.graph_skeleton_and_mesh(other_meshes=)\n",
    "#     sk.graph_skeleton_and_mesh(other_meshes=divided_neighbor_meshes_with_original,\n",
    "#                               other_meshes_colors=[\"black\",\"red\"],\n",
    "#                               other_skeletons=exported_skeletons,\n",
    "#                               other_skeletons_colors=[\"black\",\"red\"],\n",
    "#                               other_scatter=[endpoint_nodes_coord[0].reshape(-1,3),endpoint_nodes_coord[1].reshape(-1,3)],\n",
    "#                                other_scatter_colors=[\"black\",\"red\"],)\n",
    "\n",
    "    \n",
    "    # ----------------- Part G: Put Everything Back into a Limb Object ----------------- #\n",
    "    new_limbs = []\n",
    "    for curr_new_branch_idx in range(len(new_branch_groups)):\n",
    "        print(f\"\\n--- Working on new limb {curr_new_branch_idx} -------\")\n",
    "        \n",
    "        #new_limb_dict[curr_new_branch_idx][\"soma_border_verts\"] = soma_border_verts[curr_new_branch_idx]\n",
    "\n",
    "        # a) Creating the new concept network\n",
    "        curr_limb_divided_skeletons =  [curr_limb[k].skeleton for k in new_branch_groups[curr_new_branch_idx]] + [exported_skeletons[curr_new_branch_idx]]\n",
    "        closest_endpoint = starting_endpoints[curr_new_branch_idx]\n",
    "        endpoints = neuron.Branch(exported_skeletons[curr_new_branch_idx]).endpoints\n",
    "        curr_limb_concept_network = nru.branches_to_concept_network(curr_limb_divided_skeletons,closest_endpoint,np.array(endpoints).reshape(-1,3),\n",
    "                                            touching_soma_vertices= soma_border_verts[curr_new_branch_idx])\n",
    "\n",
    "        #Run some checks on the new concept network developed\n",
    "        curr_starting_branch_idx= nru.check_concept_network(curr_limb_concept_network,closest_endpoint = closest_endpoint,\n",
    "                                  curr_limb_divided_skeletons=curr_limb_divided_skeletons,print_flag=True)[0]\n",
    "\n",
    "        # b) Creating the new mesh\n",
    "\n",
    "        \"\"\"Old way: \n",
    "        remaining_meshes_faces_idx =  [curr_limb[k].mesh_face_idx for k in new_branch_groups[curr_new_branch_idx]]\n",
    "        remaining_meshes_faces_idx.append(np.array(curr_branch.mesh_face_idx[divided_submeshes_idx[curr_new_branch_idx]]))\n",
    "        \"\"\"\n",
    "\n",
    "        new_limb_branch_face_idx = []\n",
    "        remaining_meshes_face_idx = []\n",
    "        total_face_count = 0\n",
    "        for k in new_branch_groups[curr_new_branch_idx]:\n",
    "            curr_face_idx  = curr_limb[k].mesh_face_idx\n",
    "            remaining_meshes_face_idx.append(curr_face_idx)\n",
    "            new_limb_branch_face_idx.append(np.arange(total_face_count,total_face_count+len(curr_face_idx)))\n",
    "            total_face_count += len(curr_face_idx)\n",
    "\n",
    "        last_face_idx = np.array(curr_branch.mesh_face_idx[divided_submeshes_idx[curr_new_branch_idx]])\n",
    "        remaining_meshes_face_idx.append(last_face_idx)\n",
    "        new_limb_branch_face_idx.append(np.arange(total_face_count,total_face_count+len(last_face_idx)))\n",
    "\n",
    "\n",
    "        final_remaining_faces = np.concatenate(remaining_meshes_face_idx)                         \n",
    "        curr_new_limb_mesh = curr_limb.mesh.submesh([final_remaining_faces],append=True,repair=False)\n",
    "\n",
    "        \"\"\"\n",
    "        Checking that it went well:\n",
    "        reovered_mesh = curr_new_limb_mesh.submesh([new_limb_branch_face_idx[2]],append=True,repair=False)\n",
    "        original_mesh = curr_limb[new_branch_groups[curr_new_branch_idx][2]].mesh\n",
    "        reovered_mesh,original_mesh\n",
    "        \"\"\"\n",
    "\n",
    "        curr_limb_correspondence = dict()\n",
    "        for j,neighb in enumerate(new_branch_groups[curr_new_branch_idx]):\n",
    "            #calculate the new mesh correspondence\n",
    "            curr_limb_correspondence[j] = dict(branch_skeleton = curr_limb[neighb].skeleton,\n",
    "                                              width_from_skeleton = curr_limb[neighb].width,\n",
    "                                              branch_mesh=curr_limb[neighb].mesh,\n",
    "                                              branch_face_idx=new_limb_branch_face_idx[j])\n",
    "        #add on the new mesh\n",
    "        curr_limb_correspondence[len(new_branch_groups[curr_new_branch_idx])] = dict(\n",
    "                        branch_skeleton = exported_skeletons[curr_new_branch_idx],\n",
    "                        width_from_skeleton = div_st_branch_width[curr_new_branch_idx],\n",
    "                        branch_mesh=divided_submeshes[curr_new_branch_idx],\n",
    "                        branch_face_idx=new_limb_branch_face_idx[-1])\n",
    "\n",
    "        # curr_limb_concept_network_dicts = [dict(starting_endpoints=endpoints,\n",
    "        #                                        starting_node=curr_starting_branch_idx,\n",
    "        #                                        starting_soma=soma_idx,\n",
    "        #                                        starting_coordinate=closest_endpoint)]\n",
    "        curr_limb_concept_network_dicts = {soma_idx:curr_limb_concept_network}\n",
    "\n",
    "        new_limb_obj = neuron.Limb(mesh = curr_new_limb_mesh,\n",
    "                                   curr_limb_correspondence=curr_limb_correspondence,\n",
    "                                   concept_network_dict=curr_limb_concept_network_dicts)\n",
    "        new_limb_obj.all_concept_network_data = nru.compute_all_concept_network_data_from_limb(new_limb_obj,\n",
    "                                                                                               current_neuron_mesh=current_neuron_mesh,\n",
    "                                                                                              soma_meshes=soma_meshes)\n",
    "\n",
    "        new_limbs.append(new_limb_obj)\n",
    "        #new_limb_dict[curr_new_branch_idx][\"curr_starting_branch_idx\"] = new_limb_obj.current_starting_node\n",
    "        \n",
    "    return new_limbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_limb_splitting(curr_limb,soma_meshes,current_neuron_mesh,significant_skeleton_threshold=30000,\n",
    "                            print_flag=False):\n",
    "    \"\"\"\n",
    "    Purpose: To split the a limb as many times as needed if connected at the soma\n",
    "    \n",
    "    Pseudocode:\n",
    "    1) Get all the somas that the limb is attached to (from all_concept_network_data)\n",
    "    2) For each soma it is attached to, check if it needs to be split:\n",
    "    \n",
    "    If yes:\n",
    "    a. Split the limb into its parts for that soma\n",
    "    b. Compute the all_concept_network_data for all of the split limbs\n",
    "    c. Start loop where send all of the limb objects through function and collect results\n",
    "    d. concatenate results and return\n",
    "    \n",
    "    if No: \n",
    "    - continue to next soma\n",
    "    \n",
    "    if No and the last soma\n",
    "    - return the limb object\n",
    "    \n",
    "    Arguments:\n",
    "    1) Limb\n",
    "    2) Soma\n",
    "    \n",
    "    Example: \n",
    "    ex_limb = current_neuron[2]\n",
    "    split_limbs = recursive_limb_splitting(current_neuron,ex_limb)\n",
    "\n",
    "    color_choices = [\"red\",\"black\"]\n",
    "    sk.graph_skeleton_and_mesh(other_meshes=[split_limbs[0].mesh,split_limbs[1].mesh],\n",
    "                               other_meshes_colors=color_choices,\n",
    "                               other_skeletons=[split_limbs[0].skeleton,split_limbs[1].skeleton],\n",
    "                               other_skeletons_colors=color_choices)\n",
    "    \"\"\"\n",
    "\n",
    "    #1) Get all the somas that the limb is attached to (from all_concept_network_data)\n",
    "    total_somas_idx = curr_limb.touching_somas()\n",
    "    total_soams_meshes = [soma_meshes[k] for k in total_somas_idx]\n",
    "    \n",
    "    if print_flag:\n",
    "        print(f\"total_somas_idx = {total_somas_idx}\")\n",
    "        print(f\"total_soams_meshes = {total_soams_meshes}\")\n",
    "    \n",
    "    #2) For each soma it is attached to, check if it needs to be split:\n",
    "    for soma_idx,curr_soma_mesh in zip(total_somas_idx,total_soams_meshes):\n",
    "        \n",
    "        cut_coordinate = check_if_branch_needs_splitting(curr_limb,soma_idx,curr_soma_mesh,\n",
    "                                   significant_skeleton_threshold=significant_skeleton_threshold,\n",
    "                                   print_flag=print_flag)\n",
    "        if print_flag:\n",
    "            print(f\"cut_coordinate = {cut_coordinate}\")\n",
    "        \n",
    "        # If No then continue to next soma\n",
    "        if cut_coordinate is None:\n",
    "            continue\n",
    "            \n",
    "        #If yes:\n",
    "        #a. Split the limb into its parts for that soma and\n",
    "        #b. Compute the all_concept_network_data for all of the split limbs\n",
    "        \n",
    "        if print_flag:\n",
    "            split_limb_objs = split_limb_on_soma(curr_limb,soma_idx,curr_soma_mesh,\n",
    "                                                 current_neuron_mesh = current_neuron_mesh,\n",
    "                                                 soma_meshes=soma_meshes,\n",
    "                                                 cut_coordinate=cut_coordinate,\n",
    "                                                print_flag=print_flag)\n",
    "        else:\n",
    "            with su.suppress_stdout_stderr():\n",
    "                split_limb_objs = split_limb_on_soma(curr_limb,soma_idx,curr_soma_mesh,\n",
    "                                                     current_neuron_mesh = current_neuron_mesh,\n",
    "                                                     soma_meshes=soma_meshes,\n",
    "                                                 cut_coordinate=cut_coordinate,\n",
    "                                                print_flag=print_flag)\n",
    "        \n",
    "        if print_flag:\n",
    "            print(f\"split_limb_objs = {split_limb_objs}\")\n",
    "        \n",
    "        total_split_limbs = []\n",
    "        for split_limb in split_limb_objs:\n",
    "            curr_results = recursive_limb_splitting(curr_limb=split_limb,\n",
    "                                                    soma_meshes=soma_meshes,\n",
    "                                                    current_neuron_mesh = current_neuron_mesh,\n",
    "                                     significant_skeleton_threshold=significant_skeleton_threshold,\n",
    "                                    print_flag=print_flag)\n",
    "            total_split_limbs = total_split_limbs + curr_results\n",
    "        return total_split_limbs\n",
    "        \n",
    "    #If Did not need to split any of then return the current limb\n",
    "    if print_flag:\n",
    "        print(\"Hit Recursive return point and returning limb\")\n",
    "    return [curr_limb]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part that will run limb splitting for all the limbs in the neuron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = reload(neuron)\n",
    "current_neuron = neuron.Neuron(current_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run it with the Neuron Object Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nru = reload(nru)\n",
    "neuron = reload(neuron)\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "def limb_split(limbs,soma_meshes,current_neuron_mesh,print_flag=False):\n",
    "    \"\"\"\n",
    "    will map the [limb_idx AS A NUMBER][branch_idx] to \n",
    "    dict_keys(['branch_skeleton', 'width_from_skeleton', 'branch_mesh', 'branch_face_idx'])\n",
    "    \"\"\"\n",
    "    new_limb_correspondence = dict() \n",
    "    \"\"\"\n",
    "    Maps Soma to who they are connected to\n",
    "    Ex: {0: [0, 1, 3, 4, 5, 9], 1: [1, 2, 6, 7, 8]}\n",
    "    \"\"\"\n",
    "    new_soma_to_piece_connectivity = dict([(k,[]) for k,v in enumerate(soma_meshes)])\n",
    "\n",
    "    \"\"\"\n",
    "    Just a list that will hold all of the meshes\n",
    "    \"\"\"\n",
    "    new_limb_meshes = []\n",
    "\n",
    "    \"\"\"\n",
    "    a dictionary that maps the limb_idx to a dictionary mapping the soma_idx to the concept map\n",
    "    {0:{0:Graph},\n",
    "     1:{0:Graph,1:Graph},\n",
    "     2:{1:Graph}....}\n",
    "\n",
    "    ** can easily get this from the limb property concept_network_data_by_soma\n",
    "    \"\"\"\n",
    "    new_limb_concept_networks = dict()\n",
    "\n",
    "    \"\"\"\n",
    "    Labels for the limbs\n",
    "    \"\"\"\n",
    "    new_limb_labels = []\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Pseudocode: \n",
    "    Iterate through each split limb\n",
    "    1) Get all of the split limbs from that one limb\n",
    "    For each limb :\n",
    "    -look at the current length of the new_limb_meshes to get the current index for limb\n",
    "    a) Add a new entry in the new_limb_correspondence by iterating over the branches\n",
    "    b) get the somas that touch the limb and add CURRENT INDEX them to the new_soma_to_piece_connectivity dictionary\n",
    "    c) Add the limb mesh to new_limb_meshes\n",
    "    d) Use the concept_network_data_by_soma attribute to get the concept_network dictionary and add to \n",
    "        new_limb_concept_networks\n",
    "    e) make new merge labels based on the number of connections in the concept_network_data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    for limb_idx,curr_limb in enumerate(limbs):\n",
    "        print(f\"\\n----- Working on Limb {limb_idx}--------\")\n",
    "#         if limb_idx != 1:\n",
    "#             continue\n",
    "        split_limbs = recursive_limb_splitting(curr_limb,soma_meshes,\n",
    "                                              current_neuron_mesh=current_neuron_mesh,\n",
    "                                              print_flag=print_flag)\n",
    "\n",
    "        print(f\"Found {len(split_limbs)} limbs after limb split\")\n",
    "\n",
    "        for sp_limb in split_limbs:\n",
    "            #-look at the current length of the new_limb_meshes to get the current index for limb\n",
    "            new_limb_idx = len(new_limb_meshes)\n",
    "            #a) Add a new entry in the new_limb_correspondence by iterating over the branches\n",
    "            new_limb_correspondence[new_limb_idx] = dict()\n",
    "            for curr_branch_idx in sp_limb.get_branch_names():\n",
    "                curr_branch = sp_limb[curr_branch_idx]\n",
    "                new_limb_correspondence[new_limb_idx][curr_branch_idx] = dict(\n",
    "                                                        branch_skeleton = curr_branch.skeleton,\n",
    "                                                        width_from_skeleton=curr_branch.width,\n",
    "                                                        branch_mesh=curr_branch.mesh,\n",
    "                                                        branch_face_idx=curr_branch.mesh_face_idx)\n",
    "            #b) get the somas that touch the limb and add CURRENT INDEX them to the new_soma_to_piece_connectivity dictionary\n",
    "            touching_somas = sp_limb.touching_somas()\n",
    "            for s in touching_somas:\n",
    "                new_soma_to_piece_connectivity[s].append(new_limb_idx)\n",
    "\n",
    "            #c) Add the limb mesh to new_limb_meshes\n",
    "            new_limb_meshes.append(sp_limb.mesh)\n",
    "\n",
    "            #d) Use the concept_network_data_by_soma attribute to get the concept_network dictionary and add to \n",
    "            #new_limb_concept_networks\n",
    "            concept_network_dict = dict()\n",
    "            for s in sp_limb.touching_somas():\n",
    "                print(f\"Finished Soma {s}\")\n",
    "                sp_limb.set_concept_network_directional(starting_soma=s)\n",
    "                concept_network_dict[s] = deepcopy(sp_limb.concept_network)\n",
    "                print(f\"concept_network_dict = {concept_network_dict}\")\n",
    "            \n",
    "                previous_starting_node = xu.get_starting_node(concept_network_dict[s],only_one=False)\n",
    "                print(f\"previous_starting_node NEW = {previous_starting_node}\")\n",
    "                print(f'sp_limb.concept_network_directional.nodes[previous_starting_node] = {concept_network_dict[s].nodes[previous_starting_node[0]] }')\n",
    "\n",
    "            new_limb_concept_networks[new_limb_idx] = concept_network_dict\n",
    "\n",
    "            #e) make new merge labels based on the number of connections in the concept_network_data\n",
    "            # OPtions: (['Normal'], ['MergeError'])\n",
    "            if len(sp_limb.concept_network_data_by_soma.keys()) > 1:\n",
    "                new_limb_labels.append(\"MergeError\")\n",
    "            else:\n",
    "                new_limb_labels.append(\"Normal\")\n",
    "    \n",
    "    return new_limb_correspondence,new_soma_to_piece_connectivity,new_limb_meshes,new_limb_concept_networks,new_limb_labels\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_limb = current_neuron[1]\n",
    "# concept_network_dict = dict()\n",
    "# for s in ex_limb.touching_somas():\n",
    "#     ex_limb.set_concept_network_directional(starting_soma=s)\n",
    "#     concept_network_dict[s] = ex_limb.concept_network\n",
    "\n",
    "# concept_network_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the preprocessed data and make a neuron out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = reload(neuron)\n",
    "limbs = [current_neuron[k] for k in current_neuron.get_limb_node_names(return_int=True)]\n",
    "soma_meshes = [current_neuron.concept_network.nodes[nru.soma_label(k)][\"data\"].mesh for k in [0,1]]\n",
    "current_neuron_mesh = current_neuron.mesh\n",
    "\n",
    "(new_limb_correspondence,\n",
    " new_soma_to_piece_connectivity,\n",
    " new_limb_meshes,\n",
    " new_limb_concept_networks,\n",
    " new_limb_labels) = limb_split(limbs,soma_meshes,current_neuron_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limb_idx = 3\n",
    "neuron= reload(neuron)\n",
    "new_limb = neuron.Limb(mesh = new_limb_meshes[limb_idx],\n",
    "                      curr_limb_correspondence=new_limb_correspondence[limb_idx],\n",
    "                      concept_network_dict=new_limb_concept_networks[limb_idx],\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_limb.skeleton.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.graph_skeleton_and_mesh(other_meshes=[new_limb.mesh],\n",
    "                           other_skeletons=[new_limb.skeleton,new_limb.skeleton[-10:]],\n",
    "                          other_skeletons_colors=[\"red\",\"black\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_limb_concept_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xu.get_all_nodes_with_certain_attribute_key(new_limb_concept_networks[1][0],\"touching_soma_vertices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xu.get_all_nodes_with_certain_attribute_key(new_limb_concept_networks[1][1],\"touching_soma_vertices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xu.get_all_nodes_with_certain_attribute_key(new_limb_concept_networks[1][1],\"starting_coordinate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_limb_concept_networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the pn function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nru = reload(nru)\n",
    "import preprocess_neuron as pn\n",
    "pn = reload(pn)\n",
    "\n",
    "limbs = [current_neuron[k] for k in current_neuron.get_limb_node_names(return_int=True)]\n",
    "soma_meshes = [current_neuron.concept_network.nodes[nru.soma_label(k)][\"data\"].mesh for k in [0,1]]\n",
    "current_neuron_mesh = current_neuron.mesh\n",
    "\n",
    "(new_limb_correspondence,\n",
    " new_soma_to_piece_connectivity,\n",
    " new_limb_meshes,\n",
    " new_limb_concept_networks,\n",
    " new_limb_labels) = limb_split(limbs,soma_meshes,current_neuron_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the incorrect concept networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import system_utils as su\n",
    "new_limb_objs= su.decompress_pickle(\"new_limb_objs.pbz2\")\n",
    "soma_meshes = su.decompress_pickle(\"soma_meshes.pbz2\")\n",
    "limb_concept_networks = su.decompress_pickle(\"limb_concept_networks.pbz2\")\n",
    "current_neuron_mesh = su.decompress_pickle(\"current_neuron.pbz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_limb = new_limb_objs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skeleton_utils as sk\n",
    "sk.graph_skeleton_and_mesh(other_meshes=[ex_limb.mesh],\n",
    "                          other_skeletons=[ex_limb.skeleton])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11: TrackedArray([[764369.4, 961579. , 877947.4],\n",
       "               [763335.2, 961687.8, 879612.2],\n",
       "               [763925.8, 960916.3, 879439.4],\n",
       "               [763378.6, 961830.4, 878169.8],\n",
       "               [764348. , 960489.9, 878697.4],\n",
       "               [763189.8, 961895.4, 879268.1],\n",
       "               [764502.8, 960665.2, 878480.4],\n",
       "               [763201.2, 961529.1, 879621.8],\n",
       "               [763385.6, 961419.9, 879647.8],\n",
       "               [764359.6, 961425.1, 878059.9],\n",
       "               [764093.4, 960579.9, 879224.1],\n",
       "               [763293.6, 961809.8, 879253.9],\n",
       "               [763290.2, 961772.8, 879492.6],\n",
       "               [763181. , 962043.1, 879046.7],\n",
       "               [764403. , 960454.9, 878594.8],\n",
       "               [763599.8, 961072.9, 879668.7],\n",
       "               [763677.2, 960913.9, 879506.8],\n",
       "               [763037.8, 961926.4, 878059.2],\n",
       "               [764585.2, 960803.8, 878456.6],\n",
       "               [763032.2, 962070.7, 878683.1],\n",
       "               [764290.6, 961644.6, 877834. ],\n",
       "               [763548.8, 961777.9, 878050.8],\n",
       "               [764521.6, 960846.1, 878329. ],\n",
       "               [763476. , 961204.6, 879701.5],\n",
       "               [763594.4, 960867.1, 879661.1],\n",
       "               [764428.4, 960596.4, 878777.5],\n",
       "               [763226.5, 962003.4, 878745.6],\n",
       "               [764536.1, 961081.2, 878176.9],\n",
       "               [763863.7, 961619.9, 877807.4],\n",
       "               [764511.1, 960590.5, 878624.8],\n",
       "               [764334.9, 960607.1, 878982.4],\n",
       "               [764185.9, 961522.5, 877700.2],\n",
       "               [763483.9, 960940.1, 879758.4],\n",
       "               [764134.9, 961703.7, 877813.1],\n",
       "               [762943.7, 962131.7, 878243.3],\n",
       "               [763057.5, 962112.2, 878500.9],\n",
       "               [764455.9, 961286.5, 878093.2],\n",
       "               [763282.9, 961887.9, 878044.6],\n",
       "               [764206.9, 960501.9, 878885.6],\n",
       "               [763760.9, 961716.8, 877951.9],\n",
       "               [763831.5, 960844.8, 879437. ],\n",
       "               [764042.1, 960726.8, 879306.7],\n",
       "               [764236.7, 961432.5, 877960.4],\n",
       "               [763242.3, 962018.7, 878959.1]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xu.get_all_nodes_with_certain_attribute_key(new_limb_concept_networks[1][0],\"touching_soma_vertices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{42: TrackedArray([[857494.2, 996403.1, 860228.6],\n",
       "               [856763.8, 995194.4, 861137. ],\n",
       "               [856815. , 995692.9, 861534.8],\n",
       "               [856808.2, 995366.9, 861323.4],\n",
       "               [857435.2, 996167.1, 861367.6],\n",
       "               [857673. , 996411.6, 861035. ],\n",
       "               [857012.2, 995560.1, 860352.4],\n",
       "               [856939.4, 995636.8, 860228.3],\n",
       "               [857700.6, 996514.1, 860354.4],\n",
       "               [857855.4, 996494.9, 860550.9],\n",
       "               [857581.2, 996336.4, 860069.7],\n",
       "               [856718.8, 995567.6, 860360.4],\n",
       "               [857359.4, 996054.1, 859950.1],\n",
       "               [856829.2, 995195.4, 860931.1],\n",
       "               [857512. , 996274.3, 861203.9],\n",
       "               [856786.2, 995564.2, 861440.3],\n",
       "               [856853.4, 995452.4, 860459. ],\n",
       "               [857039.7, 995754.9, 860137.8],\n",
       "               [857354.9, 996202.1, 859973.8],\n",
       "               [856640.5, 995209.2, 860957.8],\n",
       "               [857708.9, 996521.4, 860641.9],\n",
       "               [857928.5, 996519.5, 860442.2],\n",
       "               [857294.9, 996163.3, 861275.6],\n",
       "               [856974.1, 996008.7, 861385.9],\n",
       "               [856653.3, 995329. , 860629.3],\n",
       "               [857703.5, 996470.8, 860838.4],\n",
       "               [857210.9, 996053.2, 861475.3],\n",
       "               [857682.1, 996319.6, 861193.1],\n",
       "               [856943.7, 995780.1, 861554.6],\n",
       "               [857797.1, 996545.6, 860522.9],\n",
       "               [857249.9, 995953.9, 860056.2],\n",
       "               [856841.5, 995268.3, 860698.6],\n",
       "               [857124.9, 995842.4, 860063.1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xu.get_all_nodes_with_certain_attribute_key(new_limb_concept_networks[1][1],\"touching_soma_vertices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Working on Limb 0--------\n",
      "Found 1 limbs after limb split\n",
      "\n",
      "----- Working on Limb 1--------\n",
      "Found 1 limbs after limb split\n",
      "\n",
      "----- Working on Limb 2--------\n",
      "Found 2 limbs after limb split\n",
      "\n",
      "----- Working on Limb 3--------\n",
      "Found 1 limbs after limb split\n",
      "\n",
      "----- Working on Limb 4--------\n",
      "Found 2 limbs after limb split\n",
      "\n",
      "----- Working on Limb 5--------\n",
      "Found 1 limbs after limb split\n",
      "\n",
      "----- Working on Limb 6--------\n",
      "Found 1 limbs after limb split\n",
      "\n",
      "----- Working on Limb 7--------\n",
      "Found 1 limbs after limb split\n",
      "\n",
      "----- Working on Limb 8--------\n",
      "Found 1 limbs after limb split\n",
      "\n",
      "----- Working on Limb 9--------\n",
      "Found 1 limbs after limb split\n"
     ]
    }
   ],
   "source": [
    "nru = reload(nru)\n",
    "import preprocess_neuron as pn\n",
    "pn = reload(pn)\n",
    "\n",
    "\n",
    "new_limb_objs\n",
    "soma_meshes \n",
    "limb_concept_networks\n",
    "current_neuron_mesh\n",
    "\n",
    "(new_limb_correspondence,\n",
    " new_soma_to_piece_connectivity,\n",
    " new_limb_meshes,\n",
    " new_limb_concept_networks,\n",
    " new_limb_labels) = pn.limb_split(new_limb_objs,soma_meshes,current_neuron_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoints': array([[764206.9, 960501.9, 878885.6],\n",
       "        [783215. , 967106. , 880939. ]]),\n",
       " 'data': <neuron.Branch at 0x7f245471d518>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_limb_concept_networks[1][0].nodes[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "debug_limb = new_limb_objs[4]\n",
    "sk.graph_skeleton_and_mesh(other_meshes=[debug_limb[k].mesh],\n",
    "                           other_meshes_colors=[\"red\",\"black\"],\n",
    "                          other_skeletons=[debug_limb.skeleton])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
