{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose: Run the Soma Finding\\nAlgorithm for all the cells\\nin our final match\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: Run the Soma Finding\n",
    "Algorithm for all the cells\n",
    "in our final match\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_version = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-13 08:35:32,333 - settings - Setting database.host to at-database.ad.bcm.edu\n",
      "INFO - 2021-01-13 08:35:32,335 - settings - Setting database.user to celiib\n",
      "INFO - 2021-01-13 08:35:32,336 - settings - Setting database.password to newceliipass\n",
      "INFO - 2021-01-13 08:35:32,340 - settings - Setting stores to {'minnie65': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65', 'stage': '/mnt/dj-stor01/platinum/minnie65'}, 'meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/meshes'}, 'decimated_meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes'}, 'skeletons': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/skeletons'}}\n",
      "INFO - 2021-01-13 08:35:32,341 - settings - Setting enable_python_native_blobs to True\n",
      "INFO - 2021-01-13 08:35:32,353 - connection - Connected celiib@at-database.ad.bcm.edu:3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@at-database.ad.bcm.edu:3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-13 08:35:32,653 - settings - Setting enable_python_native_blobs to True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "\n",
    "import datajoint_utils as du\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-13 08:35:32,736 - settings - Setting database.host to at-database.ad.bcm.edu\n",
      "INFO - 2021-01-13 08:35:32,737 - settings - Setting database.user to celiib\n",
      "INFO - 2021-01-13 08:35:32,739 - settings - Setting database.password to newceliipass\n",
      "INFO - 2021-01-13 08:35:32,741 - settings - Setting enable_python_native_blobs to True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping 167 sec before conneting\n",
      "Done sleeping\n",
      "Current path for external_segmentation_path = /mnt/dj-stor01/platinum/minnie65/02\n",
      "Current path for external_mesh_path = /mnt/dj-stor01/platinum/minnie65/02/meshes\n",
      "Current path for external_decimated_mesh_path = /mnt/dj-stor01/platinum/minnie65/02/decimated_meshes\n",
      "Current path for external_skeleton_path = /mnt/dj-stor01/platinum/minnie65/02/skeletons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-13 08:35:33,062 - settings - Setting enable_python_native_blobs to True\n"
     ]
    }
   ],
   "source": [
    "import minfig\n",
    "import time\n",
    "import numpy as np\n",
    "#want to add in a wait for the connection part\n",
    "random_sleep_sec = np.random.randint(0, 200)\n",
    "print(f\"Sleeping {random_sleep_sec} sec before conneting\")\n",
    "if not test_mode:\n",
    "    time.sleep(random_sleep_sec)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "du.config_celii()\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)\n",
    "\n",
    "#configuring will include the adapters\n",
    "minnie,schema = du.configure_minnie_vm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the list of neurons to decompose for the mutli soma testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# soma_soma_table = pd.read_csv(\"Minnie65 core proofreading - Soma-Soma.csv\")\n",
    "# no_header = soma_soma_table.iloc[1:]\n",
    "# multi_soma_ids_str = no_header[\"Dendrites\"].to_numpy()\n",
    "# multi_soma_ids = multi_soma_ids_str[~np.isnan(multi_soma_ids_str.astype(\"float\"))].astype(\"int\")\n",
    "\n",
    "# @schema\n",
    "# class MultiSomaProofread(dj.Manual):\n",
    "#     definition=\"\"\"\n",
    "#     segment_id : bigint unsigned  #segment id for those to be decimated\n",
    "#     \"\"\"\n",
    "    \n",
    "# dict_of_seg = [dict(segment_id=k) for k in multi_soma_ids]\n",
    "# minnie.MultiSomaProofread.insert(dict_of_seg,skip_duplicates=True)\n",
    "# MultiSomaProofread()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_utils as nru\n",
    "import neuron\n",
    "import trimesh_utils as tu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No port chosen so picked random port 5411\n"
     ]
    }
   ],
   "source": [
    "import meshlab\n",
    "meshlab.set_meshlab_port(current_port=None)\n",
    "temporary_folder = 'decimation_temp'\n",
    "meshlab_scripts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so that it will have the adapter defined\n",
    "from datajoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class NeuronGliaNuclei(dj.Manual):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation.proj(decimation_version='version')\n",
    "    ver : decimal(6,2) #the version number of the materializaiton\n",
    "    ---\n",
    "    n_glia_faces              : int unsigned                 # The number of faces that were saved off as belonging to glia\n",
    "    glia_faces=NULL           : <faces>                      # faces indices that were saved off as belonging to glia (external storage)\n",
    "    n_nuclei_faces            : int unsigned                 # The number of faces that were saved off as belonging to nuclie\n",
    "    nuclei_faces=NULL         : <faces>                      # faces indices that were saved off as belonging to nuclei (external storage)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema.external['faces'].delete(delete_external_files=True)\n",
    "# schema.external['somas'].delete(delete_external_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minnie.BaylorSegmentCentroid.delete()\n",
    "# minnie.NeuronGliaNuclei().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decimation_version = 0\n",
    "# decimation_ratio = 0.25\n",
    "# verts_min = 10000\n",
    "\n",
    "\n",
    "# key_source =  ((minnie.Decimation & f\"n_vertices > {verts_min}\").proj(decimation_version='version') & \n",
    "#                         \"decimation_version=\" + str(decimation_version) &\n",
    "#                    f\"decimation_ratio={decimation_ratio}\") & (dj.U(\"segment_id\") & (minnie.OldBaylorSegmentCentroid() & \"multiplicity<3\").proj()\n",
    "#                                                              & (dj.U(\"segment_id\") & nucleus_table))\n",
    "# key_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "verts_min = 10000\n",
    "current_version = 30\n",
    "\n",
    "\n",
    "import trimesh_utils as tu\n",
    "import soma_extraction_utils as sm\n",
    "@schema\n",
    "class BaylorSegmentCentroid(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation.proj(decimation_version='version')\n",
    "    soma_index : tinyint unsigned #index given to this soma to account for multiple somas in one base semgnet\n",
    "    ver : decimal(6,2) #the version number of the materializaiton\n",
    "    ---\n",
    "    centroid_x=NULL           : int unsigned                 # (EM voxels)\n",
    "    centroid_y=NULL           : int unsigned                 # (EM voxels)\n",
    "    centroid_z=NULL           : int unsigned                 # (EM voxels)\n",
    "    n_vertices=NULL           : bigint                 #number of vertices\n",
    "    n_faces=NULL            : bigint                  #number of faces\n",
    "    mesh: <somas>  #datajoint adapter to get the somas mesh objects\n",
    "    multiplicity=NULL         : tinyint unsigned             # the number of somas found for this base segment\n",
    "    sdf=NULL                  : double                       # sdf width value for the soma\n",
    "    volume=NULL               : double                       # the volume in billions (10*9 nm^3) of the convex hull\n",
    "    max_side_ratio=NULL       : double                       # the maximum of the side length ratios used for check if soma\n",
    "    bbox_volume_ratio=NULL    : double                       # ratio of bbox (axis aligned) volume to mesh volume to use for check if soma\n",
    "    max_hole_length=NULL      : double                    #euclidean distance of the maximum hole size\n",
    "    run_time=NULL : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    key_source =  (((minnie.Decimation & f\"n_vertices > {verts_min}\").proj(decimation_version='version') & \n",
    "                            \"decimation_version=\" + str(decimation_version) &\n",
    "                       f\"decimation_ratio={decimation_ratio}\") & (dj.U(\"segment_id\") & (minnie.OldBaylorSegmentCentroid() & \"multiplicity<3\").proj()) & minnie.NucleiSegmentsRun2())\n",
    "                                                                 \n",
    "     \n",
    "\n",
    "    def make(self,key):\n",
    "        \"\"\"\n",
    "        Pseudocode: \n",
    "        1) Compute all of the\n",
    "        2) Save the mesh as an h5 py file\n",
    "        3) Store the saved path as the decomposition part of the dictionary and erase the vertices and faces\n",
    "        4) Insert\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #get the mesh data\n",
    "        print(f\"\\n\\n\\n---- Working on Neuron {key['segment_id']} ----\")\n",
    "        print(key)\n",
    "        new_mesh = (minnie.Decimation() & key).fetch1(\"mesh\")\n",
    "        current_mesh_verts,current_mesh_faces = new_mesh.vertices,new_mesh.faces\n",
    "\n",
    "        segment_id = key[\"segment_id\"]\n",
    "\n",
    "        (total_soma_list, \n",
    "         run_time, \n",
    "         total_soma_list_sdf,\n",
    "         glia_pieces,\n",
    "         nuclei_pieces) = sm.extract_soma_center(\n",
    "                            segment_id,\n",
    "                            current_mesh_verts,\n",
    "                            current_mesh_faces,\n",
    "            return_glia_nuclei_pieces=True,\n",
    "        )\n",
    "        \n",
    "        # -------- 1/9 Addition: Going to save off the glia and nuclei pieces ----------- #\n",
    "        \"\"\"\n",
    "        Psuedocode:\n",
    "        For both glia and nuclie pieces\n",
    "        1) If the length of array is greater than 0 --> combine the mesh and map the indices to original mesh\n",
    "        2) If not then just put None     \n",
    "        \"\"\"\n",
    "        orig_mesh = trimesh.Trimesh(vertices=current_mesh_verts,\n",
    "                                   faces=current_mesh_faces)\n",
    "        \n",
    "        if len(glia_pieces)>0:\n",
    "            glia_faces = tu.original_mesh_faces_map(orig_mesh,tu.combine_meshes(glia_pieces))\n",
    "            n_glia_faces = len(glia_faces)\n",
    "        else:\n",
    "            glia_faces = None\n",
    "            n_glia_faces = 0\n",
    "            \n",
    "        if len(nuclei_pieces)>0:\n",
    "            nuclei_faces = tu.original_mesh_faces_map(orig_mesh,tu.combine_meshes(nuclei_pieces))\n",
    "            n_nuclei_faces = len(nuclei_faces)\n",
    "        else:\n",
    "            nuclei_faces = None\n",
    "            n_nuclei_faces = 0\n",
    "            \n",
    "        # --------- saving the nuclei and glia saves\n",
    "        glia_path,nuclei_path = du.save_glia_nuclei_files(glia_faces=glia_faces,\n",
    "                                 nuclei_faces=nuclei_faces,\n",
    "                                 segment_id=segment_id)\n",
    "        \n",
    "        print(f\" glia_path = {glia_path} \\n nuclei_path = {nuclei_path}\")\n",
    "            \n",
    "        glia_nuclei_key = dict(key,\n",
    "                               ver=current_version,\n",
    "                               n_glia_faces=n_glia_faces,\n",
    "                               #glia_faces = glia_faces,\n",
    "                               glia_faces = glia_path,\n",
    "                               n_nuclei_faces = n_nuclei_faces,\n",
    "                               #nuclei_faces = nuclei_faces\n",
    "                               nuclei_faces = nuclei_path,\n",
    "                              )\n",
    "        \n",
    "        NeuronGliaNuclei.insert1(glia_nuclei_key,replace=True)\n",
    "        print(f\"Finished saving off glia and nuclei information : {glia_nuclei_key}\")\n",
    "        \n",
    "        # ---------------- End of 1/9 Addition --------------------------------- #\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Run time was {run_time} \\n    total_soma_list = {total_soma_list}\"\n",
    "             f\"\\n    with sdf values = {total_soma_list_sdf}\")\n",
    "        \n",
    "        #check if soma list is empty and did not find soma\n",
    "        if len(total_soma_list) <= 0:\n",
    "            print(\"There were no somas found for this mesh so just writing empty data\")\n",
    "            \n",
    "\n",
    "            returned_file_path = tu.write_h5_file(\n",
    "                                                vertices=np.array([]),\n",
    "                                                  faces=np.array([]),\n",
    "                                                  segment_id=segment_id,\n",
    "                                                  filename = f'{segment_id}_0.h5',\n",
    "                                                    filepath=str(du.get_somas_path())\n",
    "                                                 )\n",
    "\n",
    "            \n",
    "            \n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=0,\n",
    "                               ver=current_version,\n",
    "                              centroid_x=None,\n",
    "                               centroid_y=None,\n",
    "                               centroid_z=None,\n",
    "                               #distance_from_prediction=None,\n",
    "                               #prediction_matching_index = None,\n",
    "                               n_vertices=0,\n",
    "                               n_faces=0,\n",
    "                               mesh=returned_file_path,\n",
    "                               multiplicity=0,\n",
    "                               sdf = None,\n",
    "                               volume = None,\n",
    "                               max_side_ratio = None,\n",
    "                               bbox_volume_ratio = None,\n",
    "                               max_hole_length=None,\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            #raise Exception(\"to prevent writing because none were found\")\n",
    "            self.insert1(insert_dict,skip_duplicates=True)\n",
    "            return\n",
    "        \n",
    "        #if there is one or more soma found, get the volume and side length checks\n",
    "        max_side_ratio =  [np.max(sm.side_length_ratios(m)) for m in total_soma_list]\n",
    "        bbox_volume_ratio =  [sm.soma_volume_ratio(m) for m in total_soma_list]\n",
    "        dicts_to_insert = []\n",
    "\n",
    "\n",
    "        for i,(current_soma,soma_sdf,sz_ratio,vol_ratio) in enumerate(zip(total_soma_list,total_soma_list_sdf,max_side_ratio,bbox_volume_ratio)):\n",
    "            print(\"Trying to write off file\")\n",
    "            \"\"\" Currently don't need to export the meshes\n",
    "            current_soma.export(f\"{key['segment_id']}/{key['segment_id']}_soma_{i}.off\")\n",
    "            \"\"\"\n",
    "            auto_prediction_center = np.mean(current_soma.vertices,axis=0) / np.array([4,4,40])\n",
    "            auto_prediction_center = auto_prediction_center.astype(\"int\")\n",
    "            print(f\"Predicted Coordinates are {auto_prediction_center}\")\n",
    "            max_hole_length = tu.largest_hole_length(current_soma)\n",
    "            \n",
    "            returned_file_path = tu.write_h5_file(\n",
    "                                            vertices=current_soma.vertices,\n",
    "                                              faces=current_soma.faces,\n",
    "                                              segment_id=segment_id,\n",
    "                                              filename = f'{segment_id}_{i}.h5',\n",
    "                                                filepath=str(du.get_somas_path())\n",
    "                                             )\n",
    "\n",
    "\n",
    "\n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=i+1,\n",
    "                               ver=current_version,\n",
    "                              centroid_x=auto_prediction_center[0],\n",
    "                               centroid_y=auto_prediction_center[1],\n",
    "                               centroid_z=auto_prediction_center[2],\n",
    "                               n_vertices = len(current_soma.vertices),\n",
    "                               n_faces = len(current_soma.faces),\n",
    "                               mesh=returned_file_path,\n",
    "                               multiplicity=len(total_soma_list),\n",
    "                               sdf = np.round(soma_sdf,3),\n",
    "                               volume = current_soma.convex_hull.volume/1000000000,\n",
    "                               max_side_ratio = np.round(sz_ratio,3),\n",
    "                               bbox_volume_ratio = np.round(vol_ratio,3),\n",
    "                               max_hole_length = np.round(max_hole_length,3),\n",
    "                               run_time=np.round(run_time,4)\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "            dicts_to_insert.append(insert_dict)\n",
    "        self.insert(dicts_to_insert,skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "        <style type=\"text/css\">\n",
       "            .Relation{\n",
       "                border-collapse:collapse;\n",
       "            }\n",
       "            .Relation th{\n",
       "                background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "                font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "            }\n",
       "            .Relation td{\n",
       "                padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "            }\n",
       "            .Relation tr:nth-child(odd){\n",
       "                background: #ffffff;\n",
       "            }\n",
       "            .Relation tr:nth-child(even){\n",
       "                background: #f3f1ff;\n",
       "            }\n",
       "            /* Tooltip container */\n",
       "            .djtooltip {\n",
       "            }\n",
       "            /* Tooltip text */\n",
       "            .djtooltip .djtooltiptext {\n",
       "                visibility: hidden;\n",
       "                width: 120px;\n",
       "                background-color: black;\n",
       "                color: #fff;\n",
       "                text-align: center;\n",
       "                padding: 5px 0;\n",
       "                border-radius: 6px;\n",
       "                /* Position the tooltip text - see examples below! */\n",
       "                position: absolute;\n",
       "                z-index: 1;\n",
       "            }\n",
       "            #primary {\n",
       "                font-weight: bold;\n",
       "                color: black;\n",
       "            }\n",
       "\n",
       "            #nonprimary {\n",
       "                font-weight: normal;\n",
       "                color: white;\n",
       "            }\n",
       "\n",
       "            /* Show the tooltip text when you mouse over the tooltip container */\n",
       "            .djtooltip:hover .djtooltiptext {\n",
       "                visibility: visible;\n",
       "            }\n",
       "        </style>\n",
       "        \n",
       "        <b>job reservation table for `microns_minnie65_02`</b>\n",
       "            <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "            <table border=\"1\" class=\"Relation\">\n",
       "                <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">table_name</p>\n",
       "                                <span class=\"djtooltiptext\">className of the table</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">key_hash</p>\n",
       "                                <span class=\"djtooltiptext\">key hash</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">status</p>\n",
       "                                <span class=\"djtooltiptext\">if tuple is missing, the job is available</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">key</p>\n",
       "                                <span class=\"djtooltiptext\">structure containing the key</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">error_message</p>\n",
       "                                <span class=\"djtooltiptext\">error message returned if failed</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">error_stack</p>\n",
       "                                <span class=\"djtooltiptext\">error stack if failed</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">user</p>\n",
       "                                <span class=\"djtooltiptext\">database user</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">host</p>\n",
       "                                <span class=\"djtooltiptext\">system hostname</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">pid</p>\n",
       "                                <span class=\"djtooltiptext\">system process id</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">connection_id</p>\n",
       "                                <span class=\"djtooltiptext\">connection_id()</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">timestamp</p>\n",
       "                                <span class=\"djtooltiptext\">automatic timestamp</span>\n",
       "                            </div> </th> </tr> </thead>\n",
       "                <tbody> <tr>  </tr> </tbody>\n",
       "            </table>\n",
       "            \n",
       "            <p>Total: 0</p></div>\n",
       "            "
      ],
      "text/plain": [
       "*table_name    *key_hash    status     key        error_message  error_stac user     host     pid     connection_id  timestamp    \n",
       "+------------+ +----------+ +--------+ +--------+ +------------+ +--------+ +------+ +------+ +-----+ +------------+ +-----------+\n",
       "\n",
       " (Total: 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_table = (minnie.schema.jobs & \"table_name='__baylor_segment_centroid'\")\n",
    "curr_table#.delete()# & \"status='error'\"#.delete()\n",
    "#curr_table.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# key_hash,error_message = curr_table.fetch(\"key_hash\",\"error_message\")\n",
    "\n",
    "# df = pd.DataFrame.from_dict([dict(key_hash = k,error_message = m) for k,m in zip(key_hash,error_message)])\n",
    "# df\n",
    "# #df.columns = [\"error\",\"key_hash\"]\n",
    "# key_hashes_to_delete = df[df[\"error_message\"].str.contains(\"OSError\")][\"key_hash\"].to_numpy()\n",
    "\n",
    "# (curr_table & [dict(key_hash=k) for k in key_hashes_to_delete]).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populate Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-13 08:35:35,363 - autopopulate - Found 75674 keys to populate\n",
      "INFO - 2021-01-13 08:35:35,376 - connection - Transaction started\n",
      "INFO - 2021-01-13 08:35:35,377 - autopopulate - Populating: {'segment_id': 864691134026921572, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "---- Working on Neuron 864691134026921572 ----\n",
      "{'segment_id': 864691134026921572, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 5000.0 \n",
      "large_mesh_threshold_inner = 3250.0 \n",
      "soma_size_threshold = 562.5 \n",
      "soma_size_threshold_max = 75000.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "max_mesh_sized_filtered_away = 22500.0\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_11003.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_11003_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_915846.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_11003.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_11003_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_915846.mls is being deleted....\n",
      "There were 1 total interior meshes\n",
      "Pieces satisfying glia requirements (volume) (x >= 2500000000000): 0\n",
      "Pieces satisfying nuclie requirements: n_faces (700 <= x) and volume (x < 2500000000000) : 1\n",
      "inside remove_mesh_interior and using precomputed inside_pieces\n",
      "Removing the following inside neurons: [<trimesh.Trimesh(vertices.shape=(2440, 3), faces.shape=(4473, 3))>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:662: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original Mesh size: 22449, Final mesh size: 17976\n",
      "Total time = 1.923771619796753\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/decimation_meshlab_25465393.mls\n",
      "Total found significant pieces before Poisson = []\n",
      "Using smaller large_mesh_threshold because no significant pieces found with 5000.0\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(1903, 3), faces.shape=(4231, 3))>\n",
      "remove_inside_pieces requested \n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_5874.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_5874_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_503229.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_5874.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_5874_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_503229.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 864\n",
      "pre_largest_mesh_path = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572_decimated_largest_piece.off\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572_decimated_largest_piece.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572_decimated_largest_piece_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/poisson_592398.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:2835: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(6617, 3), faces.shape=(13230, 3))>, <trimesh.Trimesh(vertices.shape=(4626, 3), faces.shape=(9264, 3))>]\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(6617, 3), faces.shape=(13230, 3))>\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/decimation_meshlab_25200950.mls\n",
      "\n",
      "-------Splits after inner decimation len = 1--------\n",
      "\n",
      "done exporting decimated mesh: neuron_864691134026921572_decimated_largest_piece_poisson_largest_inner.off\n",
      "\n",
      "    --- On segmentation loop 0 --\n",
      "largest_mesh_path_inner_decimated_clean = <trimesh.Trimesh(vertices.shape=(1655, 3), faces.shape=(3306, 3))>\n",
      "\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0016698837280273438\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/86469113402692157200_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.47571420669555664\n",
      "2) Finished: Generating CGAL segmentation for neuron: 0.5526044368743896\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0, soma_sdf_value = 0.594532\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.0013446807861328125\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'soma', 'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 5.793571472167969e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.002242565155029297\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.017427444458007812\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.594532\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([0, 1]), array([0.594532  , 0.00521727]))\n",
      "Sizes = [3287, 19]\n",
      "soma_size_threshold = 562.5\n",
      "soma_size_threshold_max=75000.0\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_461182.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 3.500344081829777\n",
      "----- working on mesh after poisson #1: <trimesh.Trimesh(vertices.shape=(4626, 3), faces.shape=(9264, 3))>\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/neuron_864691134026921572_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134026921572/decimation_meshlab_25200950.mls\n",
      "\n",
      "-------Splits after inner decimation len = 1--------\n",
      "\n",
      "done exporting decimated mesh: neuron_864691134026921572_decimated_largest_piece_poisson_largest_inner.off\n",
      "\n",
      "    --- On segmentation loop 0 --\n",
      "largest_mesh_path_inner_decimated_clean = <trimesh.Trimesh(vertices.shape=(1152, 3), faces.shape=(2316, 3))>\n",
      "\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00022125244140625\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/86469113402692157201_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.2597365379333496\n",
      "2) Finished: Generating CGAL segmentation for neuron: 0.2979471683502197\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0]\n",
      "soma_index = -1, soma_sdf_value = 0\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.0010221004486083984\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 4.4345855712890625e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.0022547245025634766\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.011119604110717773\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([0]), array([0.3815515]))\n",
      "Sizes = [2316]\n",
      "soma_size_threshold = 562.5\n",
      "soma_size_threshold_max=75000.0\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_18035.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 18.085032831896577\n",
      "->Attempting retry of soma because failed first checks: soma_mesh = <trimesh.Trimesh(vertices.shape=(1152, 3), faces.shape=(2316, 3))>, curr_side_len_check = True, curr_volume_check = False\n",
      "Going to run cgal segmentation with:\n",
      "File: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/14_mesh \n",
      "clusters:3 \n",
      "smoothness:0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d749d65880674d5db0a86f326b824ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_632463.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 18.085032831896577\n",
      "--->This soma mesh was not added because failed retry of sphere validation:\n",
      " soma_mesh = <trimesh.Trimesh(vertices.shape=(1152, 3), faces.shape=(2316, 3))>, curr_side_len_check = True, curr_volume_check = False\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 12.692138433456421\n",
      "Before Filtering the number of somas found = 1\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_57329.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_57329_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_520144.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_57329.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_57329_fill_holes.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_520144.mls is being deleted....\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_97406.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_97406_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_387811.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_97406.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_97406_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_387811.mls is being deleted....\n",
      "Removing the following inside neurons: [<trimesh.Trimesh(vertices.shape=(386, 3), faces.shape=(560, 3))>]\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_605549.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_605549_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_314902.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_605549.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_605549_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_314902.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 12.521690088826956\n",
      "Trying backtrack segmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b4c612ac2d4f70b3055387feba0a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:1457: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  total_submeshes_idx =np.array(list(total_submeshes_idx.values()))\n",
      "INFO - 2021-01-13 08:35:58,248 - connection - Transaction committed and closed.\n",
      "INFO - 2021-01-13 08:35:58,261 - connection - Transaction started\n",
      "INFO - 2021-01-13 08:35:58,262 - autopopulate - Populating: {'segment_id': 864691134141481226, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--->This soma mesh was not added because it did not pass the sphere validation EVEN AFTER SEGMENTATION:\n",
      " soma_mesh = <trimesh.Trimesh(vertices.shape=(4951, 3), faces.shape=(9622, 3))>, curr_side_len_check = True, curr_volume_check = False\n",
      "Saved object at /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134026921572_glia.pbz2\n",
      "File size is 4.6e-05 MB\n",
      "Saved object at /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134026921572_nuclei.pbz2\n",
      "File size is 0.006805 MB\n",
      " glia_path = /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134026921572_glia.pbz2 \n",
      " nuclei_path = /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134026921572_nuclei.pbz2\n",
      "Finished saving off glia and nuclei information : {'segment_id': 864691134026921572, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25'), 'ver': 30, 'n_glia_faces': 0, 'glia_faces': PosixPath('/mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134026921572_glia.pbz2'), 'n_nuclei_faces': 4473, 'nuclei_faces': PosixPath('/mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134026921572_nuclei.pbz2')}\n",
      "Run time was 12.692137002944946 \n",
      "    total_soma_list = []\n",
      "    with sdf values = []\n",
      "There were no somas found for this mesh so just writing empty data\n",
      "\n",
      "\n",
      "\n",
      "---- Working on Neuron 864691134141481226 ----\n",
      "{'segment_id': 864691134141481226, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 5000.0 \n",
      "large_mesh_threshold_inner = 3250.0 \n",
      "soma_size_threshold = 562.5 \n",
      "soma_size_threshold_max = 75000.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "max_mesh_sized_filtered_away = 22500.0\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_59007.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_59007_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_506086.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_59007.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_59007_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_506086.mls is being deleted....\n",
      "There were 2 total interior meshes\n",
      "Pieces satisfying glia requirements (volume) (x >= 2500000000000): 0\n",
      "Pieces satisfying nuclie requirements: n_faces (700 <= x) and volume (x < 2500000000000) : 2\n",
      "inside remove_mesh_interior and using precomputed inside_pieces\n",
      "Removing the following inside neurons: [<trimesh.Trimesh(vertices.shape=(5371, 3), faces.shape=(10521, 3))>, <trimesh.Trimesh(vertices.shape=(1434, 3), faces.shape=(2684, 3))>]\n",
      "\n",
      "\n",
      "Original Mesh size: 23495, Final mesh size: 10289\n",
      "Total time = 2.1086294651031494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:662: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134141481226/neuron_864691134141481226.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134141481226/neuron_864691134141481226_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134141481226/decimation_meshlab_25217641.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-13 08:36:01,500 - connection - Transaction committed and closed.\n",
      "INFO - 2021-01-13 08:36:01,521 - connection - Transaction started\n",
      "INFO - 2021-01-13 08:36:01,524 - autopopulate - Populating: {'segment_id': 864691134219060901, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces before Poisson = []\n",
      "Using smaller large_mesh_threshold because no significant pieces found with 5000.0\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 2.858278751373291\n",
      "Before Filtering the number of somas found = 0\n",
      "Saved object at /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134141481226_glia.pbz2\n",
      "File size is 4.6e-05 MB\n",
      "Saved object at /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134141481226_nuclei.pbz2\n",
      "File size is 0.017331 MB\n",
      " glia_path = /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134141481226_glia.pbz2 \n",
      " nuclei_path = /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134141481226_nuclei.pbz2\n",
      "Finished saving off glia and nuclei information : {'segment_id': 864691134141481226, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25'), 'ver': 30, 'n_glia_faces': 0, 'glia_faces': PosixPath('/mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134141481226_glia.pbz2'), 'n_nuclei_faces': 13206, 'nuclei_faces': PosixPath('/mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134141481226_nuclei.pbz2')}\n",
      "Run time was 2.8582775592803955 \n",
      "    total_soma_list = []\n",
      "    with sdf values = []\n",
      "There were no somas found for this mesh so just writing empty data\n",
      "\n",
      "\n",
      "\n",
      "---- Working on Neuron 864691134219060901 ----\n",
      "{'segment_id': 864691134219060901, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 5000.0 \n",
      "large_mesh_threshold_inner = 3250.0 \n",
      "soma_size_threshold = 562.5 \n",
      "soma_size_threshold_max = 75000.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "max_mesh_sized_filtered_away = 22500.0\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_85099.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_85099_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_722007.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_85099.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_85099_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_722007.mls is being deleted....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:662: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 4 total interior meshes\n",
      "Pieces satisfying glia requirements (volume) (x >= 2500000000000): 0\n",
      "Pieces satisfying nuclie requirements: n_faces (700 <= x) and volume (x < 2500000000000) : 4\n",
      "inside remove_mesh_interior and using precomputed inside_pieces\n",
      "Removing the following inside neurons: [<trimesh.Trimesh(vertices.shape=(3243, 3), faces.shape=(7526, 3))>, <trimesh.Trimesh(vertices.shape=(1839, 3), faces.shape=(3776, 3))>, <trimesh.Trimesh(vertices.shape=(711, 3), faces.shape=(1235, 3))>, <trimesh.Trimesh(vertices.shape=(516, 3), faces.shape=(920, 3))>]\n",
      "\n",
      "\n",
      "Original Mesh size: 24809, Final mesh size: 11352\n",
      "Total time = 2.142230749130249\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/neuron_864691134219060901.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/neuron_864691134219060901_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/decimation_meshlab_25923466.mls\n",
      "Total found significant pieces before Poisson = []\n",
      "Using smaller large_mesh_threshold because no significant pieces found with 5000.0\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(1650, 3), faces.shape=(2686, 3))>\n",
      "remove_inside_pieces requested \n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_68386.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_68386_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_488346.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_68386.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_68386_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_488346.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 11\n",
      "pre_largest_mesh_path = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/neuron_864691134219060901_decimated_largest_piece.off\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/neuron_864691134219060901_decimated_largest_piece.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/neuron_864691134219060901_decimated_largest_piece_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/poisson_640926.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:2835: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(12950, 3), faces.shape=(25896, 3))>]\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(12950, 3), faces.shape=(25896, 3))>\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/neuron_864691134219060901_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/neuron_864691134219060901_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134219060901/decimation_meshlab_25931697.mls\n",
      "\n",
      "-------Splits after inner decimation len = 1--------\n",
      "\n",
      "done exporting decimated mesh: neuron_864691134219060901_decimated_largest_piece_poisson_largest_inner.off\n",
      "\n",
      "    --- On segmentation loop 0 --\n",
      "largest_mesh_path_inner_decimated_clean = <trimesh.Trimesh(vertices.shape=(3238, 3), faces.shape=(6472, 3))>\n",
      "\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00040411949157714844\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/86469113421906090100_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.7222304344177246\n",
      "2) Finished: Generating CGAL segmentation for neuron: 0.852149248123169\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0, soma_sdf_value = 0.7505495\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.0026340484619140625\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 4.4345855712890625e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.004320859909057617\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.03170657157897949\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.7505495\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([0]), array([0.7505495]))\n",
      "Sizes = [6472]\n",
      "soma_size_threshold = 562.5\n",
      "soma_size_threshold_max=75000.0\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [0]\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_420646.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 2.0455432492798584\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 11.79122281074524\n",
      "Before Filtering the number of somas found = 1\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_95547.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_95547_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_155426.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_95547.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_95547_fill_holes.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_155426.mls is being deleted....\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_97191.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_97191_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_77176.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_97191.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_97191_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_77176.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (300) interior meshes present\n",
      "largest is 105\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_492380.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_492380_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_235312.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_492380.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_492380_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_235312.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 2.1690153584309297\n",
      "Skipping the segmentatio filter at end\n",
      "removing mesh interior before segmentation\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_60950.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_60950_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_731468.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_60950.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_60950_fill_holes.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_731468.mls is being deleted....\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_38461.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_38461_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_293338.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_38461.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_38461_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_293338.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 105\n",
      "Doing the soma segmentation filter at end\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2630bcd7eff74f6289a30db714d62eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Largest hole before segmentation = 600770.9194123256, after = 584776.3890632885,\n",
      "ratio = 0.9733766568383786, difference = -15994.530349037144\n",
      "Saved object at /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134219060901_glia.pbz2\n",
      "File size is 4.6e-05 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:1457: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  total_submeshes_idx =np.array(list(total_submeshes_idx.values()))\n",
      "/meshAfterParty/networkx_utils.py:556: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  cycles_list_array = np.array(cycles_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved object at /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134219060901_nuclei.pbz2\n",
      "File size is 0.01484 MB\n",
      " glia_path = /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134219060901_glia.pbz2 \n",
      " nuclei_path = /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134219060901_nuclei.pbz2\n",
      "Finished saving off glia and nuclei information : {'segment_id': 864691134219060901, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25'), 'ver': 30, 'n_glia_faces': 0, 'glia_faces': PosixPath('/mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134219060901_glia.pbz2'), 'n_nuclei_faces': 13457, 'nuclei_faces': PosixPath('/mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134219060901_nuclei.pbz2')}\n",
      "Run time was 11.791221141815186 \n",
      "    total_soma_list = [<trimesh.Trimesh(vertices.shape=(5891, 3), faces.shape=(11139, 3))>]\n",
      "    with sdf values = [0.7505495]\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_48279.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_48279_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_529189.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/networkx_utils.py:556: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  cycles_list_array = np.array(cycles_list)\n",
      "INFO - 2021-01-13 08:36:36,927 - connection - Transaction committed and closed.\n",
      "INFO - 2021-01-13 08:36:36,941 - connection - Transaction started\n",
      "INFO - 2021-01-13 08:36:36,942 - autopopulate - Populating: {'segment_id': 864691134221889045, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_48279.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_48279_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_529189.mls is being deleted....\n",
      "Trying to write off file\n",
      "Predicted Coordinates are [ 84299 173386  16102]\n",
      "\n",
      "\n",
      "\n",
      "---- Working on Neuron 864691134221889045 ----\n",
      "{'segment_id': 864691134221889045, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 5000.0 \n",
      "large_mesh_threshold_inner = 3250.0 \n",
      "soma_size_threshold = 562.5 \n",
      "soma_size_threshold_max = 75000.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "max_mesh_sized_filtered_away = 22500.0\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_254.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_254_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_626420.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_254.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_254_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_626420.mls is being deleted....\n",
      "There were 1 total interior meshes\n",
      "Pieces satisfying glia requirements (volume) (x >= 2500000000000): 0\n",
      "Pieces satisfying nuclie requirements: n_faces (700 <= x) and volume (x < 2500000000000) : 1\n",
      "inside remove_mesh_interior and using precomputed inside_pieces\n",
      "Removing the following inside neurons: [<trimesh.Trimesh(vertices.shape=(3784, 3), faces.shape=(8740, 3))>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:662: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original Mesh size: 26788, Final mesh size: 18048\n",
      "Total time = 2.2416634559631348\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/neuron_864691134221889045.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/neuron_864691134221889045_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/decimation_meshlab_25322976.mls\n",
      "Total found significant pieces before Poisson = []\n",
      "Using smaller large_mesh_threshold because no significant pieces found with 5000.0\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(2078, 3), faces.shape=(3944, 3))>\n",
      "remove_inside_pieces requested \n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_17259.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_17259_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_919619.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_17259.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_17259_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_919619.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 459\n",
      "pre_largest_mesh_path = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/neuron_864691134221889045_decimated_largest_piece.off\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/neuron_864691134221889045_decimated_largest_piece.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/neuron_864691134221889045_decimated_largest_piece_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/poisson_884943.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:2835: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(13239, 3), faces.shape=(26482, 3))>]\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(13239, 3), faces.shape=(26482, 3))>\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/neuron_864691134221889045_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/neuron_864691134221889045_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134221889045/decimation_meshlab_25510293.mls\n",
      "\n",
      "-------Splits after inner decimation len = 1--------\n",
      "\n",
      "done exporting decimated mesh: neuron_864691134221889045_decimated_largest_piece_poisson_largest_inner.off\n",
      "\n",
      "    --- On segmentation loop 0 --\n",
      "largest_mesh_path_inner_decimated_clean = <trimesh.Trimesh(vertices.shape=(3303, 3), faces.shape=(6610, 3))>\n",
      "\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0007958412170410156\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/86469113422188904500_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.7651979923248291\n",
      "2) Finished: Generating CGAL segmentation for neuron: 0.8967325687408447\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 0, soma_sdf_value = 0.8435809999999999\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.0025932788848876953\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'soma', 'unsure'}\n",
      "6) Finished: Classifying Entire Neuron: 0.00017261505126953125\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.00507354736328125\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.032839059829711914\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.8435809999999999\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([0, 1, 2]), array([0.843581, 0.728142, 0.511772]))\n",
      "Sizes = [5014, 1125, 471]\n",
      "soma_size_threshold = 562.5\n",
      "soma_size_threshold_max=75000.0\n",
      "valid_soma_segments_width\n",
      "      ------ Found 2 viable somas: [0, 1]\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_714316.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_714316_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_426810.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_714316.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_714316_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_426810.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 2.847283204308603\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_165910.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_165910_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_501568.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_165910.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_165910_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_501568.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 0.026364052669931633\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 24.3924503326416\n",
      "Before Filtering the number of somas found = 2\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_37333.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_37333_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_74989.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_37333.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_37333_fill_holes.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_74989.mls is being deleted....\n",
      "xvfb-run -n 5411 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_32309.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_32309_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_462886.mls\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "sm = reload(sm)\n",
    "\n",
    "start_time = time.time()\n",
    "if not test_mode:\n",
    "    time.sleep(random.randint(0, 800))\n",
    "print('Populate Started')\n",
    "if not test_mode:\n",
    "    BaylorSegmentCentroid.populate(reserve_jobs=True, suppress_errors=True)\n",
    "else:\n",
    "    BaylorSegmentCentroid.populate(reserve_jobs=True, suppress_errors=False)\n",
    "print('Populate Done')\n",
    "\n",
    "print(f\"Total time for BaylorSegmentCentroid populate = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
