{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose: Run the Soma Finding\\nAlgorithm for all cells in the \\nmulti soma proofreading\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: Run the Soma Finding\n",
    "Algorithm for all cells in the \n",
    "multi soma proofreading\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-11 06:10:09,794 - settings - Setting database.host to at-database.ad.bcm.edu\n",
      "INFO - 2021-01-11 06:10:09,795 - settings - Setting database.user to celiib\n",
      "INFO - 2021-01-11 06:10:09,796 - settings - Setting database.password to newceliipass\n",
      "INFO - 2021-01-11 06:10:09,800 - settings - Setting stores to {'minnie65': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65', 'stage': '/mnt/dj-stor01/platinum/minnie65'}, 'meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/meshes'}, 'decimated_meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes'}, 'skeletons': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/skeletons'}}\n",
      "INFO - 2021-01-11 06:10:09,801 - settings - Setting enable_python_native_blobs to True\n",
      "INFO - 2021-01-11 06:10:09,813 - connection - Connected celiib@at-database.ad.bcm.edu:3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@at-database.ad.bcm.edu:3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-11 06:10:10,068 - settings - Setting enable_python_native_blobs to True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "\n",
    "import datajoint_utils as du\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-11 06:10:10,106 - settings - Setting database.host to at-database.ad.bcm.edu\n",
      "INFO - 2021-01-11 06:10:10,107 - settings - Setting database.user to celiib\n",
      "INFO - 2021-01-11 06:10:10,108 - settings - Setting database.password to newceliipass\n",
      "INFO - 2021-01-11 06:10:10,110 - settings - Setting enable_python_native_blobs to True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping 2 sec before conneting\n",
      "Done sleeping\n",
      "Current path for external_segmentation_path = /mnt/dj-stor01/platinum/minnie65/02\n",
      "Current path for external_mesh_path = /mnt/dj-stor01/platinum/minnie65/02/meshes\n",
      "Current path for external_decimated_mesh_path = /mnt/dj-stor01/platinum/minnie65/02/decimated_meshes\n",
      "Current path for external_skeleton_path = /mnt/dj-stor01/platinum/minnie65/02/skeletons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-11 06:10:10,368 - settings - Setting enable_python_native_blobs to True\n"
     ]
    }
   ],
   "source": [
    "import minfig\n",
    "import time\n",
    "import numpy as np\n",
    "#want to add in a wait for the connection part\n",
    "random_sleep_sec = np.random.randint(0, 200)\n",
    "print(f\"Sleeping {random_sleep_sec} sec before conneting\")\n",
    "if not test_mode:\n",
    "    time.sleep(random_sleep_sec)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "du.config_celii()\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)\n",
    "\n",
    "#configuring will include the adapters\n",
    "minnie,schema = du.configure_minnie_vm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the list of neurons to decompose for the mutli soma testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# soma_soma_table = pd.read_csv(\"Minnie65 core proofreading - Soma-Soma.csv\")\n",
    "# no_header = soma_soma_table.iloc[1:]\n",
    "# multi_soma_ids_str = no_header[\"Dendrites\"].to_numpy()\n",
    "# multi_soma_ids = multi_soma_ids_str[~np.isnan(multi_soma_ids_str.astype(\"float\"))].astype(\"int\")\n",
    "\n",
    "# @schema\n",
    "# class MultiSomaProofread(dj.Manual):\n",
    "#     definition=\"\"\"\n",
    "#     segment_id : bigint unsigned  #segment id for those to be decimated\n",
    "#     \"\"\"\n",
    "    \n",
    "# dict_of_seg = [dict(segment_id=k) for k in multi_soma_ids]\n",
    "# minnie.MultiSomaProofread.insert(dict_of_seg,skip_duplicates=True)\n",
    "# MultiSomaProofread()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_utils as nru\n",
    "import neuron\n",
    "import trimesh_utils as tu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No port chosen so picked random port 8112\n"
     ]
    }
   ],
   "source": [
    "import meshlab\n",
    "meshlab.set_meshlab_port(current_port=None)\n",
    "temporary_folder = 'decimation_temp'\n",
    "meshlab_scripts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so that it will have the adapter defined\n",
    "from datajoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class NeuronGliaNuclei(dj.Manual):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation.proj(decimation_version='version')\n",
    "    ver : decimal(6,2) #the version number of the materializaiton\n",
    "    ---\n",
    "    n_glia_faces              : int unsigned                 # The number of faces that were saved off as belonging to glia\n",
    "    glia_faces=NULL           : <faces>                      # faces indices that were saved off as belonging to glia (external storage)\n",
    "    n_nuclei_faces            : int unsigned                 # The number of faces that were saved off as belonging to nuclie\n",
    "    nuclei_faces=NULL         : <faces>                      # faces indices that were saved off as belonging to nuclei (external storage)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema.external['somas'].delete(delete_external_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minnie.BaylorSegmentCentroid.delete()\n",
    "#minnie.NeuronGliaNuclei().drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "verts_min = 10000\n",
    "current_version = 29.0\n",
    "\n",
    "\n",
    "import trimesh_utils as tu\n",
    "import soma_extraction_utils as sm\n",
    "@schema\n",
    "class BaylorSegmentCentroid(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation.proj(decimation_version='version')\n",
    "    soma_index : tinyint unsigned #index given to this soma to account for multiple somas in one base semgnet\n",
    "    ver : decimal(6,2) #the version number of the materializaiton\n",
    "    ---\n",
    "    centroid_x=NULL           : int unsigned                 # (EM voxels)\n",
    "    centroid_y=NULL           : int unsigned                 # (EM voxels)\n",
    "    centroid_z=NULL           : int unsigned                 # (EM voxels)\n",
    "    n_vertices=NULL           : bigint                 #number of vertices\n",
    "    n_faces=NULL            : bigint                  #number of faces\n",
    "    mesh: <somas>  #datajoint adapter to get the somas mesh objects\n",
    "    multiplicity=NULL         : tinyint unsigned             # the number of somas found for this base segment\n",
    "    sdf=NULL                  : double                       # sdf width value for the soma\n",
    "    volume=NULL               : double                       # the volume in billions (10*9 nm^3) of the convex hull\n",
    "    max_side_ratio=NULL       : double                       # the maximum of the side length ratios used for check if soma\n",
    "    bbox_volume_ratio=NULL    : double                       # ratio of bbox (axis aligned) volume to mesh volume to use for check if soma\n",
    "    max_hole_length=NULL      : double                    #euclidean distance of the maximum hole size\n",
    "    run_time=NULL : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \"\"\"\n",
    "    key_source =  ((minnie.Decimation & f\"n_vertices > {verts_min}\").proj(decimation_version='version') & \n",
    "                        \"decimation_version=\" + str(decimation_version) &\n",
    "                   f\"decimation_ratio={decimation_ratio}\") & minnie.MultiSomaProofread.proj()\n",
    "\n",
    "     \n",
    "\n",
    "    def make(self,key):\n",
    "        \"\"\"\n",
    "        Pseudocode: \n",
    "        1) Compute all of the\n",
    "        2) Save the mesh as an h5 py file\n",
    "        3) Store the saved path as the decomposition part of the dictionary and erase the vertices and faces\n",
    "        4) Insert\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #get the mesh data\n",
    "        print(f\"\\n\\n\\n---- Working on Neuron {key['segment_id']} ----\")\n",
    "        print(key)\n",
    "        new_mesh = (minnie.Decimation() & key).fetch1(\"mesh\")\n",
    "        current_mesh_verts,current_mesh_faces = new_mesh.vertices,new_mesh.faces\n",
    "\n",
    "        segment_id = key[\"segment_id\"]\n",
    "\n",
    "        (total_soma_list, \n",
    "         run_time, \n",
    "         total_soma_list_sdf,\n",
    "         glia_pieces,\n",
    "         nuclei_pieces) = sm.extract_soma_center(\n",
    "                            segment_id,\n",
    "                            current_mesh_verts,\n",
    "                            current_mesh_faces,\n",
    "            return_glia_nuclei_pieces=True,\n",
    "        )\n",
    "        \n",
    "        # -------- 1/9 Addition: Going to save off the glia and nuclei pieces ----------- #\n",
    "        \"\"\"\n",
    "        Psuedocode:\n",
    "        For both glia and nuclie pieces\n",
    "        1) If the length of array is greater than 0 --> combine the mesh and map the indices to original mesh\n",
    "        2) If not then just put None     \n",
    "        \"\"\"\n",
    "        orig_mesh = trimesh.Trimesh(vertices=current_mesh_verts,\n",
    "                                   faces=current_mesh_faces)\n",
    "        \n",
    "        if len(glia_pieces)>0:\n",
    "            glia_faces = tu.original_mesh_faces_map(orig_mesh,tu.combine_meshes(glia_pieces))\n",
    "            n_glia_faces = len(glia_faces)\n",
    "        else:\n",
    "            glia_faces = None\n",
    "            n_glia_faces = 0\n",
    "            \n",
    "        if len(nuclei_pieces)>0:\n",
    "            nuclei_faces = tu.original_mesh_faces_map(orig_mesh,tu.combine_meshes(nuclei_pieces))\n",
    "            n_nuclei_faces = len(nuclei_faces)\n",
    "        else:\n",
    "            nuclei_faces = None\n",
    "            n_nuclei_faces = 0\n",
    "            \n",
    "        # --------- saving the nuclei and glia saves\n",
    "        glia_path,nuclei_path = du.save_glia_nuclei_files(glia_faces=glia_faces,\n",
    "                                 nuclei_faces=nuclei_faces,\n",
    "                                 segment_id=segment_id)\n",
    "        \n",
    "            \n",
    "        glia_nuclei_key = dict(key,\n",
    "                               ver=current_version,\n",
    "                               n_glia_faces=n_glia_faces,\n",
    "                               #glia_faces = glia_faces,\n",
    "                               glia_faces = glia_path,\n",
    "                               n_nuclei_faces = n_nuclei_faces,\n",
    "                               #nuclei_faces = nuclei_faces\n",
    "                               nuclei_faces = nuclei_path,\n",
    "                              )\n",
    "        \n",
    "        NeuronGliaNuclei.insert1(glia_nuclei_key,replace=True)\n",
    "        print(f\"Finished saving off glia and nuclei information : {glia_nuclei_key}\")\n",
    "        \n",
    "        # ---------------- End of 1/9 Addition --------------------------------- #\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Run time was {run_time} \\n    total_soma_list = {total_soma_list}\"\n",
    "             f\"\\n    with sdf values = {total_soma_list_sdf}\")\n",
    "        \n",
    "        #check if soma list is empty and did not find soma\n",
    "        if len(total_soma_list) <= 0:\n",
    "            print(\"There were no somas found for this mesh so just writing empty data\")\n",
    "            \n",
    "\n",
    "            returned_file_path = tu.write_h5_file(\n",
    "                                                vertices=np.array([]),\n",
    "                                                  faces=np.array([]),\n",
    "                                                  segment_id=segment_id,\n",
    "                                                  filename = f'{segment_id}_0.h5',\n",
    "                                                    filepath=str(du.get_somas_path())\n",
    "                                                 )\n",
    "\n",
    "            \n",
    "            \n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=0,\n",
    "                               ver=current_version,\n",
    "                              centroid_x=None,\n",
    "                               centroid_y=None,\n",
    "                               centroid_z=None,\n",
    "                               #distance_from_prediction=None,\n",
    "                               #prediction_matching_index = None,\n",
    "                               n_vertices=0,\n",
    "                               n_faces=0,\n",
    "                               mesh=returned_file_path,\n",
    "                               multiplicity=0,\n",
    "                               sdf = None,\n",
    "                               volume = None,\n",
    "                               max_side_ratio = None,\n",
    "                               bbox_volume_ratio = None,\n",
    "                               max_hole_length=None,\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            #raise Exception(\"to prevent writing because none were found\")\n",
    "            self.insert1(insert_dict,skip_duplicates=True)\n",
    "            return\n",
    "        \n",
    "        #if there is one or more soma found, get the volume and side length checks\n",
    "        max_side_ratio =  [np.max(sm.side_length_ratios(m)) for m in total_soma_list]\n",
    "        bbox_volume_ratio =  [sm.soma_volume_ratio(m) for m in total_soma_list]\n",
    "        dicts_to_insert = []\n",
    "\n",
    "\n",
    "        for i,(current_soma,soma_sdf,sz_ratio,vol_ratio) in enumerate(zip(total_soma_list,total_soma_list_sdf,max_side_ratio,bbox_volume_ratio)):\n",
    "            print(\"Trying to write off file\")\n",
    "            \"\"\" Currently don't need to export the meshes\n",
    "            current_soma.export(f\"{key['segment_id']}/{key['segment_id']}_soma_{i}.off\")\n",
    "            \"\"\"\n",
    "            auto_prediction_center = np.mean(current_soma.vertices,axis=0) / np.array([4,4,40])\n",
    "            auto_prediction_center = auto_prediction_center.astype(\"int\")\n",
    "            print(f\"Predicted Coordinates are {auto_prediction_center}\")\n",
    "            max_hole_length = tu.largest_hole_length(current_soma)\n",
    "            \n",
    "            returned_file_path = tu.write_h5_file(\n",
    "                                            vertices=current_soma.vertices,\n",
    "                                              faces=current_soma.faces,\n",
    "                                              segment_id=segment_id,\n",
    "                                              filename = f'{segment_id}_{i}.h5',\n",
    "                                                filepath=str(du.get_somas_path())\n",
    "                                             )\n",
    "\n",
    "\n",
    "\n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=i+1,\n",
    "                               ver=current_version,\n",
    "                              centroid_x=auto_prediction_center[0],\n",
    "                               centroid_y=auto_prediction_center[1],\n",
    "                               centroid_z=auto_prediction_center[2],\n",
    "                               n_vertices = len(current_soma.vertices),\n",
    "                               n_faces = len(current_soma.faces),\n",
    "                               mesh=returned_file_path,\n",
    "                               multiplicity=len(total_soma_list),\n",
    "                               sdf = np.round(soma_sdf,3),\n",
    "                               volume = current_soma.convex_hull.volume/1000000000,\n",
    "                               max_side_ratio = np.round(sz_ratio,3),\n",
    "                               bbox_volume_ratio = np.round(vol_ratio,3),\n",
    "                               max_hole_length = np.round(max_hole_length,3),\n",
    "                               run_time=np.round(run_time,4)\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "            dicts_to_insert.append(insert_dict)\n",
    "        self.insert(dicts_to_insert,skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "        <style type=\"text/css\">\n",
       "            .Relation{\n",
       "                border-collapse:collapse;\n",
       "            }\n",
       "            .Relation th{\n",
       "                background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "                font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "            }\n",
       "            .Relation td{\n",
       "                padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "            }\n",
       "            .Relation tr:nth-child(odd){\n",
       "                background: #ffffff;\n",
       "            }\n",
       "            .Relation tr:nth-child(even){\n",
       "                background: #f3f1ff;\n",
       "            }\n",
       "            /* Tooltip container */\n",
       "            .djtooltip {\n",
       "            }\n",
       "            /* Tooltip text */\n",
       "            .djtooltip .djtooltiptext {\n",
       "                visibility: hidden;\n",
       "                width: 120px;\n",
       "                background-color: black;\n",
       "                color: #fff;\n",
       "                text-align: center;\n",
       "                padding: 5px 0;\n",
       "                border-radius: 6px;\n",
       "                /* Position the tooltip text - see examples below! */\n",
       "                position: absolute;\n",
       "                z-index: 1;\n",
       "            }\n",
       "            #primary {\n",
       "                font-weight: bold;\n",
       "                color: black;\n",
       "            }\n",
       "\n",
       "            #nonprimary {\n",
       "                font-weight: normal;\n",
       "                color: white;\n",
       "            }\n",
       "\n",
       "            /* Show the tooltip text when you mouse over the tooltip container */\n",
       "            .djtooltip:hover .djtooltiptext {\n",
       "                visibility: visible;\n",
       "            }\n",
       "        </style>\n",
       "        \n",
       "        <b>job reservation table for `microns_minnie65_02`</b>\n",
       "            <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "            <table border=\"1\" class=\"Relation\">\n",
       "                <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">table_name</p>\n",
       "                                <span class=\"djtooltiptext\">className of the table</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"primary\">key_hash</p>\n",
       "                                <span class=\"djtooltiptext\">key hash</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">status</p>\n",
       "                                <span class=\"djtooltiptext\">if tuple is missing, the job is available</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">key</p>\n",
       "                                <span class=\"djtooltiptext\">structure containing the key</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">error_message</p>\n",
       "                                <span class=\"djtooltiptext\">error message returned if failed</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">error_stack</p>\n",
       "                                <span class=\"djtooltiptext\">error stack if failed</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">user</p>\n",
       "                                <span class=\"djtooltiptext\">database user</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">host</p>\n",
       "                                <span class=\"djtooltiptext\">system hostname</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">pid</p>\n",
       "                                <span class=\"djtooltiptext\">system process id</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">connection_id</p>\n",
       "                                <span class=\"djtooltiptext\">connection_id()</span>\n",
       "                            </div></th><th><div class=\"djtooltip\">\n",
       "                                <p id=\"nonprimary\">timestamp</p>\n",
       "                                <span class=\"djtooltiptext\">automatic timestamp</span>\n",
       "                            </div> </th> </tr> </thead>\n",
       "                <tbody> <tr> <td>__baylor_segment_centroid</td>\n",
       "<td>5f1104c85ae36ea8b3e8dbc741019d84</td>\n",
       "<td>error</td>\n",
       "<td>=BLOB=</td>\n",
       "<td>NameError: name 'du' is not defined</td>\n",
       "<td>=BLOB=</td>\n",
       "<td>celiib@10.21.18.111</td>\n",
       "<td>71caefafdeec</td>\n",
       "<td>17621</td>\n",
       "<td>28530</td>\n",
       "<td>2021-01-10 23:56:12</td> </tr> </tbody>\n",
       "            </table>\n",
       "            \n",
       "            <p>Total: 1</p></div>\n",
       "            "
      ],
      "text/plain": [
       "*table_name    *key_hash      status     key        error_message  error_stac user           host           pid       connection_id  timestamp     \n",
       "+------------+ +------------+ +--------+ +--------+ +------------+ +--------+ +------------+ +------------+ +-------+ +------------+ +------------+\n",
       "__baylor_segme 5f1104c85ae36e error      =BLOB=     NameError: nam =BLOB=     celiib@10.21.1 71caefafdeec   17621     28530          2021-01-10 23:\n",
       " (Total: 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_table = (minnie.schema.jobs & \"table_name='__baylor_segment_centroid'\")\n",
    "curr_table#.delete()\n",
    "#curr_table.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# key_hash,error_message = curr_table.fetch(\"key_hash\",\"error_message\")\n",
    "\n",
    "# df = pd.DataFrame.from_dict([dict(key_hash = k,error_message = m) for k,m in zip(key_hash,error_message)])\n",
    "# df\n",
    "# #df.columns = [\"error\",\"key_hash\"]\n",
    "# key_hashes_to_delete = df[df[\"error_message\"].str.contains(\"OSError\")][\"key_hash\"].to_numpy()\n",
    "\n",
    "# (curr_table & [dict(key_hash=k) for k in key_hashes_to_delete]).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-11 06:10:10,558 - autopopulate - Found 152 keys to populate\n",
      "INFO - 2021-01-11 06:10:10,568 - connection - Transaction started\n",
      "INFO - 2021-01-11 06:10:10,570 - autopopulate - Populating: {'segment_id': 864691134988402042, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populate Started\n",
      "\n",
      "\n",
      "\n",
      "---- Working on Neuron 864691134988402042 ----\n",
      "{'segment_id': 864691134988402042, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 5000.0 \n",
      "large_mesh_threshold_inner = 3250.0 \n",
      "soma_size_threshold = 562.5 \n",
      "soma_size_threshold_max = 75000.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_93355.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_93355_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_21020.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_93355.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_93355_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_21020.mls is being deleted....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2021-01-11 06:11:46,460 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,461 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,469 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,469 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,526 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,529 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,682 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,992 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,994 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,997 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:46,997 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,247 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,250 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,269 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,276 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,276 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,294 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,298 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,318 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,344 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,367 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,378 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,379 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,442 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,443 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,445 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,452 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,455 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,456 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,479 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,483 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,486 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,491 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,494 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,498 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,521 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,524 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,529 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,531 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,540 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,542 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,547 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,573 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,576 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,577 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,581 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,584 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,586 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,597 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,599 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,603 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,604 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,605 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,617 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,627 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,645 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,655 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,657 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,665 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,668 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,671 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,673 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,677 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,678 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,701 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,702 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,703 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,704 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,710 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,711 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,733 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,734 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,758 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,759 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,784 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,784 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,791 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,791 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,800 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,801 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,811 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,819 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,819 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,853 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,854 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,862 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,862 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,869 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,878 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,879 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,884 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,901 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,901 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,902 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,903 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,911 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,916 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,917 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,924 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,928 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,932 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,940 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,956 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,982 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,985 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:47,991 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,007 - base - face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2021-01-11 06:11:48,012 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,015 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,018 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,032 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,033 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,037 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,038 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,045 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,049 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,170 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,197 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,212 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,213 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,216 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,217 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,259 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,259 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,275 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:48,277 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:49,008 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:49,015 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:49,108 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:49,109 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:49,110 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:11:49,111 - base - face_normals all zero, ignoring!\n",
      "/meshAfterParty/trimesh_utils.py:660: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 14 total interior meshes\n",
      "Pieces satisfying glia requirements (volume) (x >= 2500000000000): 0\n",
      "Pieces satisfying nuclie requirements: n_faces (700 <= x) and volume (x < 2500000000000) : 14\n",
      "inside remove_mesh_interior and using precomputed inside_pieces\n",
      "Removing the following inside neurons: [<trimesh.Trimesh(vertices.shape=(16119, 3), faces.shape=(33129, 3))>, <trimesh.Trimesh(vertices.shape=(1908, 3), faces.shape=(4541, 3))>, <trimesh.Trimesh(vertices.shape=(1638, 3), faces.shape=(3772, 3))>, <trimesh.Trimesh(vertices.shape=(1192, 3), faces.shape=(2759, 3))>, <trimesh.Trimesh(vertices.shape=(805, 3), faces.shape=(1866, 3))>, <trimesh.Trimesh(vertices.shape=(793, 3), faces.shape=(1776, 3))>, <trimesh.Trimesh(vertices.shape=(456, 3), faces.shape=(1000, 3))>]\n",
      "\n",
      "\n",
      "Original Mesh size: 1891203, Final mesh size: 1842325\n",
      "Total time = 82.25200939178467\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/decimation_meshlab_2540270.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2021-01-11 06:12:46,836 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:46,837 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:46,848 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:46,849 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:46,902 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:46,905 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,034 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,254 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,255 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,258 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,259 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,301 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,399 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,492 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,511 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,529 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,540 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,629 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,630 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,632 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,638 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,642 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,642 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,661 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,665 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,786 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,792 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,799 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,822 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,822 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,826 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,829 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,831 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,841 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,843 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,847 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,849 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,850 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,877 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,891 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,894 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,897 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,899 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,903 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,904 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,921 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,922 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,923 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,927 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,947 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,948 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,992 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:47,993 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,001 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,002 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,008 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,008 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,055 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,056 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,061 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,066 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,068 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,071 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,077 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,093 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,131 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,133 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,137 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,140 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,151 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,151 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,155 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,156 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,162 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,165 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,328 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,329 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,915 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,916 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,917 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:12:48,918 - base - face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found significant pieces before Poisson = [<trimesh.Trimesh(vertices.shape=(223079, 3), faces.shape=(440791, 3))>, <trimesh.Trimesh(vertices.shape=(3846, 3), faces.shape=(5570, 3))>]\n",
      "----- working on large mesh #0: <trimesh.Trimesh(vertices.shape=(223079, 3), faces.shape=(440791, 3))>\n",
      "remove_inside_pieces requested \n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_3999.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_3999_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_15541.mls\n",
      "\n",
      "---- meshlab output -----\n",
      "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
      "Current Plugins Dir is: /meshlab/src/distrib/plugins \n",
      "Error While parsing the XML filter plugin descriptors: We are trying to load a xml file that does not correspond to any dll or javascript code; please delete all the spurious xml files\n",
      "Error While parsing the XML filter plugin descriptors: We are trying to load a xml file that does not correspond to any dll or javascript code; please delete all the spurious xml files\n",
      "Opening a file with extention off\n",
      "FilterScript\n",
      "Reading filter with name Remove Duplicate Vertices\n",
      "Reading filter with name Remove Faces from Non Manifold Edges\n",
      "Reading filter with name Close Holes\n",
      "    Reading Param with name MaxHoleSize : RichInt\n",
      "    Reading Param with name Selected : RichBool\n",
      "    Reading Param with name NewFaceSelected : RichBool\n",
      "    Reading Param with name SelfIntersection : RichBool\n",
      "Loading Plugins:\n",
      "Total 104 filtering actions\n",
      "Total 1 io plugins\n",
      "Mesh /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_3999.off loaded has 223079 vn 440791 fn\n",
      "output mesh  /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_3999_fill_holes.off\n",
      "Apply FilterScript: '/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_15541.mls'\n",
      "Starting Script of 3 actionsfilter: Remove Duplicate Vertices\n",
      "no additional memory available!!! memory required: 10643388\n",
      "LOG: 2 Removed 0 duplicated vertices\n",
      "Removed 0 duplicated vertices\n",
      "filter: Remove Faces from Non Manifold Edges\n",
      "no additional memory available!!! memory required: 10643388\n",
      "LOG: 2 Successfully removed 39 non-manifold faces\n",
      "Removed 0 duplicated vertices\n",
      "Successfully removed 39 non-manifold faces\n",
      "filter: Close Holes\n",
      "no additional memory available!!! memory required: 10642920\n",
      "meshlabserver: ../../../../vcglib/vcg/complex/algorithms/hole.h:259: bool vcg::tri::TrivialEar<MESH>::Close(vcg::tri::TrivialEar<MESH>::PosType&, vcg::tri::TrivialEar<MESH>::PosType&, vcg::tri::TrivialEar<MESH>::FaceType*) [with MESH = CMeshO; vcg::tri::TrivialEar<MESH>::PosType = vcg::face::Pos<CFaceO>; typename MeshType::FaceType = CFaceO; vcg::tri::TrivialEar<MESH>::FaceType = CFaceO]: Assertion `e1.v->IsUserBit(NonManifoldBit())' failed.\n",
      "Aborted (core dumped)\n",
      "\n",
      "\n",
      " returncode ====== 134\n",
      "\n",
      " ------ Done with meshlab output------\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_15541.mls is being deleted....\n",
      "The hole closing did not work so continuing without\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_94395.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_94395_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_832667.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_94395.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_94395_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_832667.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 19\n",
      "pre_largest_mesh_path = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece.off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:2833: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ordered_comp_indices = np.array([k.astype(\"int\") for k in ordered_components])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/poisson_332311.mls\n",
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(29450, 3), faces.shape=(58900, 3))>, <trimesh.Trimesh(vertices.shape=(26157, 3), faces.shape=(52318, 3))>]\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(29450, 3), faces.shape=(58900, 3))>\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/decimation_meshlab_25346515.mls\n",
      "\n",
      "-------Splits after inner decimation len = 1--------\n",
      "\n",
      "done exporting decimated mesh: neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner.off\n",
      "largest_mesh_path_inner_decimated_clean = <trimesh.Trimesh(vertices.shape=(7361, 3), faces.shape=(14722, 3))>\n",
      "\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0008590221405029297\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/86469113498840204200_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 1.1645700931549072\n",
      "2) Finished: Generating CGAL segmentation for neuron: 1.407663106918335\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 1\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.005084514617919922\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 4.5299530029296875e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.012243032455444336\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.07038068771362305\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.0645593\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 3, 12,  2,  6,  1, 10,  0,  4,  9,  8, 11,  5,  7]), array([0.88745   , 0.175748  , 0.148738  , 0.144843  , 0.0645593 ,\n",
      "       0.0593292 , 0.05203015, 0.0378498 , 0.0373499 , 0.0367517 ,\n",
      "       0.0365113 , 0.0357959 , 0.0324139 ]))\n",
      "Sizes = [2791, 113, 469, 347, 4301, 882, 1076, 1246, 521, 875, 689, 726, 686]\n",
      "soma_size_threshold = 562.5\n",
      "soma_size_threshold_max=75000.0\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [3]\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_379748.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_379748_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_991985.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_379748.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_379748_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_991985.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 3.028841605981852\n",
      "Going to run cgal segmentation with:\n",
      "File: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/249_mesh \n",
      "clusters:3 \n",
      "smoothness:0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8a18a9bfbe43de913c32e348e7bdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Going to run cgal segmentation with:\n",
      "File: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/655_mesh \n",
      "clusters:3 \n",
      "smoothness:0.05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f3b295a50f41d2a65596cba1e7fdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Going to run cgal segmentation with:\n",
      "File: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/531_mesh \n",
      "clusters:3 \n",
      "smoothness:0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/trimesh_utils.py:1455: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  total_submeshes_idx =np.array(list(total_submeshes_idx.values()))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f75dfbf83a4a18b1c2671efdb1772c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- working on mesh after poisson #1: <trimesh.Trimesh(vertices.shape=(26157, 3), faces.shape=(52318, 3))>\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/decimation_meshlab_25346515.mls\n",
      "\n",
      "-------Splits after inner decimation len = 1--------\n",
      "\n",
      "done exporting decimated mesh: neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner.off\n",
      "largest_mesh_path_inner_decimated_clean = <trimesh.Trimesh(vertices.shape=(6535, 3), faces.shape=(13074, 3))>\n",
      "\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.0007655620574951172\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/86469113498840204201_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 1.0647737979888916\n",
      "2) Finished: Generating CGAL segmentation for neuron: 1.2929880619049072\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 4\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.004430532455444336\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 5.53131103515625e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.01143646240234375\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.06170916557312012\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.858104\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([ 4,  8, 11,  0,  6,  3,  1,  7,  2, 10,  9,  5, 12]), array([0.858104  , 0.1300715 , 0.127857  , 0.110861  , 0.0634431 ,\n",
      "       0.0582211 , 0.05335605, 0.046898  , 0.0436413 , 0.0323281 ,\n",
      "       0.0294294 , 0.0251631 , 0.0220854 ]))\n",
      "Sizes = [3459, 516, 425, 911, 1826, 731, 954, 2430, 1253, 55, 270, 203, 41]\n",
      "soma_size_threshold = 562.5\n",
      "soma_size_threshold_max=75000.0\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [4]\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_904174.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_904174_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_398584.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_904174.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_904174_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_398584.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 3.0232641440950734\n",
      "Going to run cgal segmentation with:\n",
      "File: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/573_mesh \n",
      "clusters:3 \n",
      "smoothness:0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66106ab16234f3689b399d4d448800e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_215797.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_215797_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_255165.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_215797.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_215797_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_255165.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 2.3856473225153394\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_333589.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_333589_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_637257.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_333589.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_333589_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_637257.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = -0.028372838969797132\n",
      "----- working on large mesh #1: <trimesh.Trimesh(vertices.shape=(3846, 3), faces.shape=(5570, 3))>\n",
      "remove_inside_pieces requested \n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_89496.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_89496_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_980753.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_89496.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_89496_fill_holes.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_980753.mls is being deleted....\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_37982.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_37982_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_512526.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_37982.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_37982_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_512526.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 51\n",
      "pre_largest_mesh_path = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece.off\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/poisson_332311.mls\n",
      "Total found significant pieces AFTER Poisson = [<trimesh.Trimesh(vertices.shape=(25315, 3), faces.shape=(50271, 3))>]\n",
      "----- working on mesh after poisson #0: <trimesh.Trimesh(vertices.shape=(25315, 3), faces.shape=(50271, 3))>\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner_decimated.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/864691134988402042/decimation_meshlab_25346515.mls\n",
      "\n",
      "-------Splits after inner decimation len = 1--------\n",
      "\n",
      "done exporting decimated mesh: neuron_864691134988402042_decimated_largest_piece_poisson_largest_inner.off\n",
      "largest_mesh_path_inner_decimated_clean = <trimesh.Trimesh(vertices.shape=(6457, 3), faces.shape=(12555, 3))>\n",
      "\n",
      "1) Starting: Mesh importing and Pymesh fix\n",
      "loading mesh from vertices and triangles array\n",
      "1) Finished: Mesh importing and Pymesh fix: 0.00046443939208984375\n",
      "2) Staring: Generating CGAL segmentation for neuron\n",
      "Done writing OFF file\n",
      "\n",
      "Starting CGAL segmentation\n",
      "Right before cgal segmentation, clusters = 3, smoothness = 0.2, path_and_filename = /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/86469113498840204210_fixed \n",
      "1\n",
      "Finished CGAL segmentation algorithm: 0.8058137893676758\n",
      "2) Finished: Generating CGAL segmentation for neuron: 1.0218088626861572\n",
      "3) Staring: Generating Graph Structure and Identifying Soma using soma size threshold  = 3000\n",
      "my_list_keys = [0, 1, 2]\n",
      "changed the median value\n",
      "changed the mean value\n",
      "changed the max value\n",
      "soma_index = 1\n",
      "3) Finished: Generating Graph Structure and Identifying Soma: 0.004364967346191406\n",
      "Not finding the apical because soma_only option selected\n",
      "6) Staring: Classifying Entire Neuron\n",
      "Total Labels found = {'unsure', 'soma'}\n",
      "6) Finished: Classifying Entire Neuron: 5.245208740234375e-05\n",
      "7) Staring: Transfering Segmentation Labels to Face Labels\n",
      "7) Finished: Transfering Segmentation Labels to Face Labels: 0.008495330810546875\n",
      "8) Staring: Generating final Vertex and Face Labels\n",
      "8) Finished: Generating final Vertex and Face Labels: 0.0590205192565918\n",
      "Returning the soma_sdf value AND the classifier\n",
      "soma_sdf_value = 0.4819205\n",
      "segmentation[sorted_medians],median_values[sorted_medians] = (array([2, 1, 0]), array([0.77272  , 0.4819205, 0.       ]))\n",
      "Sizes = [273, 11022, 1260]\n",
      "soma_size_threshold = 562.5\n",
      "soma_size_threshold_max=75000.0\n",
      "valid_soma_segments_width\n",
      "      ------ Found 1 viable somas: [1]\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_285172.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_285172_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_686257.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_285172.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_285172_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_686257.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 0.007522688391921114\n",
      "Going to run cgal segmentation with:\n",
      "File: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/918_mesh \n",
      "clusters:3 \n",
      "smoothness:0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3f46b38f1744a385f2e476507ad20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_300080.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_300080_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_934506.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_300080.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_300080_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_934506.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 0.006947773385506937\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_498089.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_498089_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_842856.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_498089.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_498089_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_842856.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 0.010097544398578212\n",
      "\n",
      "\n",
      "\n",
      " Total time for run = 278.2226622104645\n",
      "Before Filtering the number of somas found = 5\n",
      "Performing Soma Mesh Backtracking to original mesh\n",
      "# total split meshes = 332\n",
      "viable_meshes = [0]\n",
      "There were 331 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0]}\n",
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "current_soma_mesh_list = [<trimesh.Trimesh(vertices.shape=(1421, 3), faces.shape=(2791, 3))>]\n",
      "current_mesh = <trimesh.Trimesh(vertices.shape=(905300, 3), faces.shape=(1804598, 3))>\n",
      "\n",
      "inside Soma subtraction\n",
      "Total Time for soma mesh cancellation = 2.673\n",
      "mesh_pieces_without_soma = [<trimesh.Trimesh(vertices.shape=(529911, 3), faces.shape=(1056005, 3))>, <trimesh.Trimesh(vertices.shape=(124084, 3), faces.shape=(247560, 3))>, <trimesh.Trimesh(vertices.shape=(82518, 3), faces.shape=(164526, 3))>, <trimesh.Trimesh(vertices.shape=(81217, 3), faces.shape=(161649, 3))>, <trimesh.Trimesh(vertices.shape=(34722, 3), faces.shape=(69209, 3))>, <trimesh.Trimesh(vertices.shape=(34124, 3), faces.shape=(68043, 3))>, <trimesh.Trimesh(vertices.shape=(141, 3), faces.shape=(266, 3))>]\n",
      "Total time for Subtract Soam = 2.673771619796753\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 1.19559645652771\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_901375.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_901375_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_893101.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_901375.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_901375_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_893101.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 3.531491702719861\n",
      "Performing Soma Mesh Backtracking to original mesh\n",
      "# total split meshes = 332\n",
      "viable_meshes = [0 1]\n",
      "min_distances_to_soma = [129583.74498200919, 3470145.8132304596]\n",
      "dist_min_to_soma = [7.553145040334256, 253.45753095934145]\n",
      "There were 331 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0]}\n",
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "current_soma_mesh_list = [<trimesh.Trimesh(vertices.shape=(1284, 3), faces.shape=(2464, 3))>]\n",
      "current_mesh = <trimesh.Trimesh(vertices.shape=(905300, 3), faces.shape=(1804598, 3))>\n",
      "\n",
      "inside Soma subtraction\n",
      "Total Time for soma mesh cancellation = 2.614\n",
      "mesh_pieces_without_soma = [<trimesh.Trimesh(vertices.shape=(548160, 3), faces.shape=(1092797, 3))>, <trimesh.Trimesh(vertices.shape=(181129, 3), faces.shape=(360711, 3))>, <trimesh.Trimesh(vertices.shape=(74694, 3), faces.shape=(148830, 3))>, <trimesh.Trimesh(vertices.shape=(61816, 3), faces.shape=(123127, 3))>, <trimesh.Trimesh(vertices.shape=(22661, 3), faces.shape=(45254, 3))>]\n",
      "Total time for Subtract Soam = 2.6145858764648438\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 1.0486187934875488\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_628887.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_628887_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_939931.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_628887.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_628887_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_939931.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = 2.7736252547504465\n",
      "Performing Soma Mesh Backtracking to original mesh\n",
      "# total split meshes = 332\n",
      "viable_meshes = [0]\n",
      "There were 331 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0]}\n",
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "current_soma_mesh_list = [<trimesh.Trimesh(vertices.shape=(436, 3), faces.shape=(792, 3))>]\n",
      "current_mesh = <trimesh.Trimesh(vertices.shape=(905300, 3), faces.shape=(1804598, 3))>\n",
      "\n",
      "inside Soma subtraction\n",
      "Total Time for soma mesh cancellation = 2.857\n",
      "mesh_pieces_without_soma = [<trimesh.Trimesh(vertices.shape=(723352, 3), faces.shape=(1441743, 3))>, <trimesh.Trimesh(vertices.shape=(97079, 3), faces.shape=(193361, 3))>, <trimesh.Trimesh(vertices.shape=(79258, 3), faces.shape=(157963, 3))>]\n",
      "Total time for Subtract Soam = 2.8578038215637207\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 1.02642822265625\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_23409.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_23409_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_229330.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_23409.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_23409_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_229330.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = -0.05110417475478518\n",
      "Performing Soma Mesh Backtracking to original mesh\n",
      "# total split meshes = 332\n",
      "viable_meshes = [0 1]\n",
      "min_distances_to_soma = [3679785.8872798956, 2271609.0567480274]\n",
      "dist_min_to_soma = [14.82497892070062, 2.167948338894729]\n",
      "There were 331 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {1: [0]}\n",
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "current_soma_mesh_list = [<trimesh.Trimesh(vertices.shape=(5122, 3), faces.shape=(9994, 3))>]\n",
      "current_mesh = <trimesh.Trimesh(vertices.shape=(8417, 3), faces.shape=(18231, 3))>\n",
      "\n",
      "inside Soma subtraction\n",
      "Inside split_significant_pieces and was passed empty mesh so retruning empty list\n",
      "Total Time for soma mesh cancellation = 0.04\n",
      "mesh_pieces_without_soma = []\n",
      "--->This soma mesh was not added because Was not able to backtrack soma to mesh\n",
      "Performing Soma Mesh Backtracking to original mesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/meshAfterParty/soma_extraction_utils.py\", line 1076, in extract_soma_center\n",
      "    sig_th_initial_split=15)[0]\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total split meshes = 332\n",
      "viable_meshes = [0]\n",
      "There were 331 pieces found after size threshold\n",
      "# of soma containing seperate meshes = 1\n",
      "meshes with somas = {0: [0]}\n",
      "\n",
      "\n",
      "----Working on soma-containing mesh piece 0----\n",
      "current_soma_mesh_list = [<trimesh.Trimesh(vertices.shape=(575, 3), faces.shape=(1028, 3))>]\n",
      "current_mesh = <trimesh.Trimesh(vertices.shape=(905300, 3), faces.shape=(1804598, 3))>\n",
      "\n",
      "inside Soma subtraction\n",
      "Total Time for soma mesh cancellation = 2.731\n",
      "mesh_pieces_without_soma = [<trimesh.Trimesh(vertices.shape=(904925, 3), faces.shape=(1803779, 3))>]\n",
      "Total time for Subtract Soam = 2.732171058654785\n",
      "Total time for Original_mesh_faces_map for mesh_pieces without soma= 1.7676324844360352\n",
      "poisson_backtrack_distance_threshold = None\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_15447.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_15447_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_618888.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_15447.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_15447_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_618888.mls is being deleted....\n",
      "Inside sphere validater: ratio_val = -0.047386709291341915\n",
      "Saved object at /notebooks/Auto_Proofreading/Preprocessing_Pipeline/filtered_soma_list.pbz2\n",
      "File size is 4.696556 MB\n",
      "Skipping the segmentatio filter at end\n",
      "removing mesh interior before segmentation\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_49804.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_49804_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_982057.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_49804.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_49804_fill_holes.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_982057.mls is being deleted....\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_75638.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_75638_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_633872.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_75638.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_75638_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_633872.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 35\n",
      "Doing the soma segmentation filter at end\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d70c63f6b04b6e928a7a9cde56a8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Largest hole before segmentation = 150463.99676767556, after = 316700.61996373546,\n",
      "ratio = 2.1048265815558396, difference = 166236.6231960599\n",
      "Skipping the segmentatio filter at end\n",
      "removing mesh interior before segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/networkx_utils.py:556: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  cycles_list_array = np.array(cycles_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_42330.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_42330_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_815096.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_42330.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_42330_fill_holes.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_815096.mls is being deleted....\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_91002.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_91002_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_181318.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_91002.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_91002_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_181318.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 835\n",
      "Doing the soma segmentation filter at end\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49831306532c4ac3b8be0baa145dba88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Largest hole before segmentation = 1552982.6084934934, after = 1586146.1108715048,\n",
      "ratio = 1.0213547158845406, difference = 33163.502378011355\n",
      "Skipping the segmentatio filter at end\n",
      "removing mesh interior before segmentation\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_99495.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_99495_fill_holes.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_147672.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_99495.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_99495_fill_holes.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/fill_holes_147672.mls is being deleted....\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_36945.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_36945_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_310234.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_36945.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_36945_remove_interior.off\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_310234.mls is being deleted....\n",
      "THERE WERE NO MESH PIECES GREATER THAN THE significance_threshold\n",
      "No significant (1000) interior meshes present\n",
      "largest is 20\n",
      "Doing the soma segmentation filter at end\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811802783b144cafb1fbf5c3713e0edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Largest hole before segmentation = 1580569.3112116766, after = 834842.8503978192,\n",
      "ratio = 0.5281912311443161, difference = -745726.4608138574\n",
      "Skipping the segmentatio filter at end\n",
      "Soma (size = 440, width=0.32) did not pass thresholds (size threshold=2000, width threshold = 0.32) \n",
      "Saved object at /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134988402042_glia.pbz2\n",
      "File size is 4.6e-05 MB\n",
      "Saved object at /mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134988402042_nuclei.pbz2\n",
      "File size is 0.071121 MB\n",
      "Finished saving off glia and nuclei information : {'segment_id': 864691134988402042, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25'), 'ver': 29.0, 'n_glia_faces': 0, 'glia_faces': PosixPath('/mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134988402042_glia.pbz2'), 'n_nuclei_faces': 48878, 'nuclei_faces': PosixPath('/mnt/dj-stor01/platinum/minnie65/02/glia_nuclei_faces/864691134988402042_nuclei.pbz2')}\n",
      "Run time was 278.22266006469727 \n",
      "    total_soma_list = [<trimesh.Trimesh(vertices.shape=(18784, 3), faces.shape=(37233, 3))>, <trimesh.Trimesh(vertices.shape=(21864, 3), faces.shape=(45198, 3))>]\n",
      "    with sdf values = [0.88745 0.66216]\n",
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_817666.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_817666_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_154258.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_817666.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_817666_poisson.off\n",
      "mesh.is_watertight = True\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_154258.mls is being deleted....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2021-01-11 06:17:53,358 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,363 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,364 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,365 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,380 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,381 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,382 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,407 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,453 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,479 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,479 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,518 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,555 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,692 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,693 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,719 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,720 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,739 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,740 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,915 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,921 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,922 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,923 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,940 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,942 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,942 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:53,970 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,015 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,041 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,042 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,079 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,113 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,347 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,349 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,376 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,377 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,398 - base - face_normals all zero, ignoring!\n",
      "WARNING - 2021-01-11 06:17:54,399 - base - face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Poisson Surface Reconstruction for watertightness in soma_volume_ratio\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_72539.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_72539_poisson.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_986692.mls\n",
      "removed temporary input file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_72539.off\n",
      "removed temporary output file: /notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/neuron_72539_poisson.off\n",
      "mesh.is_watertight = False\n",
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/Poisson_temp/poisson_986692.mls is being deleted....\n",
      "Trying to write off file\n",
      "Predicted Coordinates are [334479 152621  20511]\n",
      "Trying to write off file\n",
      "Predicted Coordinates are [340407 183329  21421]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/meshAfterParty/networkx_utils.py:556: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  cycles_list_array = np.array(cycles_list)\n",
      "INFO - 2021-01-11 06:18:06,164 - connection - Transaction committed and closed.\n",
      "INFO - 2021-01-11 06:18:06,184 - connection - Transaction started\n",
      "INFO - 2021-01-11 06:18:06,187 - autopopulate - Populating: {'segment_id': 864691134988472442, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "---- Working on Neuron 864691134988472442 ----\n",
      "{'segment_id': 864691134988472442, 'decimation_version': 0, 'decimation_ratio': Decimal('0.25')}\n",
      "Current Arguments Using (adjusted for decimation):\n",
      " large_mesh_threshold= 5000.0 \n",
      "large_mesh_threshold_inner = 3250.0 \n",
      "soma_size_threshold = 562.5 \n",
      "soma_size_threshold_max = 75000.0\n",
      "outer_decimation_ratio = 0.25\n",
      "inner_decimation_ratio = 0.25\n",
      "xvfb-run -n 8112 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_5542.off -o /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/neuron_5542_remove_interior.off -s /notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_4708.mls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-01-11 06:19:10,965 - connection - Transaction cancelled. Rolling back ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Auto_Proofreading/Preprocessing_Pipeline/temp/remove_interior_4708.mls is being deleted....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-af108618b9c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mBaylorSegmentCentroid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreserve_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mBaylorSegmentCentroid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreserve_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Populate Done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datajoint/autopopulate.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, suppress_errors, return_exception_objects, reserve_jobs, order, limit, max_calls, display_progress, *restrictions)\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_insert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                         \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0b29f25ad07f>\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     62\u001b[0m                             \u001b[0mcurrent_mesh_verts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                             \u001b[0mcurrent_mesh_faces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mreturn_glia_nuclei_pieces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/meshAfterParty/soma_extraction_utils.py\u001b[0m in \u001b[0;36mextract_soma_center\u001b[0;34m(segment_id, current_mesh_verts, current_mesh_faces, outer_decimation_ratio, large_mesh_threshold, large_mesh_threshold_inner, soma_width_threshold, soma_size_threshold, inner_decimation_ratio, volume_mulitplier, side_length_ratio_threshold, soma_size_threshold_max, delete_files, backtrack_soma_mesh_to_original, boundary_vertices_threshold, poisson_backtrack_distance_threshold, close_holes, remove_inside_pieces, size_threshold_to_remove, pymeshfix_clean, check_holes_before_pymeshfix, second_poisson, segmentation_at_end, last_size_threshold, largest_hole_threshold, max_fail_loops, perform_pairing, verbose, return_glia_nuclei_pieces, backtrack_soma_size_threshold)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0mrecov_orig_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrimesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_mesh_verts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_mesh_faces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m     \u001b[0mrecov_orig_mesh_no_interior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglia_pieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnuclei_pieces\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_nuclei_and_glia_meshes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecov_orig_mesh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;31m#recov_orig_mesh_no_interior = tu.remove_mesh_interior(recov_orig_mesh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/meshAfterParty/trimesh_utils.py\u001b[0m in \u001b[0;36mremove_nuclei_and_glia_meshes\u001b[0;34m(mesh, glia_volume_threshold, glia_n_faces_threshold, glia_n_faces_min, nucleus_min, nucleus_max, connectivity, try_hole_close, verbose, return_glia_nucleus_pieces, **kwargs)\u001b[0m\n\u001b[1;32m   3303\u001b[0m     curr_interior_mesh_non_split = mesh_interior(mesh,return_interior=True,\n\u001b[1;32m   3304\u001b[0m                                        \u001b[0mtry_hole_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3305\u001b[0;31m                                        **kwargs)\n\u001b[0m\u001b[1;32m   3306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m     curr_interior_mesh = tu.split_significant_pieces(curr_interior_mesh_non_split,significance_threshold=nucleus_min,\n",
      "\u001b[0;32m/meshAfterParty/trimesh_utils.py\u001b[0m in \u001b[0;36mmesh_interior\u001b[0;34m(mesh, return_interior, quality_max, try_hole_close, max_hole_size, self_itersect_faces, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m                                              \u001b[0mfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m                                              \u001b[0mreturn_mesh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2591\u001b[0;31m                                              \u001b[0mdelete_temp_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2592\u001b[0m                                             )\n\u001b[1;32m   2593\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmesh_remove_interior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/meshAfterParty/meshlab.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, vertices, faces, segment_id, return_mesh, input_mesh_path, mesh_filename, printout, delete_temp_files, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0minput_mesh_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0moutput_mesh_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0mprintout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprintout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         )\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/meshAfterParty/meshlab.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_mesh_path, output_mesh_path, printout, random_port, port_number)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprintout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_to_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0msubprocess_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_to_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;31m#subprocess_result = subprocess.run(command_to_run, shell=True,stdout=PIPE, stderr=STDOUT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubprocess_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "sm = reload(sm)\n",
    "\n",
    "start_time = time.time()\n",
    "if not test_mode:\n",
    "    time.sleep(random.randint(0, 800))\n",
    "print('Populate Started')\n",
    "if not test_mode:\n",
    "    BaylorSegmentCentroid.populate(reserve_jobs=True, suppress_errors=True)\n",
    "else:\n",
    "    BaylorSegmentCentroid.populate(reserve_jobs=True, suppress_errors=False)\n",
    "print('Populate Done')\n",
    "\n",
    "print(f\"Total time for BaylorSegmentCentroid populate = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
