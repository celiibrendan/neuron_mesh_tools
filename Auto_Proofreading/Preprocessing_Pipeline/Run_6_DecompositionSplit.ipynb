{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: To decompose the multi-somas for splitting\n",
    "using the new decomposition method\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "\n",
    "import datajoint_utils as du\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so that it will have the adapter defined\n",
    "from datajoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the contains method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import system_utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minfig\n",
    "import time\n",
    "import numpy as np\n",
    "#want to add in a wait for the connection part\n",
    "random_sleep_sec = np.random.randint(0, 200)\n",
    "print(f\"Sleeping {random_sleep_sec} sec before conneting\")\n",
    "if not test_mode:\n",
    "    time.sleep(random_sleep_sec)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "du.config_celii()\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)\n",
    "\n",
    "#configuring will include the adapters\n",
    "minnie,schema = du.configure_minnie_vm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_utils as nru\n",
    "import neuron\n",
    "import trimesh_utils as tu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshlab\n",
    "meshlab.set_meshlab_port(current_port=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minnie.DecompositionSplit.drop()\n",
    "# schema.external['decomposition'].delete(delete_external_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_source = minnie.Decomposition() & \"n_somas>1 OR n_error_limbs>0\"\n",
    "key_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import proofreading_utils as pru\n",
    "\n",
    "split_version = 0\n",
    "split_version = 1 #fixed the problem with split from suggestions\n",
    "\n",
    "verbose = True\n",
    "\n",
    "@schema\n",
    "class DecompositionSplit(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decomposition()\n",
    "    split_index: tinyint unsigned  #the index of the neuron object that resulted AFTER THE SPLITTING ALGORITHM\n",
    "    split_version: tinyint unsigned  #the version of the splitting algorithm used\n",
    "    ---\n",
    "    multiplicity=null    : tinyint unsigned             # the number of somas found for this base segment\n",
    "    n_splits             : int unsigned                 # the number of cuts required to help split the neuron\n",
    "    split_success        : tinyint unsigned             # the successfulness of the splitting\n",
    "    \n",
    "    n_error_limbs_cancelled : tinyint unsigned     # number of limbs that couldn't be resolved and cancelled out        \n",
    "    n_same_soma_limbs_cancelled : tinyint unsigned     # number of same soma touching limbs that couldn't be resolved and cancelled out\n",
    "    n_multi_soma_limbs_cancelled : tinyint unsigned     # number of multi soma touching limbs that couldn't be resolved and cancelled out        \n",
    "    \n",
    "    error_imbs_cancelled_area=NULL : double            # the total area (in microns^2) of the limbs that was cancelled out because touching the same soma multiple times or multiple somas\n",
    "    error_imbs_cancelled_skeletal_length = NULL : double #the total skeletal length (in microns) of the limbs that were called out because could not be resolved\n",
    "    \n",
    "    split_results: longblob #will store the results of how to split the limbs of neuron objects from original neuron\n",
    "    decomposition: <decomposition>\n",
    "    \n",
    "    \n",
    "    n_vertices           : int unsigned                 # number of vertices\n",
    "    n_faces              : int unsigned                 # number of faces\n",
    "    n_not_processed_soma_containing_meshes : int unsigned  #the number of meshes with somas that were not processed\n",
    "    n_error_limbs: int #the number of limbs that are touching multiple somas or 1 soma in multiple places\n",
    "    n_same_soma_multi_touching_limbs: int # number of limbs that touch the same soma multiple times\n",
    "    n_multi_soma_touching_limbs: int # number of limbs that touch multiple somas\n",
    "    n_somas: int #number of soma meshes detected\n",
    "    max_soma_n_faces:  int unsigned                 # The largest number of faces of the somas\n",
    "    max_soma_volume:  int unsigned                 # The largest volume of the somas the (volume in billions (10*9 nm^3))\n",
    "    n_limbs: int\n",
    "    n_branches: int\n",
    "    max_limb_n_branches=NULL:int\n",
    "    \n",
    "    skeletal_length=NULL: double\n",
    "    max_limb_skeletal_length=NULL:double\n",
    "    median_branch_length=NULL:double #gives information on average skeletal length to next branch point\n",
    "    \n",
    "    \n",
    "    width_median=NULL: double #median width from mesh center without spines removed\n",
    "    width_no_spine_median=NULL: double #median width from mesh center with spines removed\n",
    "    width_90_perc=NULL: double # 90th percentile for width without spines removed\n",
    "    width_no_spine_90_perc=NULL: double  # 90th percentile for width with spines removed\n",
    "    \n",
    "    \n",
    "    n_spines: bigint\n",
    "\n",
    "    spine_density=NULL: double # n_spines/ skeletal_length\n",
    "    spines_per_branch=NULL: double\n",
    "    \n",
    "    skeletal_length_eligible=NULL: double # the skeletal length for all branches searched for spines\n",
    "    n_spine_eligible_branches=NULL: int # the number of branches that were checked for spines because passed width threshold\n",
    "    \n",
    "    spine_density_eligible=NULL:double # n_spines/skeletal_length_eligible\n",
    "    spines_per_branch_eligible=NULL:double # n_spines/n_spine_eligible_branches\n",
    "    \n",
    "    total_spine_volume=NULL: double # the sum of all spine volume\n",
    "    spine_volume_median=NULL: double # median of the spine volume for those spines with able to calculate volume\n",
    "    spine_volume_density=NULL: double #total_spine_volume/skeletal_length\n",
    "    spine_volume_density_eligible=NULL: double #total_spine_volume/skeletal_length_eligible\n",
    "    spine_volume_per_branch_eligible=NULL: double #total_spine_volume/n_spine_eligible_branches\n",
    "    \n",
    "    run_time=NULL : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "                             \n",
    "    \n",
    "    ''' Old keysource used for inhibitory excitatory check\n",
    "    classified_table = (minnie.BaylorManualCellType() &\n",
    "                        'nucleus_version=3' & \n",
    "                        \"(cell_type = 'excitatory') or  (cell_type = 'inhibitory')\")\n",
    "    \n",
    "    key_source = ((minnie.Decomposition & \n",
    "                (minnie.NeuronSplitSuggestions.proj()) & \n",
    "                (classified_table.proj()) \n",
    "                & f\"n_somas<{max_n_somas}\" & \"n_error_limbs>0\"))'''\n",
    "    \n",
    "    # This keysource acounts that you could have more than 1 possible soma but not a significant limb connecting them (no error limbs)\n",
    "    key_source = minnie.Decomposition() & \"n_somas>1 OR n_error_limbs>0\"\n",
    "    \n",
    "    \n",
    "\n",
    "    def make(self,key):\n",
    "        \"\"\"\n",
    "        Pseudocode for process:\n",
    "\n",
    "        1) Get the segment id from the key\n",
    "        2) Get the decomposed neurong object from Decomposition table\n",
    "        3) Run the multi_soma split suggestions algorithm\n",
    "        4) Get the number of splits required for this neuron\n",
    "        5) Split the neuron into a list of neuron objects\n",
    "        6) For each neuron object in the list:\n",
    "        - get the number of errored limbs (to indicate the success type)\n",
    "        - Change the description to include the multiplicity\n",
    "        - Compute the information on the largest soma faces and volume\n",
    "        - Save the neuron object to the external\n",
    "        - Add the new write key to a list to commit \n",
    "        7) Write all of the keys \n",
    "        \"\"\"\n",
    "        \n",
    "        whole_pass_time = time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 1) Get the segment id from the key\n",
    "        segment_id = key[\"segment_id\"]\n",
    "        print(f\"\\n\\n\\n---- Working on Neuron {key['segment_id']} ----\")\n",
    "        \n",
    "        \n",
    "        # 2) Get the decomposed neuron object from Decomposition table and the split suggestions\n",
    "        neuron_obj = (minnie.Decomposition & key).fetch1(\"decomposition\")\n",
    "        \n",
    "        \"\"\" Old way that downloaded from another table\n",
    "        # 3) Retrieve the multi soma suggestions\n",
    "        split_results = (minnie.NeuronSplitSuggestions & key).fetch1(\"split_results\")\n",
    "        \"\"\"\n",
    "        #3) Calculated the split results\n",
    "        split_results = pru.multi_soma_split_suggestions(neuron_obj,plot_intermediates=False)\n",
    "        \n",
    "        # 4) Get the number of splits required for this neuron\n",
    "        n_paths_cut = pru.get_n_paths_cut(split_results)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"n_paths_cut = {n_paths_cut}\")\n",
    "            \n",
    "            \n",
    "        # 5) Split the neuron into a list of neuron objects\n",
    "        (neuron_list,\n",
    "        neuron_list_errored_limbs_area,\n",
    "         neuron_list_errored_limbs_skeletal_length,\n",
    "        neuron_list_n_multi_soma_errors,\n",
    "        neuron_list_n_same_soma_errors) = pru.split_neuron(neuron_obj,\n",
    "                        limb_results=split_results,\n",
    "                                       verbose=verbose,\n",
    "                                        return_error_info=True\n",
    "                                            )\n",
    "        \n",
    "        print(f\"neuron_list = {neuron_list}\")\n",
    "        print(f\"neuron_list_errored_limbs_area = {neuron_list_errored_limbs_area}\")\n",
    "        print(f\"neuron_list_n_multi_soma_errors = {neuron_list_n_multi_soma_errors}\")\n",
    "        print(f\"neuron_list_n_same_soma_errors = {neuron_list_n_same_soma_errors}\")\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Number of neurons: {len(neuron_list)}\")\n",
    "        \n",
    "        neuron_entries = []\n",
    "        for neuron_idx in range(len(neuron_list)):\n",
    "            \n",
    "            \"\"\"\n",
    "            # 6) For each neuron object in the list:\n",
    "            # - get the number of errored limbs (to indicate the success type)\n",
    "            # - Compute the information on the largest soma faces and volume\n",
    "            # - Save the neuron object to the external\n",
    "            # - Add the new write key to a list to commit \n",
    "            \"\"\"\n",
    "            n = neuron_list[neuron_idx]\n",
    "            \n",
    "            error_imbs_cancelled_area = neuron_list_errored_limbs_area[neuron_idx]\n",
    "            error_imbs_cancelled_skeletal_length = neuron_list_errored_limbs_skeletal_length[neuron_idx]\n",
    "            n_multi_soma_limbs_cancelled = neuron_list_n_multi_soma_errors[neuron_idx]\n",
    "            n_same_soma_limbs_cancelled = neuron_list_n_same_soma_errors[neuron_idx]\n",
    "            \n",
    "            \n",
    "            #for n in neuron_list:\n",
    "            #     nviz.visualize_neuron(n,\n",
    "            #                          limb_branch_dict=\"all\")\n",
    "\n",
    "            # - get the number of errored limbs (to indicate the success type)\n",
    "            if n.n_error_limbs == 0:\n",
    "                split_success = 0\n",
    "            elif n.multi_soma_touching_limbs == 0:\n",
    "                split_successs = 1\n",
    "            elif n.same_soma_multi_touching_limbs == 0:\n",
    "                split_success = 2\n",
    "            else:\n",
    "                split_success = 3\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"split_success = {split_success}\")\n",
    "\n",
    "            # - Compute the information on the largest soma faces and volume\n",
    "            soma_volumes = [n[k].volume/1000000000 for k in n.get_soma_node_names()] \n",
    "            soma_n_faces = [len(n[k].mesh.faces) for k in n.get_soma_node_names()] \n",
    "\n",
    "            largest_n_faces = np.max(soma_n_faces)\n",
    "            largest_volume = np.max(soma_volumes)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"largest_n_faces = {largest_n_faces}\")\n",
    "                print(f\"largest_volume = {largest_volume}\")\n",
    "\n",
    "            if \"split\" not in n.description:\n",
    "                n.description += \"_soma_0_split\"\n",
    "                \n",
    "            #6) Save the file in a certain location\n",
    "            if True:\n",
    "                save_time = time.time()\n",
    "                ret_file_path = n.save_compressed_neuron(output_folder=str(du.get_decomposition_path()),\n",
    "                                                  return_file_path=True,\n",
    "                                                 export_mesh=False,\n",
    "                                                 suppress_output=True)\n",
    "\n",
    "                ret_file_path_str = str(ret_file_path.absolute()) + \".pbz2\"\n",
    "                print(f\"Save time = {time.time() - save_time}\")\n",
    "            else:\n",
    "                print(\"Storing a dummy value for neuron\")\n",
    "                ret_file_path_str = \"dummy\"\n",
    "\n",
    "\n",
    "\n",
    "            #7) Pass stats and file location to insert\n",
    "            new_key = dict(key,\n",
    "                           split_index = neuron_idx,\n",
    "                           split_version = split_version,\n",
    "                           \n",
    "                           multiplicity=len(neuron_list),\n",
    "\n",
    "                           n_splits = n_paths_cut,\n",
    "                           split_success = split_success,\n",
    "                           \n",
    "                           n_error_limbs_cancelled = len(error_imbs_cancelled_area),\n",
    "                           \n",
    "                           n_multi_soma_limbs_cancelled =n_multi_soma_limbs_cancelled,\n",
    "                           n_same_soma_limbs_cancelled = n_same_soma_limbs_cancelled,\n",
    "                           error_imbs_cancelled_area = np.round(np.sum(error_imbs_cancelled_area),4),\n",
    "                           error_imbs_cancelled_skeletal_length = np.round(np.sum(error_imbs_cancelled_skeletal_length)/1000,4),\n",
    "                           \n",
    "                           split_results=split_results,\n",
    "\n",
    "                           max_soma_n_faces = largest_n_faces,\n",
    "                           max_soma_volume = largest_volume,\n",
    "\n",
    "\n",
    "                           decomposition=ret_file_path_str,\n",
    "                           n_vertices=len(n.mesh.vertices),\n",
    "                           n_faces=len(n.mesh.faces),\n",
    "                           run_time=np.round(time.time() - whole_pass_time,4)\n",
    "                          )\n",
    "\n",
    "            stats_dict = n.neuron_stats()\n",
    "            new_key.update(stats_dict)\n",
    "\n",
    "\n",
    "            neuron_entries.append(new_key)\n",
    "\n",
    "        \n",
    "        self.insert(neuron_entries, allow_direct_insert=True, skip_duplicates=True)\n",
    "        \n",
    "\n",
    "        print(f\"\\n\\n ------ Total time for {segment_id} = {time.time() - whole_pass_time} ------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_table = (minnie.schema.jobs & \"table_name='__decomposition_split'\")\n",
    "(curr_table)#.delete()# & \"status='error'\")\n",
    "#curr_table.delete()\n",
    "#(curr_table & \"error_message = 'ValueError: need at least one array to concatenate'\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "pru = reload(pru)\n",
    "nru = reload(nru)\n",
    "import neuron\n",
    "neuron = reload(neuron)\n",
    "\n",
    "start_time = time.time()\n",
    "if not test_mode:\n",
    "    time.sleep(random.randint(0, 800))\n",
    "print('Populate Started')\n",
    "if not test_mode:\n",
    "    DecompositionSplit.populate(reserve_jobs=True, suppress_errors=True, order=\"random\")\n",
    "else:\n",
    "    DecompositionSplit.populate(reserve_jobs=True, suppress_errors=False,order=\"random\")\n",
    "print('Populate Done')\n",
    "\n",
    "print(f\"Total time for DecompositionSplit populate = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
