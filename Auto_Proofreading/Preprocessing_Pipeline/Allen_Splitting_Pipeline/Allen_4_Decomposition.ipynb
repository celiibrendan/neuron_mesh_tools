{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: To decompose the multi-somas for splitting\n",
    "using the new decomposition method\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "sys.path.append(\"/meshAfterParty/meshAfterParty\")\n",
    "\n",
    "import datajoint_utils as du\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so that it will have the adapter defined\n",
    "from datajoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the contains method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minfig\n",
    "import time\n",
    "import numpy as np\n",
    "#want to add in a wait for the connection part\n",
    "random_sleep_sec = np.random.randint(0, 200)\n",
    "print(f\"Sleeping {random_sleep_sec} sec before conneting\")\n",
    "if not test_mode:\n",
    "    time.sleep(random_sleep_sec)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "du.config_celii()\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)\n",
    "\n",
    "#configuring will include the adapters\n",
    "minnie,schema = du.configure_minnie_vm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(schema.jobs & \"table_name='__decomposition'\").delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_utils as nru\n",
    "import neuron\n",
    "import trimesh_utils as tu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshlab\n",
    "meshlab.set_meshlab_port(current_port=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "\n",
    "key_source = ((minnie.Decimation).proj(decimation_version='version') & \n",
    "                            \"decimation_version=\" + str(decimation_version) &\n",
    "                       f\"decimation_ratio={decimation_ratio}\" &  (minnie.BaylorSegmentCentroid() & \"multiplicity>0\")  & (minnie.AllenProofreading() & dict(month=3,day=3,year=2021)).proj())\n",
    "key_source                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minnie.DecompositionVersions.insert1(dict(process_version=6,description=\"3/4: retry limb processing with meshparty if meshafterparty fails\"),skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema.external['decomposition'].delete(delete_external_files=True)\n",
    "# schema.external['somas'].delete(delete_external_files=True)\n",
    "# schema.external['faces'].delete(delete_external_files=True)\n",
    "# # schema.external['decimated_meshes'].delete(delete_external_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_source = ((minnie.Decimation).proj(decimation_version='version') & \n",
    "#                             \"decimation_version=\" + str(decimation_version) &\n",
    "#                        f\"decimation_ratio={decimation_ratio}\" &  (minnie.BaylorSegmentCentroid() & \"multiplicity>0\")  & (minnie.AllenProofreading() & dict(month=3,day=3,year=2021)).proj())               \n",
    "# minnie.Decomposition() & key_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "process_version = 6\n",
    "\n",
    "@schema\n",
    "class Decomposition(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation.proj(decimation_version='version')\n",
    "    ver : decimal(6,2) #the version number of the materializaiton\n",
    "    process_version : int unsigned #the version of the preprocessing pipeline run\n",
    "    index : tinyint unsigned  #the index of the neuron object that resulted from that mesh (indexed starting at 0)\n",
    "    ---\n",
    "    multiplicity=null    : tinyint unsigned             # the number of somas found for this base segment\n",
    "    decomposition: <decomposition>\n",
    "    n_vertices           : int unsigned                 # number of vertices\n",
    "    n_faces              : int unsigned                 # number of faces\n",
    "    n_not_processed_soma_containing_meshes : int unsigned  #the number of meshes with somas that were not processed\n",
    "    n_error_limbs: int #the number of limbs that are touching multiple somas or 1 soma in multiple places\n",
    "    n_same_soma_multi_touching_limbs: int # number of limbs that touch the same soma multiple times\n",
    "    n_multi_soma_touching_limbs: int # number of limbs that touch multiple somas\n",
    "    n_somas: int #number of soma meshes detected\n",
    "    n_limbs: int\n",
    "    n_branches: int\n",
    "    max_limb_n_branches=NULL:int\n",
    "    \n",
    "    skeletal_length=NULL: double\n",
    "    max_limb_skeletal_length=NULL:double\n",
    "    median_branch_length=NULL:double #gives information on average skeletal length to next branch point\n",
    "    \n",
    "    \n",
    "    width_median=NULL: double #median width from mesh center without spines removed\n",
    "    width_no_spine_median=NULL: double #median width from mesh center with spines removed\n",
    "    width_90_perc=NULL: double # 90th percentile for width without spines removed\n",
    "    width_no_spine_90_perc=NULL: double  # 90th percentile for width with spines removed\n",
    "    \n",
    "    \n",
    "    n_spines: bigint\n",
    "\n",
    "    spine_density=NULL: double # n_spines/ skeletal_length\n",
    "    spines_per_branch=NULL: double\n",
    "    \n",
    "    skeletal_length_eligible=NULL: double # the skeletal length for all branches searched for spines\n",
    "    n_spine_eligible_branches=NULL: int # the number of branches that were checked for spines because passed width threshold\n",
    "    \n",
    "    spine_density_eligible=NULL:double # n_spines/skeletal_length_eligible\n",
    "    spines_per_branch_eligible=NULL:double # n_spines/n_spine_eligible_branches\n",
    "    \n",
    "    total_spine_volume=NULL: double # the sum of all spine volume\n",
    "    spine_volume_median=NULL: double # median of the spine volume for those spines with able to calculate volume\n",
    "    spine_volume_density=NULL: double #total_spine_volume/skeletal_length\n",
    "    spine_volume_density_eligible=NULL: double #total_spine_volume/skeletal_length_eligible\n",
    "    spine_volume_per_branch_eligible=NULL: double #total_spine_volume/n_spine_eligible_branches\n",
    "    \n",
    "    run_time=NULL : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    key_source = ((minnie.Decimation).proj(decimation_version='version') & \n",
    "                            \"decimation_version=\" + str(decimation_version) &\n",
    "                       f\"decimation_ratio={decimation_ratio}\" &  (minnie.BaylorSegmentCentroid() & \"multiplicity>0\")  & (minnie.AllenProofreading() & dict(month=3,day=3,year=2021)).proj())               \n",
    "    \n",
    "\n",
    "    def make(self,key):\n",
    "        \"\"\"\n",
    "        Pseudocode for process:\n",
    "\n",
    "        1) Get the segment id from the key\n",
    "        2) Get the decimated mesh\n",
    "        3) Get the somas info\n",
    "        4) Run the preprocessing\n",
    "        5) Calculate all starter stats\n",
    "        6) Save the file in a certain location\n",
    "        7) Pass stats and file location to insert\n",
    "        \"\"\"\n",
    "        whole_pass_time = time.time()\n",
    "        #1) Get the segment id from the key\n",
    "        segment_id = key[\"segment_id\"]\n",
    "        description = str(key['decimation_version']) + \"_25\"\n",
    "        print(f\"\\n\\n\\n---- Working on Neuron {key['segment_id']} ----\")\n",
    "        global_start = time.time()\n",
    "        \n",
    "        #2) Get the decimated mesh\n",
    "        current_neuron_mesh = du.fetch_segment_id_mesh(segment_id)\n",
    "        \n",
    "\n",
    "        #3) Get the somas info \n",
    "        somas = du.get_soma_mesh_list(segment_id) \n",
    "        soma_ver = du.get_soma_mesh_list_ver(segment_id)\n",
    "                  \n",
    "        \n",
    "        print(f\"somas = {somas}\")\n",
    "                  \n",
    "        #3b) Get the glia and nuclei information \n",
    "        glia_faces,nuclei_faces = du.get_segment_glia_nuclei_faces(segment_id,return_empty_list=True)\n",
    "        \n",
    "                  \n",
    "        #4) Run the preprocessing\n",
    "\n",
    "\n",
    "        total_neuron_process_time = time.time()\n",
    "\n",
    "        print(f\"\\n--- Beginning preprocessing of {segment_id}---\")\n",
    "        recovered_neuron = neuron.Neuron(\n",
    "        mesh = current_neuron_mesh,\n",
    "        somas = somas,\n",
    "        segment_id=segment_id,\n",
    "        description=description,\n",
    "        suppress_preprocessing_print=False,\n",
    "        suppress_output=False,\n",
    "        calculate_spines=True,\n",
    "        widths_to_calculate=[\"no_spine_median_mesh_center\"],\n",
    "        glia_faces=glia_faces,\n",
    "        nuclei_faces = nuclei_faces,\n",
    "\n",
    "                )\n",
    "\n",
    "        print(f\"\\n\\n\\n---- Total preprocessing time = {time.time() - total_neuron_process_time}\")\n",
    "\n",
    "\n",
    "        #5) Don't have to do any of the processing anymore because will do in the neuron object\n",
    "        stats_dict = recovered_neuron.neuron_stats()\n",
    "\n",
    "\n",
    "\n",
    "        #6) Save the file in a certain location\n",
    "        save_time = time.time()\n",
    "        ret_file_path = recovered_neuron.save_compressed_neuron(output_folder=str(du.get_decomposition_path()),\n",
    "                                          return_file_path=True,\n",
    "                                         export_mesh=False,\n",
    "                                         suppress_output=True)\n",
    "\n",
    "        ret_file_path_str = str(ret_file_path.absolute()) + \".pbz2\"\n",
    "        print(f\"Save time = {time.time() - save_time}\")\n",
    "\n",
    "        #7) Pass stats and file location to insert\n",
    "        new_key = dict(key,\n",
    "                       ver = soma_ver,\n",
    "                       process_version = process_version,\n",
    "                       index = 0,\n",
    "                       multiplicity=1,\n",
    "                       decomposition=ret_file_path_str,\n",
    "                       n_vertices=len(current_neuron_mesh.vertices),\n",
    "                       n_faces=len(current_neuron_mesh.faces),\n",
    "                       run_time=np.round(time.time() - whole_pass_time,4)\n",
    "                      )\n",
    "        new_key.update(stats_dict)\n",
    "        \n",
    "        keys_to_delete = [\"axon_length\",\n",
    "        \"axon_area\",\n",
    "        \"max_soma_volume\",\n",
    "        \"max_soma_n_faces\"]\n",
    "        \n",
    "        for k_to_delete in keys_to_delete:\n",
    "            del new_key[k_to_delete]\n",
    "\n",
    "        self.insert1(new_key, allow_direct_insert=True, skip_duplicates=True)\n",
    "\n",
    "        print(f\"\\n\\n ------ Total time for {segment_id} = {time.time() - global_start} ------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_table = (minnie.schema.jobs & \"table_name='__decomposition'\")\n",
    "(curr_table).delete()\n",
    "#curr_table.delete()\n",
    "#(curr_table & \"error_message = 'ValueError: need at least one array to concatenate'\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import compartment_utils as cu\n",
    "cu = reload(cu)\n",
    "import preprocessing_vp2 as pre\n",
    "pre = reload(pre)\n",
    "import skeleton_utils as sk\n",
    "sk = reload(sk)\n",
    "nru = reload(nru)\n",
    "\n",
    "start_time = time.time()\n",
    "if not test_mode:\n",
    "    time.sleep(random.randint(0, 800))\n",
    "print('Populate Started')\n",
    "if not test_mode:\n",
    "    Decomposition.populate(reserve_jobs=True, suppress_errors=True)\n",
    "else:\n",
    "    Decomposition.populate(reserve_jobs=True, suppress_errors=False)\n",
    "print('Populate Done')\n",
    "\n",
    "print(f\"Total time for DecompositionMultiSoma populate = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
