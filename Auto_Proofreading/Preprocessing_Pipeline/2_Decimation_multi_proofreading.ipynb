{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: To decompose the multi-somas for splitting\n",
    "using the new decomposition method\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "\n",
    "import datajoint_utils as du\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minfig\n",
    "import time\n",
    "import numpy as np\n",
    "#want to add in a wait for the connection part\n",
    "random_sleep_sec = np.random.randint(0, 200)\n",
    "print(f\"Sleeping {random_sleep_sec} sec before conneting\")\n",
    "if not test_mode:\n",
    "    time.sleep(random_sleep_sec)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "du.config_celii()\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)\n",
    "\n",
    "#configuring will include the adapters\n",
    "minnie,schema = du.configure_minnie_vm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the list of neurons to decompose for the mutli soma testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# soma_soma_table = pd.read_csv(\"Minnie65 core proofreading - Soma-Soma.csv\")\n",
    "# no_header = soma_soma_table.iloc[1:]\n",
    "# multi_soma_ids_str = no_header[\"Dendrites\"].to_numpy()\n",
    "# multi_soma_ids = multi_soma_ids_str[~np.isnan(multi_soma_ids_str.astype(\"float\"))].astype(\"int\")\n",
    "\n",
    "# @schema\n",
    "# class MultiSomaProofread(dj.Manual):\n",
    "#     definition=\"\"\"\n",
    "#     segment_id : bigint unsigned  #segment id for those to be decimated\n",
    "#     \"\"\"\n",
    "    \n",
    "# dict_of_seg = [dict(segment_id=k) for k in multi_soma_ids]\n",
    "# minnie.MultiSomaProofread.insert(dict_of_seg,skip_duplicates=True)\n",
    "# MultiSomaProofread()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_utils as nru\n",
    "import neuron\n",
    "import trimesh_utils as tu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshlab\n",
    "meshlab.set_meshlab_port(current_port=None)\n",
    "temporary_folder = 'decimation_temp'\n",
    "meshlab_scripts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so that it will have the adapter defined\n",
    "from datajoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "\n",
    "from minfig.minnie65_config import external_decimated_mesh_path\n",
    "\n",
    "@schema\n",
    "class Decimation(dj.Computed):\n",
    "#     definition = minnie.Decimation.describe(printout=False)\n",
    "    key_source = minnie.Mesh.proj() * (minnie.DecimationConfig & 'decimation_ratio=0.25') & minnie.MultiSomaProofread2.proj()\n",
    "\n",
    "    # Creates hf file at the proper location, returns the filepath of the newly created file\n",
    "    @classmethod\n",
    "    def make_file(cls, segment_id, version, decimation_ratio, vertices, faces):\n",
    "        \"\"\"Creates hf file at the proper location, returns the filepath of the newly created file\"\"\"\n",
    "\n",
    "        assert vertices.ndim == 2 and vertices.shape[1] == 3\n",
    "        assert faces.ndim == 2 and faces.shape[1] == 3\n",
    "\n",
    "        filename = f'{segment_id}_{version}_{int(decimation_ratio*100):02}.h5'\n",
    "        filepath = os.path.join(external_decimated_mesh_path, filename)\n",
    "        with h5py.File(filepath, 'w') as hf:\n",
    "            hf.create_dataset('segment_id', data=segment_id)\n",
    "            hf.create_dataset('version', data=version)\n",
    "            hf.create_dataset('decimation_ratio', data=float(decimation_ratio))\n",
    "            hf.create_dataset('vertices', data=vertices)\n",
    "            hf.create_dataset('faces', data=faces)\n",
    "\n",
    "        return filepath\n",
    "\n",
    "    @classmethod\n",
    "    def make_entry(cls, segment_id, version, decimation_ratio, vertices, faces):\n",
    "        key = dict(\n",
    "            segment_id=segment_id,\n",
    "            version=version,\n",
    "            decimation_ratio=decimation_ratio,\n",
    "            n_vertices=len(vertices),\n",
    "            n_faces=len(faces)\n",
    "        )\n",
    "\n",
    "        filepath = cls.make_file(segment_id, version, decimation_ratio, vertices, faces)\n",
    "\n",
    "        cls.insert1(dict(key, mesh=filepath), allow_direct_insert=True)\n",
    "\n",
    "    \n",
    "\n",
    "    def make(self, key):\n",
    "        print(key)\n",
    "        mesh = (minnie.Mesh & key).fetch1('mesh')\n",
    "        segment_id = key['segment_id']\n",
    "        version = key['version']\n",
    "        decimation_ratio = key['decimation_ratio']\n",
    "        print(f\"Mesh size: n_vertices = {len(mesh.vertices)}, n_faces = {len(mesh.faces)}\")\n",
    "\n",
    "        if decimation_ratio not in meshlab_scripts:\n",
    "            meshlab_scripts[decimation_ratio] = meshlab.Decimator(decimation_ratio, temporary_folder, overwrite=False)\n",
    "        mls_func = meshlab_scripts[decimation_ratio]\n",
    "\n",
    "        try:\n",
    "            expected_filepath = os.path.join(external_decimated_mesh_path, f'{segment_id}_{version}.h5')\n",
    "            if not os.path.isfile(expected_filepath):\n",
    "                new_mesh, _path = mls_func(mesh.vertices, mesh.faces, segment_id)\n",
    "                new_vertices, new_faces = new_mesh.vertices, new_mesh.faces\n",
    "\n",
    "                self.make_entry(\n",
    "                    segment_id=segment_id,\n",
    "                    version=version,\n",
    "                    decimation_ratio=decimation_ratio,\n",
    "                    vertices=new_vertices,\n",
    "                    faces=new_faces,\n",
    "                    )\n",
    "            else:\n",
    "                print('File already exists.')\n",
    "                with h5py.File(expected_filepath, 'r') as hf:\n",
    "                    vertices = hf['vertices'][()].astype(np.float64)\n",
    "                    faces = hf['faces'][()].reshape(-1, 3).astype(np.uint32)\n",
    "                self.insert1(dict(key, n_vertices=len(vertices), n_faces=len(faces), mesh=expected_filepath), allow_direct_insert=True)\n",
    "        except Exception as e:\n",
    "            minnie.DecimationError.insert1(dict(key, log=str(e)))\n",
    "            print(e)\n",
    "            raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_table = (minnie.schema.jobs & \"table_name='__decimation'\")\n",
    "#curr_table.delete()\n",
    "#curr_table.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# key_hash,error_message = curr_table.fetch(\"key_hash\",\"error_message\")\n",
    "\n",
    "# df = pd.DataFrame.from_dict([dict(key_hash = k,error_message = m) for k,m in zip(key_hash,error_message)])\n",
    "# df\n",
    "# #df.columns = [\"error\",\"key_hash\"]\n",
    "# key_hashes_to_delete = df[df[\"error_message\"].str.contains(\"OSError\")][\"key_hash\"].to_numpy()\n",
    "\n",
    "# (curr_table & [dict(key_hash=k) for k in key_hashes_to_delete]).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "start_time = time.time()\n",
    "if not test_mode:\n",
    "    time.sleep(random.randint(0, 800))\n",
    "print('Populate Started')\n",
    "if not test_mode:\n",
    "    Decimation.populate(reserve_jobs=True, suppress_errors=True)\n",
    "else:\n",
    "    Decimation.populate(reserve_jobs=True, suppress_errors=False)\n",
    "print('Populate Done')\n",
    "\n",
    "print(f\"Total time for Decimation populate = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
