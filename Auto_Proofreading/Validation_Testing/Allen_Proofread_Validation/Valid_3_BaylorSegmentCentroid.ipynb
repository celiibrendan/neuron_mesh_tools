{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Run the Soma Finding\n",
    "Algorithm for all the cells\n",
    "in our final match\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_version = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-02-23 06:03:14,042 - settings - Setting database.host to at-database.ad.bcm.edu\n",
      "INFO - 2021-02-23 06:03:14,045 - settings - Setting database.user to celiib\n",
      "INFO - 2021-02-23 06:03:14,046 - settings - Setting database.password to newceliipass\n",
      "INFO - 2021-02-23 06:03:14,049 - settings - Setting stores to {'minnie65': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65', 'stage': '/mnt/dj-stor01/platinum/minnie65'}, 'meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/meshes'}, 'decimated_meshes': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes', 'stage': '/mnt/dj-stor01/platinum/minnie65/02/decimated_meshes'}, 'skeletons': {'protocol': 'file', 'location': '/mnt/dj-stor01/platinum/minnie65/02/skeletons'}}\n",
      "INFO - 2021-02-23 06:03:14,050 - settings - Setting enable_python_native_blobs to True\n",
      "INFO - 2021-02-23 06:03:14,063 - connection - Connected celiib@at-database.ad.bcm.edu:3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@at-database.ad.bcm.edu:3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-02-23 06:03:14,396 - settings - Setting enable_python_native_blobs to True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "sys.path.append(\"/meshAfterParty/\")\n",
    "sys.path.append(\"/meshAfterParty/meshAfterParty/\")\n",
    "\n",
    "import datajoint_utils as du\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-02-23 06:03:14,466 - settings - Setting database.host to at-database.ad.bcm.edu\n",
      "INFO - 2021-02-23 06:03:14,467 - settings - Setting database.user to celiib\n",
      "INFO - 2021-02-23 06:03:14,468 - settings - Setting database.password to newceliipass\n",
      "INFO - 2021-02-23 06:03:14,471 - settings - Setting enable_python_native_blobs to True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping 98 sec before conneting\n",
      "Done sleeping\n",
      "Current path for external_segmentation_path = /mnt/dj-stor01/platinum/minnie65/02\n",
      "Current path for external_mesh_path = /mnt/dj-stor01/platinum/minnie65/02/meshes\n",
      "Current path for external_decimated_mesh_path = /mnt/dj-stor01/platinum/minnie65/02/decimated_meshes\n",
      "Current path for external_skeleton_path = /mnt/dj-stor01/platinum/minnie65/02/skeletons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-02-23 06:03:14,800 - settings - Setting enable_python_native_blobs to True\n"
     ]
    }
   ],
   "source": [
    "import minfig\n",
    "import time\n",
    "import numpy as np\n",
    "#want to add in a wait for the connection part\n",
    "random_sleep_sec = np.random.randint(0, 200)\n",
    "print(f\"Sleeping {random_sleep_sec} sec before conneting\")\n",
    "if not test_mode:\n",
    "    time.sleep(random_sleep_sec)\n",
    "print(\"Done sleeping\")\n",
    "\n",
    "du.config_celii()\n",
    "du.set_minnie65_config_segmentation(minfig)\n",
    "du.print_minnie65_config_paths(minfig)\n",
    "\n",
    "#configuring will include the adapters\n",
    "minnie,schema = du.configure_minnie_vm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuron_utils as nru\n",
    "import neuron\n",
    "import trimesh_utils as tu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No port chosen so picked random port 3694\n"
     ]
    }
   ],
   "source": [
    "import meshlab\n",
    "meshlab.set_meshlab_port(current_port=None)\n",
    "temporary_folder = 'decimation_temp'\n",
    "meshlab_scripts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so that it will have the adapter defined\n",
    "from datajoint_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class NeuronGliaNuclei(dj.Manual):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation.proj(decimation_version='version')\n",
    "    ver : decimal(6,2) #the version number of the materializaiton\n",
    "    ---\n",
    "    n_glia_faces              : int unsigned                 # The number of faces that were saved off as belonging to glia\n",
    "    glia_faces=NULL           : <faces>                      # faces indices that were saved off as belonging to glia (external storage)\n",
    "    n_nuclei_faces            : int unsigned                 # The number of faces that were saved off as belonging to nuclie\n",
    "    nuclei_faces=NULL         : <faces>                      # faces indices that were saved off as belonging to nuclei (external storage)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema.external['faces'].delete(delete_external_files=True)\n",
    "# schema.external['somas'].delete(delete_external_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minnie.BaylorSegmentCentroid.delete()\n",
    "# minnie.NeuronGliaNuclei().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decimation_version = 0\n",
    "# decimation_ratio = 0.25\n",
    "# verts_min = 10000\n",
    "\n",
    "\n",
    "# key_source =  ((minnie.Decimation & f\"n_vertices > {verts_min}\").proj(decimation_version='version') & \n",
    "#                         \"decimation_version=\" + str(decimation_version) &\n",
    "#                    f\"decimation_ratio={decimation_ratio}\") & (dj.U(\"segment_id\") & (minnie.OldBaylorSegmentCentroid() & \"multiplicity<3\").proj()\n",
    "#                                                              & (dj.U(\"segment_id\") & nucleus_table))\n",
    "# key_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "verts_min = 10000\n",
    "current_version = 30\n",
    "\n",
    "\n",
    "import trimesh_utils as tu\n",
    "import soma_extraction_utils as sm\n",
    "@schema\n",
    "class BaylorSegmentCentroid(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation.proj(decimation_version='version')\n",
    "    soma_index : tinyint unsigned #index given to this soma to account for multiple somas in one base semgnet\n",
    "    ver : decimal(6,2) #the version number of the materializaiton\n",
    "    ---\n",
    "    centroid_x=NULL           : int unsigned                 # (EM voxels)\n",
    "    centroid_y=NULL           : int unsigned                 # (EM voxels)\n",
    "    centroid_z=NULL           : int unsigned                 # (EM voxels)\n",
    "    n_vertices=NULL           : bigint                 #number of vertices\n",
    "    n_faces=NULL            : bigint                  #number of faces\n",
    "    mesh: <somas>  #datajoint adapter to get the somas mesh objects\n",
    "    multiplicity=NULL         : tinyint unsigned             # the number of somas found for this base segment\n",
    "    sdf=NULL                  : double                       # sdf width value for the soma\n",
    "    volume=NULL               : double                       # the volume in billions (10*9 nm^3) of the convex hull\n",
    "    max_side_ratio=NULL       : double                       # the maximum of the side length ratios used for check if soma\n",
    "    bbox_volume_ratio=NULL    : double                       # ratio of bbox (axis aligned) volume to mesh volume to use for check if soma\n",
    "    max_hole_length=NULL      : double                    #euclidean distance of the maximum hole size\n",
    "    run_time=NULL : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    key_source =  (((minnie.Decimation & f\"n_vertices > {verts_min}\").proj(decimation_version='version') & \n",
    "                            \"decimation_version=\" + str(decimation_version) &\n",
    "                       f\"decimation_ratio={decimation_ratio}\") & (dj.U(\"segment_id\") & minnie.AutoProofreadValidationSegment()))\n",
    "                                                                 \n",
    "     \n",
    "\n",
    "    def make(self,key):\n",
    "        \"\"\"\n",
    "        Pseudocode: \n",
    "        1) Compute all of the\n",
    "        2) Save the mesh as an h5 py file\n",
    "        3) Store the saved path as the decomposition part of the dictionary and erase the vertices and faces\n",
    "        4) Insert\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #get the mesh data\n",
    "        print(f\"\\n\\n\\n---- Working on Neuron {key['segment_id']} ----\")\n",
    "        print(key)\n",
    "        new_mesh = (minnie.Decimation() & key).fetch1(\"mesh\")\n",
    "        current_mesh_verts,current_mesh_faces = new_mesh.vertices,new_mesh.faces\n",
    "\n",
    "        segment_id = key[\"segment_id\"]\n",
    "\n",
    "        (total_soma_list, \n",
    "         run_time, \n",
    "         total_soma_list_sdf,\n",
    "         glia_pieces,\n",
    "         nuclei_pieces) = sm.extract_soma_center(\n",
    "                            segment_id,\n",
    "                            current_mesh_verts,\n",
    "                            current_mesh_faces,\n",
    "            return_glia_nuclei_pieces=True,\n",
    "        )\n",
    "        \n",
    "        # -------- 1/9 Addition: Going to save off the glia and nuclei pieces ----------- #\n",
    "        \"\"\"\n",
    "        Psuedocode:\n",
    "        For both glia and nuclie pieces\n",
    "        1) If the length of array is greater than 0 --> combine the mesh and map the indices to original mesh\n",
    "        2) If not then just put None     \n",
    "        \"\"\"\n",
    "        orig_mesh = trimesh.Trimesh(vertices=current_mesh_verts,\n",
    "                                   faces=current_mesh_faces)\n",
    "        \n",
    "        if len(glia_pieces)>0:\n",
    "            glia_faces = tu.original_mesh_faces_map(orig_mesh,tu.combine_meshes(glia_pieces))\n",
    "            n_glia_faces = len(glia_faces)\n",
    "        else:\n",
    "            glia_faces = None\n",
    "            n_glia_faces = 0\n",
    "            \n",
    "        if len(nuclei_pieces)>0:\n",
    "            nuclei_faces = tu.original_mesh_faces_map(orig_mesh,tu.combine_meshes(nuclei_pieces))\n",
    "            n_nuclei_faces = len(nuclei_faces)\n",
    "        else:\n",
    "            nuclei_faces = None\n",
    "            n_nuclei_faces = 0\n",
    "            \n",
    "        # --------- saving the nuclei and glia saves\n",
    "        glia_path,nuclei_path = du.save_glia_nuclei_files(glia_faces=glia_faces,\n",
    "                                 nuclei_faces=nuclei_faces,\n",
    "                                 segment_id=segment_id)\n",
    "        \n",
    "        print(f\" glia_path = {glia_path} \\n nuclei_path = {nuclei_path}\")\n",
    "            \n",
    "        glia_nuclei_key = dict(key,\n",
    "                               ver=current_version,\n",
    "                               n_glia_faces=n_glia_faces,\n",
    "                               #glia_faces = glia_faces,\n",
    "                               glia_faces = glia_path,\n",
    "                               n_nuclei_faces = n_nuclei_faces,\n",
    "                               #nuclei_faces = nuclei_faces\n",
    "                               nuclei_faces = nuclei_path,\n",
    "                              )\n",
    "        \n",
    "        NeuronGliaNuclei.insert1(glia_nuclei_key,replace=True)\n",
    "        print(f\"Finished saving off glia and nuclei information : {glia_nuclei_key}\")\n",
    "        \n",
    "        # ---------------- End of 1/9 Addition --------------------------------- #\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Run time was {run_time} \\n    total_soma_list = {total_soma_list}\"\n",
    "             f\"\\n    with sdf values = {total_soma_list_sdf}\")\n",
    "        \n",
    "        #check if soma list is empty and did not find soma\n",
    "        if len(total_soma_list) <= 0:\n",
    "            print(\"There were no somas found for this mesh so just writing empty data\")\n",
    "            \n",
    "\n",
    "            returned_file_path = tu.write_h5_file(\n",
    "                                                vertices=np.array([]),\n",
    "                                                  faces=np.array([]),\n",
    "                                                  segment_id=segment_id,\n",
    "                                                  filename = f'{segment_id}_0.h5',\n",
    "                                                    filepath=str(du.get_somas_path())\n",
    "                                                 )\n",
    "\n",
    "            \n",
    "            \n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=0,\n",
    "                               ver=current_version,\n",
    "                              centroid_x=None,\n",
    "                               centroid_y=None,\n",
    "                               centroid_z=None,\n",
    "                               #distance_from_prediction=None,\n",
    "                               #prediction_matching_index = None,\n",
    "                               n_vertices=0,\n",
    "                               n_faces=0,\n",
    "                               mesh=returned_file_path,\n",
    "                               multiplicity=0,\n",
    "                               sdf = None,\n",
    "                               volume = None,\n",
    "                               max_side_ratio = None,\n",
    "                               bbox_volume_ratio = None,\n",
    "                               max_hole_length=None,\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            #raise Exception(\"to prevent writing because none were found\")\n",
    "            self.insert1(insert_dict,skip_duplicates=True)\n",
    "            return\n",
    "        \n",
    "        #if there is one or more soma found, get the volume and side length checks\n",
    "        max_side_ratio =  [np.max(sm.side_length_ratios(m)) for m in total_soma_list]\n",
    "        bbox_volume_ratio =  [sm.soma_volume_ratio(m) for m in total_soma_list]\n",
    "        dicts_to_insert = []\n",
    "\n",
    "\n",
    "        for i,(current_soma,soma_sdf,sz_ratio,vol_ratio) in enumerate(zip(total_soma_list,total_soma_list_sdf,max_side_ratio,bbox_volume_ratio)):\n",
    "            print(\"Trying to write off file\")\n",
    "            \"\"\" Currently don't need to export the meshes\n",
    "            current_soma.export(f\"{key['segment_id']}/{key['segment_id']}_soma_{i}.off\")\n",
    "            \"\"\"\n",
    "            auto_prediction_center = np.mean(current_soma.vertices,axis=0) / np.array([4,4,40])\n",
    "            auto_prediction_center = auto_prediction_center.astype(\"int\")\n",
    "            print(f\"Predicted Coordinates are {auto_prediction_center}\")\n",
    "            max_hole_length = tu.largest_hole_length(current_soma)\n",
    "            \n",
    "            returned_file_path = tu.write_h5_file(\n",
    "                                            vertices=current_soma.vertices,\n",
    "                                              faces=current_soma.faces,\n",
    "                                              segment_id=segment_id,\n",
    "                                              filename = f'{segment_id}_{i}.h5',\n",
    "                                                filepath=str(du.get_somas_path())\n",
    "                                             )\n",
    "\n",
    "\n",
    "\n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=i+1,\n",
    "                               ver=current_version,\n",
    "                              centroid_x=auto_prediction_center[0],\n",
    "                               centroid_y=auto_prediction_center[1],\n",
    "                               centroid_z=auto_prediction_center[2],\n",
    "                               n_vertices = len(current_soma.vertices),\n",
    "                               n_faces = len(current_soma.faces),\n",
    "                               mesh=returned_file_path,\n",
    "                               multiplicity=len(total_soma_list),\n",
    "                               sdf = np.round(soma_sdf,3),\n",
    "                               volume = current_soma.convex_hull.volume/1000000000,\n",
    "                               max_side_ratio = np.round(sz_ratio,3),\n",
    "                               bbox_volume_ratio = np.round(vol_ratio,3),\n",
    "                               max_hole_length = np.round(max_hole_length,3),\n",
    "                               run_time=np.round(run_time,4)\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "            dicts_to_insert.append(insert_dict)\n",
    "        self.insert(dicts_to_insert,skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_table = (minnie.schema.jobs & \"table_name='__baylor_segment_centroid'\")\n",
    "#((curr_table) & \"timestamp>'2021-02-22'\").delete()#.delete()# & \"status='error'\"#.delete()\n",
    "#curr_table.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# key_hash,error_message = curr_table.fetch(\"key_hash\",\"error_message\")\n",
    "\n",
    "# df = pd.DataFrame.from_dict([dict(key_hash = k,error_message = m) for k,m in zip(key_hash,error_message)])\n",
    "# df\n",
    "# #df.columns = [\"error\",\"key_hash\"]\n",
    "# key_hashes_to_delete = df[df[\"error_message\"].str.contains(\"OSError\")][\"key_hash\"].to_numpy()\n",
    "\n",
    "# (curr_table & [dict(key_hash=k) for k in key_hashes_to_delete]).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "sm = reload(sm)\n",
    "\n",
    "start_time = time.time()\n",
    "if not test_mode:\n",
    "    time.sleep(random.randint(0, 800))\n",
    "print('Populate Started')\n",
    "if not test_mode:\n",
    "    BaylorSegmentCentroid.populate(reserve_jobs=True, suppress_errors=True)\n",
    "else:\n",
    "    BaylorSegmentCentroid.populate(reserve_jobs=True, suppress_errors=False)\n",
    "print('Populate Done')\n",
    "\n",
    "print(f\"Total time for BaylorSegmentCentroid populate = {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
