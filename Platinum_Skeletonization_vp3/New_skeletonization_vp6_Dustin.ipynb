{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPurpose: New skeletonization method that does\\nnot use a recursive method but simply uses the followoing\\nalgorithm: \\n- Do mesh splitting to find which mesh pieces aren't connect to\\n  the soma\\n0) Do Soma Extraction and split the meshes from there\\nFor Each significant mesh that was split off in beginning\\n1) Poisson Surface Reconstruction\\n2) CGAL skeletonization of all signfiicant pieces\\n3) Using CGAL skeleton, find the leftover mesh not skeletonized\\n4) Do surface reconstruction on the parts that are left over\\n- with some downsampling\\n5) Stitch the skeleton \\n\\n\\n---- Afterwards stitching:\\n1) Compute the soma mesh center point\\n2) For meshes that were originally connected to soma\\na. Find the closest skeletal point to soma center\\nb. Add an edge from closest point to soma center\\n3) Then do stitching algorithm on all of remaining disconnected\\n    skeletons\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: New skeletonization method that does\n",
    "not use a recursive method but simply uses the followoing\n",
    "algorithm: \n",
    "- Do mesh splitting to find which mesh pieces aren't connect to\n",
    "  the soma\n",
    "0) Do Soma Extraction and split the meshes from there\n",
    "For Each significant mesh that was split off in beginning\n",
    "1) Poisson Surface Reconstruction\n",
    "2) CGAL skeletonization of all signfiicant pieces\n",
    "3) Using CGAL skeleton, find the leftover mesh not skeletonized\n",
    "4) Do surface reconstruction on the parts that are left over\n",
    "- with some downsampling\n",
    "5) Stitch the skeleton \n",
    "\n",
    "\n",
    "---- Afterwards stitching:\n",
    "1) Compute the soma mesh center point\n",
    "2) For meshes that were originally connected to soma\n",
    "a. Find the closest skeletal point to soma center\n",
    "b. Add an edge from closest point to soma center\n",
    "3) Then do stitching algorithm on all of remaining disconnected\n",
    "    skeletons\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykdtree.kdtree import KDTree\n",
    "import time\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshlab\n",
    "from importlib import reload\n",
    "meshlab = reload(meshlab)\n",
    "from meshlab import Decimator , Poisson\n",
    "import skeleton_utils as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the calcification edge size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Example Mesh and Example Soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soma_extraction_utils as soma_utils\n",
    "from pathlib import Path\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_somas(segment_id,main_mesh_total):\n",
    "    try:\n",
    "        current_soma = trimesh.load_mesh(\"./Dustin_soma.off\")\n",
    "        return [current_soma]\n",
    "    except:\n",
    "        print(\"No Soma currently available so must compute own\")\n",
    "        (total_soma_list, \n",
    "             run_time, \n",
    "             total_soma_list_sdf) = soma_utils.extract_soma_center(\n",
    "                                segment_id,\n",
    "                                main_mesh_total.vertices,\n",
    "                                main_mesh_total.faces,\n",
    "                                outer_decimation_ratio= 0.25,\n",
    "                                large_mesh_threshold = 60000,\n",
    "                                large_mesh_threshold_inner = 40000,\n",
    "                                soma_width_threshold = 0.32,\n",
    "                                soma_size_threshold = 20000,\n",
    "                               inner_decimation_ratio = 0.25,\n",
    "                               volume_mulitplier=7,\n",
    "                               side_length_ratio_threshold=3,\n",
    "                                soma_size_threshold_max=192000,\n",
    "                                delete_files=True\n",
    "            )\n",
    "        return total_soma_list\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soma List = [<trimesh.Trimesh(vertices.shape=(1864, 3), faces.shape=(3640, 3))>]\n",
      "soma_mesh_list_centers = [array([1326076.34924893,  732678.79350858,  884152.51410944])]\n"
     ]
    }
   ],
   "source": [
    "segment_id = 12345\n",
    "\n",
    "# Load soma mesh\n",
    "# #loads in the Dustin mesh\n",
    "main_mesh_path = Path(\"./Dustin.off\")\n",
    "main_mesh_total = trimesh.load_mesh(str(main_mesh_path.absolute()))\n",
    "\n",
    "soma_mesh_list = load_somas(segment_id,main_mesh_total)\n",
    "print(f\"Soma List = {soma_mesh_list}\")\n",
    "\n",
    "soma_mesh_list_centers = [np.array(np.mean(k.vertices,axis=0)).astype(\"float\")\n",
    "                           for k in soma_mesh_list]\n",
    "print(f\"soma_mesh_list_centers = {soma_mesh_list_centers}\")\n",
    "\n",
    "if len(soma_mesh_list) <= 0:\n",
    "    print(f\"**** No Somas Found for Mesh {segment_id}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "sk = reload(sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do mesh splitting to find which mesh pieces aren't connect to the soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_meshes = sk.split_significant_pieces(\n",
    "                            main_mesh_total,\n",
    "                            significance_threshold=15,\n",
    "                            print_flag=False)\n",
    "len(split_meshes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find which bounding box contains the soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pseudocode: \n",
    "For all meshes in list\n",
    "1) compute soma center\n",
    "2) Find all the bounding boxes that contain the soma center\n",
    "3) Find the mesh with the closest distance from \n",
    "   one vertex to the soma center and tht is winner\n",
    "\"\"\"\n",
    "containing_mesh_indices=dict([(i,[]) for i,sm_c in enumerate(soma_mesh_list_centers)])\n",
    "for k,sm_center in enumerate(soma_mesh_list_centers):\n",
    "\n",
    "    viable_meshes = [j for j,m in enumerate(split_meshes) \n",
    "             if trimesh.bounds.contains(m.bounds,sm_center.reshape(-1,3))\n",
    "                    ]\n",
    "    if len(viable_meshes) == 0:\n",
    "        raise Exception(f\"The Soma {k} with {sm_center} was not contained in any of the boundying boxes\")\n",
    "    elif len(viable_meshes) == 1:\n",
    "        containing_mesh_indices[k] = viable_meshes[0]\n",
    "    else:\n",
    "        #find which mesh is closer to the soma midpoint\n",
    "        min_distances_to_soma = []\n",
    "        for v_i in viable_meshes:\n",
    "            # build the KD Tree\n",
    "            viable_neuron_kdtree = KDTree(soma_mesh_list[v_i].vertices)\n",
    "            distances,closest_node = viable_neuron_kdtree.query(sm_centers.reshape(-1,3))\n",
    "            min_distances_to_soma.append(np.min(distances))\n",
    "        print(f\"min_distances_to_soma = {min_distances_to_soma}\")\n",
    "        containing_mesh_indices[k] = np.argmin(min_distances_to_soma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <trimesh.Trimesh(vertices.shape=(325120, 3), faces.shape=(651866, 3))>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_soma_touching_meshes = [m for i,m in enumerate(split_meshes)\n",
    "                     if i not in list(containing_mesh_indices.values())]\n",
    "len(non_soma_touching_meshes)\n",
    "soma_touching_meshes = dict([(i,split_meshes[m_i]) \n",
    "                             for i,m_i in containing_mesh_indices.items()])\n",
    "soma_touching_meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: For each soma containing mesh: Do Skeletonization and stitching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - util functions for skeletonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_soma(current_soma,main_mesh):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    face_midpoints_soma = np.mean(current_soma.vertices[current_soma.faces],axis=1)\n",
    "\n",
    "\n",
    "    curr_mesh_bbox_restriction,faces_bbox_inclusion = (\n",
    "                    sk.bbox_mesh_restrcition(main_mesh,\n",
    "                                             current_soma.bounds,\n",
    "                                            mult_ratio=1.3)\n",
    "    )\n",
    "\n",
    "    face_midpoints_neuron = np.mean(curr_mesh_bbox_restriction.vertices[curr_mesh_bbox_restriction.faces],axis=1)\n",
    "\n",
    "    soma_kdtree = KDTree(face_midpoints_soma)\n",
    "\n",
    "    distances,closest_node = soma_kdtree.query(face_midpoints_neuron)\n",
    "\n",
    "    distance_threshold = 550\n",
    "    distance_passed_faces  = distances<distance_threshold\n",
    "\n",
    "    faces_to_keep = np.array(list(set(np.arange(0,len(main_mesh.faces))).difference(set(faces_bbox_inclusion[distance_passed_faces]))))\n",
    "    without_soma_mesh = main_mesh.submesh([faces_to_keep],append=True)\n",
    "\n",
    "    #get the significant mesh pieces\n",
    "    mesh_pieces = sk.split_significant_pieces(without_soma_mesh,significance_threshold=200)\n",
    "    print(f\"Total Time for soma mesh cancellation = {np.round(time.time() - start_time,3)}\")\n",
    "    return mesh_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPseudocode: \\n0) Do Soma Extraction and split the meshes from there\\nFor Each significant mesh that was split off in beginning\\n1) Poisson Surface Reconstruction\\n2) CGAL skeletonization of all signfiicant pieces \\n    (if above certain size ! threshold) \\n            --> if not skip straight to surface skeletonization\\n3) Using CGAL skeleton, find the leftover mesh not skeletonized\\n4) Do surface reconstruction on the parts that are left over\\n- with some downsampling\\n5) Stitch the skeleton \\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pseudocode: \n",
    "0) Do Soma Extraction and split the meshes from there\n",
    "For Each significant mesh that was split off in beginning\n",
    "1) Poisson Surface Reconstruction\n",
    "2) CGAL skeletonization of all signfiicant pieces \n",
    "    (if above certain size ! threshold) \n",
    "            --> if not skip straight to surface skeletonization\n",
    "3) Using CGAL skeleton, find the leftover mesh not skeletonized\n",
    "4) Do surface reconstruction on the parts that are left over\n",
    "- with some downsampling\n",
    "5) Stitch the skeleton \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - setting up the paths for data writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from shutil import rmtree\n",
    "from pathlib import Path\n",
    "\n",
    "mesh_base_path=Path(\"./Dustin_vp6/\")\n",
    "current_name=\"Dustin\"\n",
    "\n",
    "if mesh_base_path.exists():\n",
    "    rmtree(str(mesh_base_path.absolute()))\n",
    "mesh_base_path.mkdir(parents=True,exist_ok=True)\n",
    "print(list(mesh_base_path.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeletonize_connected_branch(current_mesh,\n",
    "                        output_folder=\"./temp\",\n",
    "                        delete_temp_files=True,\n",
    "                        name=\"None\",\n",
    "                        surface_reconstruction_size=50,\n",
    "                        n_surface_downsampling = 1,\n",
    "                        n_surface_samples=1000,\n",
    "                        skeleton_print=False,\n",
    "                        mesh_subtraction_distance_threshold=3000,\n",
    "                        mesh_subtraction_buffer=50,\n",
    "                        max_stitch_distance = 18000,\n",
    "                        current_min_edge = 200\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Purpose: To take a mesh and construct a full skeleton of it\n",
    "    (Assuming the Soma is already extracted)\n",
    "    \n",
    "    1) Poisson Surface Reconstruction\n",
    "    2) CGAL skeletonization of all signfiicant pieces \n",
    "        (if above certain size ! threshold) \n",
    "                --> if not skip straight to surface skeletonization\n",
    "    3) Using CGAL skeleton, find the leftover mesh not skeletonized\n",
    "    4) Do surface reconstruction on the parts that are left over\n",
    "    - with some downsampling\n",
    "    5) Stitch the skeleton \n",
    "    \"\"\"\n",
    "    \n",
    "    #check that the mesh is all one piece\n",
    "    current_mesh_splits = sk.split_significant_pieces(current_mesh,\n",
    "                               significance_threshold=1)\n",
    "    if len(current_mesh_splits) > 1:\n",
    "        raise Exception(f\"The mesh passed has {len(current_mesh_splits)} pieces\")\n",
    "\n",
    "    # check the size of the branch and if small enough then just do\n",
    "    # Surface Skeletonization\n",
    "    if len(current_mesh.faces) < surface_reconstruction_size:\n",
    "        #do a surface skeletonization\n",
    "        print(\"Doing skeleton surface reconstruction\")\n",
    "        surf_sk = sk.generate_surface_skeleton(current_mesh.vertices,\n",
    "                                    current_mesh.faces,\n",
    "                                    surface_samples=n_surface_samples,\n",
    "                                             n_surface_downsampling=n_surface_downsampling )\n",
    "        return surf_sk\n",
    "    else:\n",
    "    \n",
    "        #if can't simply do a surface skeletonization then \n",
    "        #use cgal method that requires temp folder\n",
    "\n",
    "        if type(output_folder) != type(Path()):\n",
    "            output_folder = Path(str(output_folder))\n",
    "            output_folder.mkdir(parents=True,exist_ok=True)\n",
    "            \n",
    "        # CGAL Step 1: Do Poisson Surface Reconstruction\n",
    "        Poisson_obj = Poisson(output_folder,overwrite=True)\n",
    "        \n",
    "\n",
    "        skeleton_start = time.time()\n",
    "        print(\"     Starting Screened Poisson\")\n",
    "        new_mesh,output_subprocess_obj = Poisson_obj(   \n",
    "                                    vertices=current_mesh.vertices,\n",
    "                                     faces=current_mesh.faces,\n",
    "                                    mesh_filename=name + \".off\",\n",
    "                                     return_mesh=True,\n",
    "                                     delete_temp_files=False,\n",
    "                                    )\n",
    "        print(f\"-----Time for Screened Poisson= {time.time()-skeleton_start}\")\n",
    "            \n",
    "        #2) Filter away for largest_poisson_piece:\n",
    "        mesh_pieces = sk.split_significant_pieces(new_mesh,\n",
    "                                            significance_threshold=surface_reconstruction_size)\n",
    "        \n",
    "        if skeleton_print:\n",
    "            print(f\"Signifiant mesh pieces of {surface_reconstruction_size} size \"\n",
    "                 f\"after poisson = {len(mesh_pieces)}\")\n",
    "        skeleton_ready_for_stitching = np.array([])\n",
    "        skeleton_files = [] # to be erased later on if need be\n",
    "        if len(mesh_pieces) <= 0:\n",
    "            if skeleton_print:\n",
    "                print(\"No signficant skeleton pieces so just doing surface skeletonization\")\n",
    "            # do surface skeletonization on all of the pieces\n",
    "            surface_mesh_pieces = sk.split_significant_pieces(new_mesh,\n",
    "                                            significance_threshold=2)\n",
    "            \n",
    "            #get the skeletons for all those pieces\n",
    "            current_mesh_skeleton_list = [\n",
    "                sk.generate_surface_skeleton(p.vertices,\n",
    "                                    p.faces,\n",
    "                                    surface_samples=n_surface_samples,\n",
    "                                    n_surface_downsampling=n_surface_downsampling )\n",
    "                for p in surface_mesh_pieces\n",
    "            ]\n",
    "            \n",
    "            skeleton_ready_for_stitching = sk.stack_skeletons(current_mesh_skeleton_list)\n",
    "            \n",
    "            #will stitch them together later\n",
    "        else: #if there are parts that can do the cgal skeletonization\n",
    "            skeleton_start = time.time()\n",
    "            print(\"     Starting Calcification\")\n",
    "            for zz,piece in enumerate(mesh_pieces):\n",
    "                current_mesh_path = output_folder / f\"{name}_{zz}\"\n",
    "                \n",
    "                written_path = sk.write_neuron_off(piece,current_mesh_path)\n",
    "                \n",
    "                #print(f\"Path sending to calcification = {written_path[:-4]}\")\n",
    "                returned_value, sk_file_name = sk.calcification(written_path,\n",
    "                                                               min_edge_length = current_min_edge)\n",
    "                #print(f\"Time for skeletonizatin = {time.time() - skeleton_start}\")\n",
    "                skeleton_files.append(sk_file_name)\n",
    "                \n",
    "            if skeleton_print:\n",
    "                print(f\"-----Time for Running Calcification = {time.time()-skeleton_start}\")\n",
    "            \n",
    "            #collect the skeletons and subtract from the mesh\n",
    "            \n",
    "            significant_poisson_skeleton = sk.read_skeleton_edges_coordinates(skeleton_files)\n",
    "            \n",
    "            if len(significant_poisson_skeleton) > 0:\n",
    "                boolean_significance_threshold=5\n",
    "\n",
    "                print(f\"Before mesh subtraction number of skeleton edges = {significant_poisson_skeleton.shape[0]+1}\")\n",
    "                mesh_pieces_leftover =  sk.mesh_subtraction_by_skeleton(current_mesh,\n",
    "                                                            significant_poisson_skeleton,\n",
    "                                                            buffer=mesh_subtraction_buffer,\n",
    "                                                            bbox_ratio=1.2,\n",
    "                                                            distance_threshold=significant_poisson_skeleton,\n",
    "                                                            significance_threshold=boolean_significance_threshold,\n",
    "                                                            print_flag=False\n",
    "                                                           )\n",
    "\n",
    "                # *****adding another significance threshold*****\n",
    "                leftover_meshes_sig = [k for k in mesh_pieces_leftover if len(k.faces) > 50]\n",
    "                leftover_meshes = sk.combine_meshes(leftover_meshes_sig)\n",
    "            else:\n",
    "                print(\"No recorded skeleton so skiipping\"\n",
    "                     \" to surface skeletonization\")\n",
    "                leftover_meshes_sig = [current_mesh]\n",
    "    \n",
    "            leftover_meshes_sig_surf_sk = []\n",
    "            for m in tqdm(leftover_meshes_sig):\n",
    "                surf_sk = sk.generate_surface_skeleton(m.vertices,\n",
    "                                               m.faces,\n",
    "                                               surface_samples=n_surface_samples,\n",
    "                                    n_surface_downsampling=n_surface_downsampling )\n",
    "                if len(surf_sk) > 0:\n",
    "                    leftover_meshes_sig_surf_sk.append(surf_sk)\n",
    "            leftovers_stacked = sk.stack_skeletons(leftover_meshes_sig_surf_sk)\n",
    "            #print(f\"significant_poisson_skeleton = {significant_poisson_skeleton}\")\n",
    "            #print(f\"leftover_meshes_sig_surf_sk = {leftover_meshes_sig_surf_sk}\")\n",
    "            skeleton_ready_for_stitching = sk.stack_skeletons([significant_poisson_skeleton,leftovers_stacked])\n",
    "            \n",
    "        #now want to stitch together whether generated from \n",
    "        if skeleton_print:\n",
    "            print(f\"After cgal process the un-stitched skeleton has shape {skeleton_ready_for_stitching.shape}\")\n",
    "        \n",
    "        stitched_skeletons_full = sk.stitch_skeleton(\n",
    "                                                  skeleton_ready_for_stitching,\n",
    "                                                  max_stitch_distance=max_stitch_distance,\n",
    "                                                  stitch_print = False,\n",
    "                                                  main_mesh = []\n",
    "                                                )\n",
    "        stitched_skeletons_full_cleaned = sk.clean_skeleton(stitched_skeletons_full)\n",
    "        \n",
    "        # erase the skeleton files if need to be\n",
    "        if delete_temp_files:\n",
    "            for sk_fi in skeleton_files:\n",
    "                if Path(sk_fi).exists():\n",
    "                    Path(sk_fi).unlink()\n",
    "        \n",
    "        # if created temp folder then erase if empty\n",
    "        if str(output_folder.absolute()) == str(Path(\"./temp\").absolute()):\n",
    "            print(\"The process was using a temp folder\")\n",
    "            if len(list(output_folder.iterdir())) == 0:\n",
    "                print(\"Temp folder was empty so deleting it\")\n",
    "                if output_folder.exists():\n",
    "                    rmtree(str(output_folder.absolute()))\n",
    "        \n",
    "        return stitched_skeletons_full_cleaned    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soma_skeleton_stitching(total_soma_skeletons,soma_mesh):\n",
    "    \"\"\"\n",
    "    Purpose: Will stitch together the meshes that are touching\n",
    "    the soma \n",
    "    \n",
    "    Pseudocode: \n",
    "    1) Compute the soma mesh center point\n",
    "    2) For meshes that were originally connected to soma\n",
    "    a. Find the closest skeletal point to soma center\n",
    "    b. Add an edge from closest point to soma center\n",
    "    3) Then do stitching algorithm on all of remaining disconnected\n",
    "        skeletons\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # 1) Compute the soma mesh center point\n",
    "    soma_center = np.mean(soma_mesh.vertices,axis=0)\n",
    "    \n",
    "    soma_connecting_skeleton = []\n",
    "    for skel in total_soma_skeletons:\n",
    "        #get the unique vertex points\n",
    "        unique_skeleton_nodes = np.unique(skel.reshape(-1,3),axis=0)\n",
    "        \n",
    "        # a. Find the closest skeletal point to soma center\n",
    "        # b. Add an edge from closest point to soma center\n",
    "        mesh_tree = KDTree(unique_skeleton_nodes)\n",
    "        distances,closest_node = mesh_tree.query(soma_center.reshape(-1,3))\n",
    "        closest_skeleton_vert = unique_skeleton_nodes[closest_node[np.argmin(distances)]]\n",
    "        soma_connecting_skeleton.append(np.array([closest_skeleton_vert,soma_center]).reshape(-1,2,3))\n",
    "    \n",
    "    print(f\"soma_connecting_skeleton[0].shape = {soma_connecting_skeleton[0].shape}\")\n",
    "    print(f\"total_soma_skeletons[0].shape = {total_soma_skeletons[0].shape}\")\n",
    "    # stith all of the ekeletons together\n",
    "    soma_stitched_sk = sk.stack_skeletons(total_soma_skeletons + soma_connecting_skeleton)\n",
    "    \n",
    "    return soma_stitched_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm.calcification(\"/notebooks3/Users/celii/Documents/Complete_Pinky100_Pipeline/notebooks/Platinum/Platinum_Skeletonization_vp3/Dustin_vp4/Dustin_soma_0_branch_0_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time for soma mesh cancellation = 1.216\n",
      "mesh_pieces after the soma subtraction = 7\n",
      "\n",
      "\n",
      "Working on Dustin_soma_0_branch_0\n",
      "     Starting Screened Poisson\n",
      "IN INPUT FILE VALIDATION LOOP\n",
      "LEAVING LOOP, MESH VALIDATED\n",
      "Using port = 925\n",
      "xvfb-run -n 925 -s \"-screen 0 800x600x24\" meshlabserver $@  -i /notebooks/Platinum_Skeletonization_vp3/Dustin_vp6/Dustin_soma_0_branch_0.off -o /notebooks/Platinum_Skeletonization_vp3/Dustin_vp6/Dustin_soma_0_branch_0_poisson.off -s /notebooks/Platinum_Skeletonization_vp3/Dustin_vp6/poisson_872526.mls\n",
      "-----Time for Screened Poisson= 35.17968153953552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n",
      "face_normals all zero, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signifiant mesh pieces of 50 size after poisson = 502\n",
      "     Starting Calcification\n",
      "-----Time for Running Calcification = 19.00791907310486\n",
      "Before mesh subtraction number of skeleton edges = 4441\n",
      "Inside mesh subtraction, len(main_mesh_bbox_restricted.faces) = 247794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c2939ff6304601a5a90b5b088b7657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Mesh subtraction time = 201.0881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cd5861ae384c60aa3ff95697b284ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=201.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After cgal process the un-stitched skeleton has shape (5650, 2, 3)\n",
      "len_subgraphs AT BEGINNING of the loop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c015ac8d1c124835ba2baf8732f56234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=702.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all graph is one component!\n",
      "\n",
      "Total time for skeleton stitching = 2.668152093887329\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "clean_skeleton() missing 1 required positional argument: 'distance_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9c6ade58a5a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                        \u001b[0moutput_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmesh_base_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                        \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdendrite_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                                         skeleton_print = True)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstitched_dendrite_skeleton\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-190f713d50f2>\u001b[0m in \u001b[0;36mskeletonize_connected_branch\u001b[0;34m(current_mesh, output_folder, delete_temp_files, name, surface_reconstruction_size, n_surface_downsampling, n_surface_samples, skeleton_print, mesh_subtraction_distance_threshold, mesh_subtraction_buffer, max_stitch_distance, current_min_edge)\u001b[0m\n\u001b[1;32m    159\u001b[0m                                                   \u001b[0mmain_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                                                 )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mstitched_skeletons_full_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_skeleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstitched_skeletons_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# erase the skeleton files if need to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: clean_skeleton() missing 1 required positional argument: 'distance_func'"
     ]
    }
   ],
   "source": [
    "sk = reload(sk)\n",
    "\n",
    "soma_touching_time = time.time()\n",
    "soma_touching_meshes_skeletons = []\n",
    "for s_i,main_mesh in soma_touching_meshes.items():\n",
    "    #Do the mesh subtraction to get the disconnected pieces\n",
    "    current_soma = soma_mesh_list[s_i]\n",
    "    \n",
    "    mesh_pieces = subtract_soma(current_soma,main_mesh)\n",
    "    print(f\"mesh_pieces after the soma subtraction = {len(mesh_pieces)}\")\n",
    "    #get each branch skeleton\n",
    "    total_soma_skeletons = []\n",
    "    for dendrite_index,picked_dendrite in enumerate(mesh_pieces):\n",
    "        dendrite_name=current_name + f\"_soma_{s_i}_branch_{dendrite_index}\"\n",
    "        print(f\"\\n\\nWorking on {dendrite_name}\")\n",
    "        stitched_dendrite_skeleton = skeletonize_connected_branch(picked_dendrite,\n",
    "                                                       output_folder=mesh_base_path,\n",
    "                                                       name=dendrite_name,\n",
    "                                                        skeleton_print = True)\n",
    "        \n",
    "        if len(stitched_dendrite_skeleton)<=0:\n",
    "            print(f\"*** Dendrite {dendrite_index} did not have skeleton computed***\")\n",
    "        else: \n",
    "            total_soma_skeletons.append(stitched_dendrite_skeleton)\n",
    "            \n",
    "#print(f\"Total time for soma touching skeletons: {time.time() - soma_touching_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total time for soma touching skeletons: {time.time() - soma_touching_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_soma_skeletons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stitch the skeletons together with the soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_stitched_skeleton = soma_skeleton_stitching(total_soma_skeletons,current_soma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the stitched skeleton for the parts touching the soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_stitched_skeleton.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.graph_skeleton_and_mesh(main_mesh_total.vertices,\n",
    "                           main_mesh_total.faces,\n",
    "                           edge_coordinates=soma_stitched_skeleton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Skeletonization of all non-soma touching branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_soma_touching_meshes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = reload(sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_soma_time = time.time()\n",
    "\n",
    "non_soma_touching_meshes\n",
    "\n",
    "total_non_soma_skeletons = []\n",
    "for j,picked_non_soma_branch in enumerate(non_soma_touching_meshes):\n",
    "#     if j<66:\n",
    "#         continue\n",
    "    dendrite_name=current_name + f\"_non_soma_{j}\"\n",
    "    print(f\"\\n\\nWorking on {dendrite_name}\")\n",
    "    stitched_dendrite_skeleton = skeletonize_connected_branch(picked_non_soma_branch,\n",
    "                                                   output_folder=mesh_base_path,\n",
    "                                                   name=dendrite_name,\n",
    "                                                    skeleton_print = True)\n",
    "\n",
    "    if len(stitched_dendrite_skeleton)<=0:\n",
    "        print(f\"*** Dendrite {dendrite_index} did not have skeleton computed***\")\n",
    "    else: \n",
    "        total_non_soma_skeletons.append(stitched_dendrite_skeleton)\n",
    "\n",
    "\n",
    "print(f\"Time for non-soma skeletons = {time.time() - non_soma_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch the All Soma touching and non soma touching branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_non_soma_skeletons = sk.stack_skeletons(total_non_soma_skeletons)\n",
    "stacked_soma_skeletons = sk.stack_skeletons([soma_stitched_skeleton])\n",
    "whole_skeletons_for_stitching = sk.stack_skeletons([stacked_non_soma_skeletons,stacked_soma_skeletons])\n",
    "\n",
    "final_skeleton_pre_clean = sk.stitch_skeleton(\n",
    "                                                  whole_skeletons_for_stitching,\n",
    "                                                  stitch_print = False,\n",
    "                                                  main_mesh = []\n",
    "                                                )\n",
    "final_skeleton_clean = sk.clean_skeleton(final_skeleton_pre_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_skeleton_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.graph_skeleton_and_mesh(\n",
    "                            main_mesh_total.vertices,\n",
    "                          main_mesh_total.faces,\n",
    "                          edge_coordinates=final_skeleton_clean\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = reload(sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_time = time.time()\n",
    "new_cleaned_skeleton = sk.clean_skeleton(final_skeleton_clean,\n",
    "                        distance_func=sk.skeletal_distance,\n",
    "                  min_distance_to_junction=5000,\n",
    "                  return_skeleton=True,\n",
    "                  print_flag=False)\n",
    "print(f\"Total time for skeleton clean {time.time() - clean_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.graph_skeleton_and_mesh(\n",
    "                            main_mesh_total.vertices,\n",
    "                          main_mesh_total.faces,\n",
    "                          edge_coordinates=new_cleaned_skeleton,\n",
    "                            axis_box_off = True\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.save_skeleton_cgal(new_cleaned_skeleton,f\"{current_name}_sk_cleaned.cgal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving as an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "ipv.pylab.save(\"Dusting_cleaned_5000.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
