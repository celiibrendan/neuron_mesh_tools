{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "import time\n",
    "\n",
    "\n",
    "@schema\n",
    "class Martinotti_Attributes(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation #segment ids and mesh data for all decimated segments\n",
    "    soma_index : tinyint unsigned #index given to this soma to account for multiple somas in one base semgnet\n",
    "    ---\n",
    "    centroid_x           : int unsigned                 # (EM voxels)\n",
    "    centroid_y           : int unsigned                 # (EM voxels)\n",
    "    centroid_z           : int unsigned                 # (EM voxels)\n",
    "    distance_from_prediction : double                   # the distance of the ALLEN predicted centroid soma center from the algorithms prediction\n",
    "    soma_vertices             : longblob                # array of vertices\n",
    "    soma_faces             : longblob                   # array of faces\n",
    "    multiplicity         : tinyint unsigned             # the number of somas found for this base segment\n",
    "    run_time : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #further restrict table to include just subset\n",
    "    key_source = (minnie.Decimation & (dj.U(\"segment_id\") & (m65.AllenSegmentCentroid & \"status=1\").proj()) & \"version=\" + str(decimation_version))\n",
    "    \n",
    "    # MAKE IS WHAT CALCULATES THE ATTRIBUTES AND INSERTS IT INTO YOUR TABLE\n",
    "    def make(self,key):\n",
    "        \n",
    "        # Get the Mesh Data\n",
    "        print(f\"\\n\\n\\n---- Working on {key['segment_id']} ----\")\n",
    "        \n",
    "        new_mesh = (minnie.Decimation() & key).fetch1(\"mesh\")\n",
    "        current_mesh_verts,current_mesh_faces = new_mesh.vertices,new_mesh.faces\n",
    "        \n",
    "        segment_id = key[\"segment_id\"]\n",
    "        \n",
    "        # Extract Soma Center\n",
    "        total_soma_list, run_time = extract_soma_center(segment_id,current_mesh_verts,current_mesh_faces)\n",
    "        print(f\"Run time was {run_time}  and the total_soma_list = {total_soma_list} and \")\n",
    "        \n",
    "        # IF NO SOMA\n",
    "        if len(total_soma_list) <= 0:\n",
    "            print(\"There were no somas found for this mesh so just writing empty data\")\n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=-1,\n",
    "                              centroid_x=None,\n",
    "                               centroid_y=None,\n",
    "                               centroid_z=None,\n",
    "                               distance_from_prediction=None,\n",
    "                               soma_vertices=None,\n",
    "                               soma_faces=None,\n",
    "                               multiplicity=0,\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            raise Exception(\"to prevent writing because none were found\")\n",
    "            # Store the attributes that you found in a dictionary\n",
    "            # Incert the row of attributes for that key\n",
    "            self.insert1(insert_dict,skip_duplicates=True)\n",
    "            return\n",
    "        \n",
    "        # If >0 Soma\n",
    "        \n",
    "        #get the soma prediction\n",
    "        c_x,c_y,c_z = (m65.AllenSegmentCentroid() & key).fetch1(\"centroid_x\",\"centroid_y\",\"centroid_z\")\n",
    "        allen_centroid_prediction = np.array([c_x,c_y,c_z])\n",
    "        \n",
    "        dicts_to_insert = []\n",
    "        \n",
    "        # If >1 Soma\n",
    "        if len(total_soma_list) > 1:\n",
    "            raise Exception(\"to prevent writing MULTILPLE SOMAS TO DATABASE\")\n",
    "            \n",
    "        for i,current_soma in enumerate(total_soma_list):\n",
    "            print(\"Trying to write off file\")\n",
    "            current_soma.export(f\"{key['segment_id']}/{key['segment_id']}_soma_{i}.off\")\n",
    "            auto_prediction_center = np.mean(current_soma.vertices,axis=0)\n",
    "            \n",
    "            error_distance = np.linalg.norm(allen_centroid_prediction-auto_prediction_center)\n",
    "            \n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=i,\n",
    "                              centroid_x=auto_prediction_center[0],\n",
    "                               centroid_y=auto_prediction_center[1],\n",
    "                               centroid_z=auto_prediction_center[2],\n",
    "                               distance_from_prediction=error_distance,\n",
    "                               soma_vertices=current_soma.vertices,\n",
    "                               soma_faces=current_soma.faces,\n",
    "                               multiplicity=len(total_soma_list),\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            \n",
    "            \n",
    "            dicts_to_insert.append(insert_dict)\n",
    "            \n",
    "        #raise Exception(\"to prevent writing\")\n",
    "            \n",
    "        self.insert(dicts_to_insert,skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actually will do the population\n",
    "\"\"\"\n",
    "\n",
    "#(schema.jobs & \"table_name='__whole_auto_annotations_label_clusters3'\")#.delete()\n",
    "dj.config[\"enable_python_native_blobs\"] = True\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "Martinotti_Attributes.populate(reserve_jobs=True) # runs parallel\n",
    "print(f\"Total time for SomaCentroidValidation populate = {time.time() - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
